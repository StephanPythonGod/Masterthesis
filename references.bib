
@misc{zhu_retrieving_2021,
	title = {Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering},
	url = {http://arxiv.org/abs/2101.00774},
	doi = {10.48550/arXiv.2101.00774},
	shorttitle = {Retrieving and Reading},
	abstract = {Open-domain Question Answering ({OpenQA}) is an important task in Natural Language Processing ({NLP}), which aims to answer a question in the form of natural language based on large-scale unstructured documents. Recently, there has been a surge in the amount of research literature on {OpenQA}, particularly on techniques that integrate with neural Machine Reading Comprehension ({MRC}). While these research works have advanced performance to new heights on benchmark datasets, they have been rarely covered in existing surveys on {QA} systems. In this work, we review the latest research trends in {OpenQA}, with particular attention to systems that incorporate neural {MRC} techniques. Specifically, we begin with revisiting the origin and development of {OpenQA} systems. We then introduce modern {OpenQA} architecture named "Retriever-Reader" and analyze the various systems that follow this architecture as well as the specific techniques adopted in each of the components. We then discuss key challenges to developing {OpenQA} systems and offer an analysis of benchmarks that are commonly used. We hope our work would enable researchers to be informed of the recent advancement and also the open challenges in {OpenQA} research, so as to stimulate further progress in this field.},
	number = {{arXiv}:2101.00774},
	publisher = {{arXiv}},
	author = {Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
	urldate = {2023-08-27},
	date = {2021-05-08},
	eprinttype = {arxiv},
	eprint = {2101.00774 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Example Architectures, overview, {QA}, {RRS}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7JSH329N/Zhu et al. - 2021 - Retrieving and Reading A Comprehensive Survey on .pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/6WWFWPPE/2101.html:text/html},
}

@misc{zamani_conversational_2023,
	title = {Conversational Information Seeking},
	url = {http://arxiv.org/abs/2201.08808},
	doi = {10.48550/arXiv.2201.08808},
	abstract = {Conversational information seeking ({CIS}) is concerned with a sequence of interactions between one or more users and an information system. Interactions in {CIS} are primarily based on natural language dialogue, while they may include other types of interactions, such as click, touch, and body gestures. This monograph provides a thorough overview of {CIS} definitions, applications, interactions, interfaces, design, implementation, and evaluation. This monograph views {CIS} applications as including conversational search, conversational question answering, and conversational recommendation. Our aim is to provide an overview of past research related to {CIS}, introduce the current state-of-the-art in {CIS}, highlight the challenges still being faced in the community. and suggest future directions.},
	number = {{arXiv}:2201.08808},
	publisher = {{arXiv}},
	author = {Zamani, Hamed and Trippas, Johanne R. and Dalton, Jeff and Radlinski, Filip},
	urldate = {2023-08-27},
	date = {2023-01-25},
	eprinttype = {arxiv},
	eprint = {2201.08808 [cs]},
	keywords = {Background, {CIS}, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval, overview, {QA}, {SvD}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7WFGQWLT/Zamani et al. - 2023 - Conversational Information Seeking.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/MBB85CRQ/2201.html:text/html},
}

@misc{gao_neural_2022,
	title = {Neural Approaches to Conversational Information Retrieval},
	url = {http://arxiv.org/abs/2201.05176},
	doi = {10.48550/arXiv.2201.05176},
	abstract = {A conversational information retrieval ({CIR}) system is an information retrieval ({IR}) system with a conversational interface which allows users to interact with the system to seek information via multi-turn conversations of natural language, in spoken or written form. Recent progress in deep learning has brought tremendous improvements in natural language processing ({NLP}) and conversational {AI}, leading to a plethora of commercial conversational services that allow naturally spoken and typed interaction, increasing the need for more human-centric interactions in {IR}. As a result, we have witnessed a resurgent interest in developing modern {CIR} systems in both research communities and industry. This book surveys recent advances in {CIR}, focusing on neural approaches that have been developed in the last few years. This book is based on the authors' tutorial at {SIGIR}'2020 (Gao et al., 2020b), with {IR} and {NLP} communities as the primary target audience. However, audiences with other background, such as machine learning and human-computer interaction, will also find it an accessible introduction to {CIR}. We hope that this book will prove a valuable resource for students, researchers, and software developers. This manuscript is a working draft. Comments are welcome.},
	number = {{arXiv}:2201.05176},
	publisher = {{arXiv}},
	author = {Gao, Jianfeng and Xiong, Chenyan and Bennett, Paul and Craswell, Nick},
	urldate = {2023-08-27},
	date = {2022-01-13},
	eprinttype = {arxiv},
	eprint = {2201.05176 [cs]},
	keywords = {Background, {CIS}, Computer Science - Computation and Language, Computer Science - Information Retrieval, overview, {QA}, {SvD}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/RZ38AQSJ/Gao et al. - 2022 - Neural Approaches to Conversational Information Re.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/5UH2JCTT/2201.html:text/html},
}

@misc{pandya_question_2021,
	title = {Question Answering Survey: Directions, Challenges, Datasets, Evaluation Matrices},
	url = {http://arxiv.org/abs/2112.03572},
	doi = {10.48550/arXiv.2112.03572},
	shorttitle = {Question Answering Survey},
	abstract = {The usage and amount of information available on the internet increase over the past decade. This digitization leads to the need for automated answering system to extract fruitful information from redundant and transitional knowledge sources. Such systems are designed to cater the most prominent answer from this giant knowledge source to the user query using natural language understanding ({NLU}) and thus eminently depends on the Question-answering({QA}) field. Question answering involves but not limited to the steps like mapping of user question to pertinent query, retrieval of relevant information, finding the best suitable answer from the retrieved information etc. The current improvement of deep learning models evince compelling performance improvement in all these tasks. In this review work, the research directions of {QA} field are analyzed based on the type of question, answer type, source of evidence-answer, and modeling approach. This detailing followed by open challenges of the field like automatic question generation, similarity detection and, low resource availability for a language. In the end, a survey of available datasets and evaluation measures is presented.},
	number = {{arXiv}:2112.03572},
	publisher = {{arXiv}},
	author = {Pandya, Hariom A. and Bhatt, Brijesh S.},
	urldate = {2023-09-19},
	date = {2021-12-07},
	eprinttype = {arxiv},
	eprint = {2112.03572 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval, Computer Science - Machine Learning, overview, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/88TJBBSV/Pandya und Bhatt - 2021 - Question Answering Survey Directions, Challenges,.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/87EC3RU7/2112.html:text/html},
}

@article{hao_recent_2022,
	title = {Recent progress in leveraging deep learning methods for question answering},
	volume = {34},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-021-06748-3},
	doi = {10.1007/s00521-021-06748-3},
	abstract = {Question answering, serving as one of important tasks in natural language processing, enables machines to understand questions in natural language and answer the questions concisely. From web search to expert systems, question answering systems are widely applied to various domains in assisting information seeking. Deep learning methods have boosted various tasks of question answering and have demonstrated dramatic effects in performance improvement for essential steps of question answering. Thus, leveraging deep learning methods for question answering has drawn much attention from both academia and industry in recent years. This paper provides a systematic review of the recent development of deep learning methods for question answering. The survey covers the scope including methods, datasets, and applications. The methods are discussed in terms of network structure characteristics, methodology innovations, and their effectiveness. The survey is expected to be a contribution to the summarization of recent research progress and future directions of deep learning methods for question answering.},
	pages = {2765--2783},
	number = {4},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Hao, Tianyong and Li, Xinxin and He, Yulan and Wang, Fu Lee and Qu, Yingying},
	urldate = {2023-09-19},
	date = {2022-02},
	langid = {english},
	keywords = {Background, basics, overview, {QA}},
	file = {Hao et al. - 2022 - Recent progress in leveraging deep learning method.pdf:/Users/lenert/Zotero/storage/247GDDR2/Hao et al. - 2022 - Recent progress in leveraging deep learning method.pdf:application/pdf},
}

@article{etezadi_state_2023,
	title = {The state of the art in open domain complex question answering: a survey},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-022-03732-9},
	doi = {10.1007/s10489-022-03732-9},
	shorttitle = {The state of the art in open domain complex question answering},
	abstract = {Research on question answering ({QA}) systems has a long tradition. {QA} systems, as widely used systems in various applications, seek to find the answers to the given questions through the available resources. These systems are expected to be capable of answering various types of questions, including simple questions whose answers can be found in a single passage or sentence and complex questions which need more complicated reasoning to find the answer or their answer should be found by traversing several relations. Nowadays, answering complex questions from texts or structured data is a challenge in {QA} systems. In this paper, we have a comparative study on {QA} approaches and systems for answering complex questions. For this purpose, firstly, this paper discusses what a complex question is and surveys different types of constraints that may appear in complex questions. Furthermore, it addresses the challenges of these types of questions, the methods proposed to deal with them, and benchmark datasets used to evaluate their strengths and weaknesses.},
	pages = {4124--4144},
	number = {4},
	journaltitle = {Applied Intelligence},
	shortjournal = {Appl Intell},
	author = {Etezadi, Romina and Shamsfard, Mehrnoush},
	urldate = {2023-09-19},
	date = {2023-02},
	langid = {english},
	keywords = {Background, basics, overview, {QA}},
	file = {Etezadi und Shamsfard - 2023 - The state of the art in open domain complex questi.pdf:/Users/lenert/Zotero/storage/TTQ32SLA/Etezadi und Shamsfard - 2023 - The state of the art in open domain complex questi.pdf:application/pdf},
}

@book{jurafsky_speech_2023,
	location = {Palo Alto},
	edition = {3},
	title = {Speech and Language Processing},
	abstract = {An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
	author = {Jurafsky, Dan and Martin, James H.},
	date = {2023-01-03},
	file = {Speech and Language Processing.pdf:/Users/lenert/Zotero/storage/3K7TMW7N/14.pdf:application/pdf},
}

@misc{farea_evaluation_2022,
	title = {Evaluation of Question Answering Systems: Complexity of judging a natural language},
	url = {http://arxiv.org/abs/2209.12617},
	doi = {10.48550/arXiv.2209.12617},
	shorttitle = {Evaluation of Question Answering Systems},
	abstract = {Question answering ({QA}) systems are among the most important and rapidly developing research topics in natural language processing ({NLP}). A reason, therefore, is that a {QA} system allows humans to interact more naturally with a machine, e.g., via a virtual assistant or search engine. In the last decades, many {QA} systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a {QA} system. This survey attempts to provide a systematic overview of the general framework of {QA}, {QA} paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of {QA} systems. The latter is particularly important because not only is the construction of a {QA} system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.},
	number = {{arXiv}:2209.12617},
	publisher = {{arXiv}},
	author = {Farea, Amer and Yang, Zhen and Duong, Kien and Perera, Nadeesha and Emmert-Streib, Frank},
	urldate = {2023-09-20},
	date = {2022-09-10},
	eprinttype = {arxiv},
	eprint = {2209.12617 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, overview, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/UATP8ZJ9/Farea et al. - 2022 - Evaluation of Question Answering Systems Complexi.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/XP44XJ2E/2209.html:text/html;Full Text PDF:/Users/lenert/Zotero/storage/ISM7H746/Farea et al. - 2022 - Evaluation of Question Answering Systems Complexi.pdf:application/pdf},
}


@inproceedings{voorhees_trec-8_1999,
	title = {The {TREC}-8 Question Answering Track Report},
	url = {https://www.semanticscholar.org/paper/The-TREC-8-Question-Answering-Track-Report-Voorhees/646d4888871aca2a25111eb2520e4c47e253b014},
	abstract = {The {TREC}-8 Question Answering track was the  first large-scale evaluation of domain-independent question answering systems. This paper summarizes the results of the track by giving a brief overview of the different approaches taken to solve the problem. The most accurate systems found a correct response for more than 2/3 of the questions. Relatively simple bag-of-words approaches were adequate for  finding answers when responses could be as long as a paragraph (250 bytes), but more sophisticated processing was necessary for more direct responses (50 bytes). 
 
The {TREC}-8 Question Answering track was an initial e ort to bring the bene ts of large-scale evaluation to bear on a question answering ({QA}) task. The goal in the {QA} task is to retrieve small snippets of text that contain the actual answer to a question rather than the document lists traditionally returned by text retrieval systems. The assumption is that users would usually prefer to be given the answer rather than  and the answer themselves in a document. 
 
This paper summarizes the retrieval results of the track; a companion paper ({\textbackslash}The {TREC}-8 Question Answering Track Evaluation") gives details about how the evaluation was implemented. By necessity, a track report can give only an overview of the different approaches used in the track. Readers are urged to consult the participants' papers elsewhere in the Proceedings for details regarding a particular approach.},
	eventtitle = {Text Retrieval Conference},
	author = {Voorhees, E.},
	urldate = {2023-09-19},
	date = {1999},
	keywords = {Background, basics, groundwork, historical, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/RSDRLSMP/Voorhees - 1999 - The TREC-8 Question Answering Track Report.pdf:application/pdf},
}

@inproceedings{green_baseball_1961,
	location = {New York, {NY}, {USA}},
	title = {Baseball: an automatic question-answerer},
	isbn = {978-1-4503-7872-7},
	url = {https://dl.acm.org/doi/10.1145/1460690.1460714},
	doi = {10.1145/1460690.1460714},
	series = {{IRE}-{AIEE}-{ACM} '61 (Western)},
	shorttitle = {Baseball},
	abstract = {{\textless}u{\textgreater}Baseball{\textless}/u{\textgreater} is a computer program that answers questions phrased in ordinary English about stored data. The program reads the question from punched cards. After the words and idioms are looked up in a dictionary, the phrase structure and other syntactic facts are determined for a content analysis, which lists attribute-value pairs specifying the information given and the information requested. The requested information is then extracted from the data matching the specifications, and any necessary processing is done. Finally, the answer is printed. The program's present context is baseball games; it answers such questions as "Where did each team play on July 7?"},
	pages = {219--224},
	booktitle = {Papers presented at the May 9-11, 1961, western joint {IRE}-{AIEE}-{ACM} computer conference},
	publisher = {Association for Computing Machinery},
	author = {Green, Bert F. and Wolf, Alice K. and Chomsky, Carol and Laughery, Kenneth},
	urldate = {2023-09-19},
	date = {1961-05-09},
	keywords = {Background, groundwork, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/A9B5I6B8/Green et al. - 1961 - Baseball an automatic question-answerer.pdf:application/pdf},
}

@article{ferrucci_introduction_2012,
	title = {Introduction to This is Watson},
	volume = {56},
	issn = {0018-8646},
	doi = {10.1147/JRD.2012.2184356},
	abstract = {In 2007, {IBM} Research took on the grand challenge of building a computer system that could compete with champions at the game of Jeopardy!™. In 2011, the open-domain question-answering ({QA}) system, dubbed Watson, beat the two highest ranked players in a nationally televised two-game Jeopardy! match. This paper provides a brief history of the events and ideas that positioned our team to take on the Jeopardy! challenge, build Watson, {IBM} Watson™, and ultimately triumph. It describes both the nature of the {QA} challenge represented by Jeopardy! and our overarching technical approach. The main body of this paper provides a narrative of the {DeepQA} processing pipeline to introduce the articles in this special issue and put them in context of the overall system. Finally, this paper summarizes our main results, describing how the system, as a holistic combination of many diverse algorithmic techniques, performed at champion levels, and it briefly discusses the team's future research plans.},
	pages = {1:1--1:15},
	number = {3},
	journaltitle = {{IBM} Journal of Research and Development},
	author = {Ferrucci, D. A.},
	date = {2012-05},
	note = {Conference Name: {IBM} Journal of Research and Development},
	keywords = {Algorithm design and analysis, Background, Computer architecture, Computers, Games, groundwork, History, {QA}, Semantics},
	file = {IEEE Xplore Abstract Record:/Users/lenert/Zotero/storage/9LBMHVTC/6177724.html:text/html},
}

@inproceedings{zhang_survey_2023,
	location = {Toronto, Canada},
	title = {A Survey for Efficient Open Domain Question Answering},
	url = {https://aclanthology.org/2023.acl-long.808},
	doi = {10.18653/v1/2023.acl-long.808},
	abstract = {Open domain question answering ({ODQA}) is a longstanding task aimed at answering factual questions from a large knowledge corpus without any explicit evidence in natural language processing ({NLP}). Recent works have predominantly focused on improving the answering accuracy and have achieved promising progress. However, higher accuracy often requires more memory consumption and inference latency, which might not necessarily be efficient enough for direct deployment in the real world. Thus, a trade-off between accuracy, memory consumption and processing speed is pursued. In this paper, we will survey recent advancements in the efficiency of {ODQA} models and conclude core techniques for achieving efficiency. Additionally, we will provide a quantitative analysis of memory cost, query speed, accuracy, and overall performance comparison. Our goal is to keep scholars informed of the latest advancements and open challenges in {ODQA} efficiency research and contribute to the further development of {ODQA} efficiency.},
	eventtitle = {{ACL} 2023},
	pages = {14447--14465},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Qin and Chen, Shangsi and Xu, Dongkuan and Cao, Qingqing and Chen, Xiaojun and Cohn, Trevor and Fang, Meng},
	urldate = {2023-08-27},
	date = {2023-07},
	keywords = {Background, Evaluation, Limits, Memory, Metrics, overview, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/4B43W6HL/Zhang et al. - 2023 - A Survey for Efficient Open Domain Question Answer.pdf:application/pdf},
}


@misc{dasigi_dataset_2021,
	title = {A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},
	url = {http://arxiv.org/abs/2105.03011},
	doi = {10.48550/arXiv.2105.03011},
	abstract = {Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present {QASPER}, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an {NLP} practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of {NLP} practitioners who also provide supporting evidence to answers. We find that existing models that do well on other {QA} tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking {QA}, which our dataset is designed to facilitate.},
	number = {{arXiv}:2105.03011},
	publisher = {{arXiv}},
	author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt},
	urldate = {2023-08-27},
	date = {2021-05-06},
	eprinttype = {arxiv},
	eprint = {2105.03011 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Dataset, Evaluation, groundwork, Indexing, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/WTHDV2NM/Dasigi et al. - 2021 - A Dataset of Information-Seeking Questions and Ans.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/BC2HI6RJ/2105.html:text/html},
}

@misc{rajpurkar_squad_2016,
	title = {{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
	url = {http://arxiv.org/abs/1606.05250},
	doi = {10.48550/arXiv.1606.05250},
	shorttitle = {{SQuAD}},
	abstract = {We present the Stanford Question Answering Dataset ({SQuAD}), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
	number = {{arXiv}:1606.05250},
	publisher = {{arXiv}},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	urldate = {2023-09-21},
	date = {2016-10-10},
	eprinttype = {arxiv},
	eprint = {1606.05250 [cs]},
	keywords = {Background, Computer Science - Computation and Language, dataset, groundwork, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/EIBICFD5/Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/HQLULCXF/1606.html:text/html},
}


@article{mishra_survey_2016,
	title = {A survey on question answering systems with classification},
	volume = {28},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157815000890},
	doi = {10.1016/j.jksuci.2014.10.007},
	abstract = {Question answering systems ({QASs}) generate answers of questions asked in natural languages. Early {QASs} were developed for restricted domains and have limited capabilities. Current {QASs} focus on types of questions generally asked by users, characteristics of data sources consulted, and forms of correct answers generated. Research in the area of {QASs} began in 1960s and since then, a large number of {QASs} have been developed. To identify the future scope of research in this area, the need of a comprehensive survey on {QASs} arises naturally. This paper surveys {QASs} and classifies them based on different criteria. We identify the current status of the research in the each category of {QASs}, and suggest future scope of the research.},
	pages = {345--361},
	number = {3},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	shortjournal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Mishra, Amit and Jain, Sanjay Kumar},
	urldate = {2023-09-21},
	date = {2016-07-01},
	keywords = {Background, basics, Information retrieval, Natural language processing, Natural language understanding, overview, {QA}, Question answering system, Search engine},
	file = {ScienceDirect Snapshot:/Users/lenert/Zotero/storage/ZGAGW9F2/S1319157815000890.html:text/html},
}


@inproceedings{roberts_how_2020,
	location = {Online},
	title = {How Much Knowledge Can You Pack Into the Parameters of a Language Model?},
	url = {https://aclanthology.org/2020.emnlp-main.437},
	doi = {10.18653/v1/2020.emnlp-main.437},
	abstract = {It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.},
	eventtitle = {{EMNLP} 2020},
	pages = {5418--5426},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Roberts, Adam and Raffel, Colin and Shazeer, Noam},
	urldate = {2023-09-21},
	date = {2020-11},
	keywords = {Background, Example Architectures, generative, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/B98FDWN7/Roberts et al. - 2020 - How Much Knowledge Can You Pack Into the Parameter.pdf:application/pdf},
}


@article{dimitrakis_survey_2020,
	title = {A survey on question answering systems over linked data and documents},
	volume = {55},
	issn = {1573-7675},
	url = {https://doi.org/10.1007/s10844-019-00584-7},
	doi = {10.1007/s10844-019-00584-7},
	abstract = {Question Answering ({QA}) systems aim at supplying precise answers to questions, posed by users in a natural language form. They are used in a wide range of application areas, from bio-medicine to tourism. Their underlying knowledge source can be structured data (e.g. {RDF} graphs and {SQL} databases), unstructured data in the form of plain text (e.g. textual excerpts from Wikipedia), or combinations of the above. In this paper we survey the recent work that has been done in the area of stateless {QA} systems with emphasis on methods that have been applied in {RDF} and Linked Data, documents, and mixtures of these. We identify the main challenges, we categorize the existing approaches according to various aspects, we review 21 recent systems, and 23 evaluation and training datasets that are most commonly used in the literature categorized according to the type of the domain, the underlying knowledge source, the provided tasks, and the associated evaluation metrics.},
	pages = {233--259},
	number = {2},
	journaltitle = {Journal of Intelligent Information Systems},
	shortjournal = {J Intell Inf Syst},
	author = {Dimitrakis, Eleftherios and Sgontzos, Konstantinos and Tzitzikas, Yannis},
	urldate = {2023-09-21},
	date = {2020-10-01},
	langid = {english},
	keywords = {Background, dialogue systems, evaluation collections, Example, {KBQA}, overview, {QA}, Question answering, {RDF} and linked data},
}

@article{harabagiu_open_domain_2003,
	title = {Open-domain textual question answering techniques},
	volume = {9},
	issn = {1469-8110, 1351-3249},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/opendomain-textual-question-answering-techniques/31811E06D60998460030D304F6869D7E},
	doi = {10.1017/S1351324903003176},
	pages = {231--267},
	number = {3},
	journaltitle = {Natural Language Engineering},
	author = {Harabagiu, Sanda M. and Maiorano, Steven J. and Pasca, Marius A.},
	urldate = {2023-09-21},
	date = {2003-09},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	keywords = {Background, Example Architectures, {QA}},
}


@article{nassiri_transformer_2023,
	title = {Transformer models used for text-based question answering systems},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-022-04052-8},
	doi = {10.1007/s10489-022-04052-8},
	abstract = {The question answering system is frequently applied in the area of natural language processing ({NLP}) because of the wide variety of applications. It consists of answering questions using natural language. The problem is, in general, solved by employing a dataset that consists of an input text, a query, and the text segment or span from the input text that provides the question’s answer. The ability to make human-level predictions from data has improved significantly thanks to deep learning models, particularly the Transformer architecture, which has been state-of-the-art in text-based models in recent years. This paper reviews studies related to the use of transformer models in the implementation of question-answering ({QA}) systems. The paper’s first focus is on the attention and transformer models. A brief description of the architectures is presented by classifying them into models based on encoders, decoders, and on both Encoder-Decoder. Following that, we examine the most recent research trends in textual {QA} datasets by highlighting the architecture of {QA} systems and categorizing them according to various criteria. We survey also a significant set of evaluation metrics that have been developed in order to evaluate the models’ performance. Finally, we highlight solutions built to simplify the implementation of Transformer models.},
	pages = {10602--10635},
	number = {9},
	journaltitle = {Applied Intelligence},
	shortjournal = {Appl Intell},
	author = {Nassiri, Khalid and Akhloufi, Moulay},
	urldate = {2023-09-21},
	date = {2023-05},
	langid = {english},
	keywords = {Background, basics, Example Architectures, overview, {QA}},
	file = {Nassiri und Akhloufi - 2023 - Transformer models used for text-based question an.pdf:/Users/lenert/Zotero/storage/CBW5CYTP/Nassiri und Akhloufi - 2023 - Transformer models used for text-based question an.pdf:application/pdf},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream {NLP} tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation ({RAG}) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce {RAG} models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two {RAG} formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive {NLP} tasks and set the state-of-the-art on three open domain {QA} tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that {RAG} models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	number = {{arXiv}:2005.11401},
	publisher = {{arXiv}},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	urldate = {2023-09-21},
	date = {2021-04-12},
	eprinttype = {arxiv},
	eprint = {2005.11401 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, Example Architectures, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/W8IWGTEN/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Inten.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/AWEXY5RF/2005.html:text/html},
}

@misc{guu_realm_2020,
	title = {{REALM}: Retrieval-Augmented Language Model Pre-Training},
	url = {http://arxiv.org/abs/2002.08909},
	doi = {10.48550/arXiv.2002.08909},
	shorttitle = {{REALM}},
	abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for {NLP} tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training ({REALM}) by fine-tuning on the challenging task of Open-domain Question Answering (Open-{QA}). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-{QA} benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
	number = {{arXiv}:2002.08909},
	publisher = {{arXiv}},
	author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
	urldate = {2023-09-21},
	date = {2020-02-10},
	eprinttype = {arxiv},
	eprint = {2002.08909 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, Example Architectures, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/2HCGEMTA/Guu et al. - 2020 - REALM Retrieval-Augmented Language Model Pre-Trai.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZBUCDVGK/2002.html:text/html},
}

@inproceedings{nishida_retrieve-and-read_2018,
	title = {Retrieve-and-Read: Multi-task Learning of Information Retrieval and Reading Comprehension},
	url = {http://arxiv.org/abs/1808.10628},
	doi = {10.1145/3269206.3271702},
	shorttitle = {Retrieve-and-Read},
	abstract = {This study considers the task of machine reading at scale ({MRS}) wherein, given a question, a system first performs the information retrieval ({IR}) task of finding relevant passages in a knowledge source and then carries out the reading comprehension ({RC}) task of extracting an answer span from the passages. Previous {MRS} studies, in which the {IR} component was trained without considering answer spans, struggled to accurately find a small number of relevant passages from a large set of passages. In this paper, we propose a simple and effective approach that incorporates the {IR} and {RC} tasks by using supervised multi-task learning in order that the {IR} component can be trained by considering answer spans. Experimental results on the standard benchmark, answering {SQuAD} questions using the full Wikipedia as the knowledge source, showed that our model achieved state-of-the-art performance. Moreover, we thoroughly evaluated the individual contributions of our model components with our new Japanese dataset and {SQuAD}. The results showed significant improvements in the {IR} task and provided a new perspective on {IR} for {RC}: it is effective to teach which part of the passage answers the question rather than to give only a relevance score to the whole passage.},
	pages = {647--656},
	booktitle = {Proceedings of the 27th {ACM} International Conference on Information and Knowledge Management},
	author = {Nishida, Kyosuke and Saito, Itsumi and Otsuka, Atsushi and Asano, Hisako and Tomita, Junji},
	urldate = {2023-09-21},
	date = {2018-10-17},
	eprinttype = {arxiv},
	eprint = {1808.10628 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, Example Architectures, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/YZ4J87GJ/Nishida et al. - 2018 - Retrieve-and-Read Multi-task Learning of Informat.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/TRPAWYK9/1808.html:text/html},
}


@misc{mcdonald_detect_2022,
	title = {Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot Document-Level Question Answering},
	url = {http://arxiv.org/abs/2210.01959},
	doi = {10.48550/arXiv.2210.01959},
	shorttitle = {Detect, Retrieve, Comprehend},
	abstract = {Researchers produce thousands of scholarly documents containing valuable technical knowledge. The community faces the laborious task of reading these documents to identify, extract, and synthesize information. To automate information gathering, document-level question answering ({QA}) offers a flexible framework where human-posed questions can be adapted to extract diverse knowledge. Finetuning {QA} systems requires access to labeled data (tuples of context, question and answer). However, data curation for document {QA} is uniquely challenging because the context (i.e. answer evidence passage) needs to be retrieved from potentially long, ill-formatted documents. Existing {QA} datasets sidestep this challenge by providing short, well-defined contexts that are unrealistic in real-world applications. We present a three-stage document {QA} approach: (1) text extraction from {PDF}; (2) evidence retrieval from extracted texts to form well-posed contexts; (3) {QA} to extract knowledge from contexts to return high-quality answers -- extractive, abstractive, or Boolean. Using {QASPER} for evaluation, our detect-retrieve-comprehend ({DRC}) system achieves a +7.19 improvement in Answer-F1 over existing baselines while delivering superior context selection. Our results demonstrate that {DRC} holds tremendous promise as a flexible framework for practical scientific document {QA}.},
	number = {{arXiv}:2210.01959},
	publisher = {{arXiv}},
	author = {{McDonald}, Tavish and Tsan, Brian and Saini, Amar and Ordonez, Juanita and Gutierrez, Luis and Nguyen, Phan and Mason, Blake and Ng, Brenda},
	urldate = {2023-08-27},
	date = {2022-12-15},
	eprinttype = {arxiv},
	eprint = {2210.01959 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Background, {QA}, Indexing, Retrieval, Computer Science - Machine Learning, pdfs, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/2TUGS7SU/McDonald et al. - 2022 - Detect, Retrieve, Comprehend A Flexible Framework.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/6Z9ZSPFB/2210.html:text/html},
}


@article{wang_modern_2022,
	title = {Modern Question Answering Datasets and Benchmarks: A Survey},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2206.15030},
	doi = {10.48550/ARXIV.2206.15030},
	shorttitle = {Modern Question Answering Datasets and Benchmarks},
	abstract = {Question Answering ({QA}) is one of the most important natural language processing ({NLP}) tasks. It aims using {NLP} technologies to generate a corresponding answer to a given question based on the massive unstructured corpus. With the development of deep learning, more and more challenging {QA} datasets are being proposed, and lots of new methods for solving them are also emerging. In this paper, we investigate influential {QA} datasets that have been released in the era of deep learning. Specifically, we begin with introducing two of the most common {QA} tasks - textual question answer and visual question answering - separately, covering the most representative datasets, and then give some current challenges of {QA} research.},
	author = {Wang, Zhen},
	urldate = {2023-09-22},
	date = {2022},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Background, Computation and Language (cs.{CL}), Dataset, {FOS}: Computer and information sciences, overview, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/8SLQGJSR/Wang - 2022 - Modern Question Answering Datasets and Benchmarks.pdf:application/pdf},
}


@misc{karpukhin_dense_2020,
	title = {Dense Passage Retrieval for Open-Domain Question Answering},
	url = {http://arxiv.org/abs/2004.04906},
	doi = {10.48550/arXiv.2004.04906},
	abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as {TF}-{IDF} or {BM}25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain {QA} datasets, our dense retriever outperforms a strong Lucene-{BM}25 system largely by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end {QA} system establish new state-of-the-art on multiple open-domain {QA} benchmarks.},
	number = {{arXiv}:2004.04906},
	publisher = {{arXiv}},
	author = {Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
	urldate = {2023-08-27},
	date = {2020-09-30},
	eprinttype = {arxiv},
	eprint = {2004.04906 [cs]},
	keywords = {Background, Computer Science - Computation and Language, groundwork, Indexing, {LLM}, {QA}, Retrieval},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/VP8YAJD5/Karpukhin et al. - 2020 - Dense Passage Retrieval for Open-Domain Question A.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/R25C9NEX/2004.html:text/html},
}

@incollection{tito_document_2021,
	title = {Document Collection Visual Question Answering},
	volume = {12822},
	url = {http://arxiv.org/abs/2104.14336},
	abstract = {Current tasks and methods in Document Understanding aims to process documents as single elements. However, documents are usually organized in collections (historical records, purchase invoices), that provide context useful for their interpretation. To address this problem, we introduce Document Collection Visual Question Answering ({DocCVQA}) a new dataset and related task, where questions are posed over a whole collection of document images and the goal is not only to provide the answer to the given question, but also to retrieve the set of documents that contain the information needed to infer the answer. Along with the dataset we propose a new evaluation metric and baselines which provide further insights to the new dataset and task.},
	pages = {778--792},
	author = {Tito, Rubèn and Karatzas, Dimosthenis and Valveny, Ernest},
	urldate = {2023-09-22},
	date = {2021},
	doi = {10.1007/978-3-030-86331-9_50},
	eprinttype = {arxiv},
	eprint = {2104.14336 [cs]},
	keywords = {Background, Computer Science - Information Retrieval, Indexing, {QA}, {VQA}},
	file = {2104.14336.pdf:/Users/lenert/Zotero/storage/UKBJDGUY/2104.14336.pdf:application/pdf;arXiv Fulltext PDF:/Users/lenert/Zotero/storage/AC44N9FM/Tito et al. - 2021 - Document Collection Visual Question Answering.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/2HDBBTH3/2104.html:text/html},
}

@misc{wang_multi-passage_2019,
	title = {Multi-passage {BERT}: A Globally Normalized {BERT} Model for Open-domain Question Answering},
	url = {http://arxiv.org/abs/1908.08167},
	doi = {10.48550/arXiv.1908.08167},
	shorttitle = {Multi-passage {BERT}},
	abstract = {{BERT} model has been successfully applied to open-domain {QA} tasks. However, previous work trains {BERT} by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage {BERT} model to globally normalize answer scores across all passages of the same question, and this change enables our {QA} model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4\%. By leveraging a passage ranker to select high-quality passages, multi-passage {BERT} gains additional 2\%. Experiments on four standard benchmarks showed that our multi-passage {BERT} outperforms all state-of-the-art models on all benchmarks. In particular, on the {OpenSQuAD} dataset, our model gains 21.4\% {EM} and 21.5\% \$F\_1\$ over all non-{BERT} models, and 5.8\% {EM} and 6.5\% \$F\_1\$ over {BERT}-based models.},
	number = {{arXiv}:1908.08167},
	publisher = {{arXiv}},
	author = {Wang, Zhiguo and Ng, Patrick and Ma, Xiaofei and Nallapati, Ramesh and Xiang, Bing},
	urldate = {2023-09-22},
	date = {2019-10-01},
	eprinttype = {arxiv},
	eprint = {1908.08167 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Indexing, {LLM}, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/MAQMI4J2/Wang et al. - 2019 - Multi-passage BERT A Globally Normalized BERT Mod.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/HLTCZN8I/1908.html:text/html},
}


@misc{mathew_document_2021,
	title = {Document Visual Question Answering Challenge 2020},
	url = {http://arxiv.org/abs/2008.08899},
	doi = {10.48550/arXiv.2008.08899},
	abstract = {This paper presents results of Document Visual Question Answering Challenge organized as part of "Text and Documents in the Deep Learning Era" workshop, in {CVPR} 2020. The challenge introduces a new problem - Visual Question Answering on document images. The challenge comprised two tasks. The first task concerns with asking questions on a single document image. On the other hand, the second task is set as a retrieval task where the question is posed over a collection of images. For the task 1 a new dataset is introduced comprising 50,000 questions-answer(s) pairs defined over 12,767 document images. For task 2 another dataset has been created comprising 20 questions over 14,362 document images which share the same document template.},
	number = {{arXiv}:2008.08899},
	publisher = {{arXiv}},
	author = {Mathew, Minesh and Tito, Ruben and Karatzas, Dimosthenis and Manmatha, R. and Jawahar, C. V.},
	urldate = {2023-09-22},
	date = {2021-07-17},
	eprinttype = {arxiv},
	eprint = {2008.08899 [cs]},
	keywords = {Background, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Indexing, {QA}, {VQA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/6A5UEC82/Mathew et al. - 2021 - Document Visual Question Answering Challenge 2020.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/MYGTHFBZ/2008.html:text/html},
}


@misc{li_dit_2022,
	title = {{DiT}: Self-supervised Pre-training for Document Image Transformer},
	url = {http://arxiv.org/abs/2203.02378},
	doi = {10.48550/arXiv.2203.02378},
	shorttitle = {{DiT}},
	abstract = {Image Transformer has recently achieved significant progress for natural image understanding, either using supervised ({ViT}, {DeiT}, etc.) or self-supervised ({BEiT}, {MAE}, etc.) pre-training techniques. In this paper, we propose {\textbackslash}textbf\{{DiT}\}, a self-supervised pre-trained {\textbackslash}textbf\{D\}ocument {\textbackslash}textbf\{I\}mage {\textbackslash}textbf\{T\}ransformer model using large-scale unlabeled text images for Document {AI} tasks, which is essential since no supervised counterparts ever exist due to the lack of human-labeled document images. We leverage {DiT} as the backbone network in a variety of vision-based Document {AI} tasks, including document image classification, document layout analysis, table detection as well as text detection for {OCR}. Experiment results have illustrated that the self-supervised pre-trained {DiT} model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 \${\textbackslash}rightarrow\$ 92.69), document layout analysis (91.0 \${\textbackslash}rightarrow\$ 94.9), table detection (94.23 \${\textbackslash}rightarrow\$ 96.55) and text detection for {OCR} (93.07 \${\textbackslash}rightarrow\$ 94.29). The code and pre-trained models are publicly available at {\textbackslash}url\{https://aka.ms/msdit\}.},
	number = {{arXiv}:2203.02378},
	publisher = {{arXiv}},
	author = {Li, Junlong and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
	urldate = {2023-09-22},
	date = {2022-07-19},
	eprinttype = {arxiv},
	eprint = {2203.02378 [cs]},
	keywords = {Background, Computer Science - Computer Vision and Pattern Recognition, Indexing, {QA}},
}
@incollection{meuschke_benchmark_2023,
	title = {A Benchmark of {PDF} Information Extraction Tools using a Multi-Task and Multi-Domain Evaluation Framework for Academic Documents},
	volume = {13972},
	url = {http://arxiv.org/abs/2303.09957},
	abstract = {Extracting information from academic {PDF} documents is crucial for numerous indexing, retrieval, and analysis use cases. Choosing the best tool to extract specific content elements is difficult because many, technically diverse tools are available, but recent performance benchmarks are rare. Moreover, such benchmarks typically cover only a few content elements like header metadata or bibliographic references and use smaller datasets from specific academic disciplines. We provide a large and diverse evaluation framework that supports more extraction tasks than most related datasets. Our framework builds upon {DocBank}, a multi-domain dataset of 1.5M annotated content elements extracted from 500K pages of research papers on {arXiv}. Using the new framework, we benchmark ten freely available tools in extracting document metadata, bibliographic references, tables, and other content elements from academic {PDF} documents. {GROBID} achieves the best metadata and reference extraction results, followed by {CERMINE} and Science Parse. For table extraction, Adobe Extract outperforms other tools, even though the performance is much lower than for other content elements. All tools struggle to extract lists, footers, and equations. We conclude that more research on improving and combining tools is necessary to achieve satisfactory extraction quality for most content elements. Evaluation datasets and frameworks like the one we present support this line of research. We make our data and code publicly available to contribute toward this goal.},
	pages = {383--405},
	author = {Meuschke, Norman and Jagdale, Apurva and Spinde, Timo and Mitrovic, Jelena and Gipp, Bela},
	urldate = {2023-09-22},
	date = {2023},
	doi = {10.1007/978-3-031-28032-0_31},
	eprinttype = {arxiv},
	eprint = {2303.09957 [cs]},
	keywords = {Background, Computer Science - Information Retrieval, Indexing, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/P8TZJEYY/Meuschke et al. - 2023 - A Benchmark of PDF Information Extraction Tools us.pdf:application/pdf},
}

@online{noauthor_langchain-ailangchain_nodate,
	title = {langchain-ai/langchain: Building applications with {LLMs} through composability},
	url = {https://github.com/langchain-ai/langchain},
	shorttitle = {https://github.com/langchain-ai/langchain},
	urldate = {2023-09-22},
	langid = {english},
	keywords = {Background, Indexing, {LLM}, Open-Source, {QA}},
	file = {Snapshot:/Users/lenert/Zotero/storage/625ZNSPF/master.html:text/html},
}

@online{noauthor_question_nodate,
	title = {Question Answering {\textbar} Langchain},
	url = {https://python.langchain.com/docs/use_cases/question_answering/},
	abstract = {Open In Collab},
	urldate = {2023-09-24},
	langid = {english},
	keywords = {Background, {LLM}, {QA}, related work},
	file = {Snapshot:/Users/lenert/Zotero/storage/XMNN5KJ9/question_answering.html:text/html},
}


@software{noauthor_chatgpt_2023,
	title = {{ChatGPT} Retrieval Plugin},
	rights = {{MIT}},
	url = {https://github.com/openai/chatgpt-retrieval-plugin},
	abstract = {The {ChatGPT} Retrieval Plugin lets you easily find personal or work documents by asking questions in natural language.},
	publisher = {{OpenAI}},
	urldate = {2023-09-22},
	date = {2023-09-22},
	note = {original-date: 2023-03-23T06:06:22Z},
	keywords = {Background, chatgpt, chatgpt-plugins, Indexing, {LLM}, Open-Source, {QA}},
}


@misc{liu_dense_2021,
	title = {Dense Hierarchical Retrieval for Open-Domain Question Answering},
	url = {http://arxiv.org/abs/2110.15439},
	doi = {10.48550/arXiv.2110.15439},
	abstract = {Dense neural text retrieval has achieved promising results on open-domain Question Answering ({QA}), where latent representations of questions and passages are exploited for maximum inner product search in the retrieval process. However, current dense retrievers require splitting documents into short passages that usually contain local, partial, and sometimes biased context, and highly depend on the splitting process. As a consequence, it may yield inaccurate and misleading hidden representations, thus deteriorating the final retrieval result. In this work, we propose Dense Hierarchical Retrieval ({DHR}), a hierarchical framework that can generate accurate dense representations of passages by utilizing both macroscopic semantics in the document and microscopic semantics specific to each passage. Specifically, a document-level retriever first identifies relevant documents, among which relevant passages are then retrieved by a passage-level retriever. The ranking of the retrieved passages will be further calibrated by examining the document-level relevance. In addition, hierarchical title structure and two negative sampling strategies (i.e., In-Doc and In-Sec negatives) are investigated. We apply {DHR} to large-scale open-domain {QA} datasets. {DHR} significantly outperforms the original dense passage retriever and helps an end-to-end {QA} system outperform the strong baselines on multiple open-domain {QA} benchmarks.},
	number = {{arXiv}:2110.15439},
	publisher = {{arXiv}},
	author = {Liu, Ye and Hashimoto, Kazuma and Zhou, Yingbo and Yavuz, Semih and Xiong, Caiming and Yu, Philip S.},
	urldate = {2023-08-27},
	date = {2021-10-28},
	eprinttype = {arxiv},
	eprint = {2110.15439 [cs]},
	keywords = {Background, Computer Science - Information Retrieval, {QA}, Indexing, Retrieval},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/JWPAQPNM/Liu et al. - 2021 - Dense Hierarchical Retrieval for Open-Domain Quest.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZWUJ8UF5/2110.html:text/html},
}


@misc{thakur_beir_2021,
	title = {{BEIR}: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
	url = {http://arxiv.org/abs/2104.08663},
	doi = {10.48550/arXiv.2104.08663},
	shorttitle = {{BEIR}},
	abstract = {Existing neural information retrieval ({IR}) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution ({OOD}) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-{IR} ({BEIR}), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the {BEIR} benchmark. Our results show {BM}25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. {BEIR} is publicly available at https://github.com/{UKPLab}/beir.},
	number = {{arXiv}:2104.08663},
	publisher = {{arXiv}},
	author = {Thakur, Nandan and Reimers, Nils and Rücklé, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
	urldate = {2023-09-22},
	date = {2021-10-20},
	eprinttype = {arxiv},
	eprint = {2104.08663 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, overview, {QA}, Retrieval},
}


@article{robertson_probabilistic_2009,
	title = {The Probabilistic Relevance Framework: {BM}25 and Beyond},
	volume = {3},
	doi = {10.1561/1500000019},
	shorttitle = {The Probabilistic Relevance Framework},
	abstract = {The Probabilistic Relevance Framework ({PRF}) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, {BM}25. In recent years, research in the {PRF} has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, {BM}25F. This work presents the {PRF} from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, {BM}25 and {BM}25F. It also discusses the relation between the {PRF} and other statistical models for {IR}, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
	pages = {333--389},
	journaltitle = {Foundations and Trends in Information Retrieval},
	shortjournal = {Foundations and Trends in Information Retrieval},
	author = {Robertson, Stephen and Zaragoza, Hugo},
	date = {2009-01-01},
	keywords = {Background, historical, {QA}, Retrieval},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/T6AIFA9G/Robertson und Zaragoza - 2009 - The Probabilistic Relevance Framework BM25 and Be.pdf:application/pdf},
}


@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2023-09-22},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Background, Computer Science - Computation and Language, groundwork, {LLM}, {QA}, Retriever},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/2LWVZYFE/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/RTNVN3RG/1810.html:text/html},
}


@misc{khattab_colbert_2020,
	title = {{ColBERT}: Efficient and Effective Passage Search via Contextualized Late Interaction over {BERT}},
	url = {http://arxiv.org/abs/2004.12832},
	doi = {10.48550/arXiv.2004.12832},
	shorttitle = {{ColBERT}},
	abstract = {Recent progress in Natural Language Understanding ({NLU}) is driving fast-paced advances in Information Retrieval ({IR}), largely owed to fine-tuning deep language models ({LMs}) for document ranking. While remarkably effective, the ranking models based on these {LMs} increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present {ColBERT}, a novel ranking model that adapts deep {LMs} (in particular, {BERT}) for efficient retrieval. {ColBERT} introduces a late interaction architecture that independently encodes the query and the document using {BERT} and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, {ColBERT} can leverage the expressiveness of deep {LMs} while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, {ColBERT}'s pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate {ColBERT} using two recent passage search datasets. Results show that {ColBERT}'s effectiveness is competitive with existing {BERT}-based models (and outperforms every non-{BERT} baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer {FLOPs} per query.},
	number = {{arXiv}:2004.12832},
	publisher = {{arXiv}},
	author = {Khattab, Omar and Zaharia, Matei},
	urldate = {2023-09-22},
	date = {2020-06-04},
	eprinttype = {arxiv},
	eprint = {2004.12832 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, {LLM}, {QA}, Retriever},
}


@misc{yang_hotpotqa_2018,
	title = {{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},
	url = {http://arxiv.org/abs/1809.09600},
	doi = {10.48550/arXiv.1809.09600},
	shorttitle = {{HotpotQA}},
	abstract = {Existing question answering ({QA}) datasets fail to train {QA} systems to perform complex reasoning and provide explanations for answers. We introduce {HotpotQA}, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing {QA} systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test {QA} systems' ability to extract relevant facts and perform necessary comparison. We show that {HotpotQA} is challenging for the latest {QA} systems, and the supporting facts enable models to improve performance and make explainable predictions.},
	number = {{arXiv}:1809.09600},
	publisher = {{arXiv}},
	author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
	urldate = {2023-09-23},
	date = {2018-09-25},
	eprinttype = {arxiv},
	eprint = {1809.09600 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Dataset, {QA}, Retrieval},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/STYG4CHM/Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZRU8LJS5/1809.html:text/html},
}


@misc{zhang_beam_2023,
	title = {Beam Retrieval: General End-to-End Retrieval for Multi-Hop Question Answering},
	url = {http://arxiv.org/abs/2308.08973},
	doi = {10.48550/arXiv.2308.08973},
	shorttitle = {Beam Retrieval},
	abstract = {Multi-hop {QA} involves finding multiple relevant passages and step-by-step reasoning to answer complex questions. While previous approaches have developed retrieval modules for selecting relevant passages, they face challenges in scenarios beyond two hops, owing to the limited performance of one-step methods and the failure of two-step methods when selecting irrelevant passages in earlier stages. In this work, we introduce Beam Retrieval, a general end-to-end retrieval framework for multi-hop {QA}. This approach maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. Moreover, Beam Retrieval jointly optimizes an encoder and two classification heads by minimizing the combined loss across all hops. To establish a complete {QA} system, we incorporate a supervised reader or a zero-shot {GPT}-3.5. Experimental results demonstrate that Beam Retrieval achieves a nearly 50\% improvement compared with baselines on challenging {MuSiQue}-Ans, and it also surpasses all previous retrievers on {HotpotQA} and 2WikiMultiHopQA. Providing high-quality context, Beam Retrieval helps our supervised reader achieve new state-of-the-art performance and substantially improves (up to 28.8 points) the {QA} performance of zero-shot {GPT}-3.5.},
	number = {{arXiv}:2308.08973},
	publisher = {{arXiv}},
	author = {Zhang, Jiahao and Zhang, Haiyang and Zhang, Dongmei and Liu, Yong and Huang, Shen},
	urldate = {2023-09-23},
	date = {2023-08-17},
	eprinttype = {arxiv},
	eprint = {2308.08973 [cs]},
	keywords = {Background, Computer Science - Computation and Language, {LLM}, {QA}, Retrieval},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/CRKYSGUB/Zhang et al. - 2023 - Beam Retrieval General End-to-End Retrieval for M.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/HKGI5N7M/2308.html:text/html},
}


@inproceedings{he_deberta_2020,
	title = {{DEBERTA}: {DECODING}-{ENHANCED} {BERT} {WITH} {DISENTANGLED} {ATTENTION}},
	url = {https://openreview.net/forum?id=XPZIaotutsD},
	shorttitle = {{DEBERTA}},
	abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing ({NLP}) tasks. In this paper we propose a new model architecture {DeBERTa} (Decoding-enhanced {BERT} with disentangled attention) that improves the {BERT} and {RoBERTa} models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models’ generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understand({NLU}) and natural langauge generation ({NLG}) downstream tasks. Compared to {RoBERTa}-Large, a {DeBERTa} model trained on half of the training data performs consistently better on a wide range of {NLP} tasks, achieving improvements on {MNLI} by +0.9\% (90.2\% vs. 91.1\%), on {SQuAD} v2.0 by +2.3\% (88.4\% vs. 90.7\%) and {RACE} by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up {DeBERTa} by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single {DeBERTa} model surpass the human performance on the {SuperGLUE} benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble {DeBERTa} model sits atop the {SuperGLUE} leaderboard as of January 6, 2021, outperforming the human baseline by a decent margin (90.3 versus 89.8). The pre-trained {DeBERTa} models and the source code were released at: https://github.com/microsoft/{DeBERTa}.},
	eventtitle = {International Conference on Learning Representations},
	author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
	urldate = {2023-09-23},
	date = {2020-10-02},
	langid = {english},
	keywords = {Background, {LLM}, {QA}, Retrieval},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/ICFYT6KS/He et al. - 2020 - DEBERTA DECODING-ENHANCED BERT WITH DISENTANGLED .pdf:application/pdf},
}


@misc{luo_choose_2022,
	title = {Choose Your {QA} Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering},
	url = {http://arxiv.org/abs/2203.07522},
	doi = {10.48550/arXiv.2203.07522},
	shorttitle = {Choose Your {QA} Model Wisely},
	abstract = {While both extractive and generative readers have been successfully applied to the Question Answering ({QA}) task, little attention has been paid toward the systematic comparison of them. Characterizing the strengths and weaknesses of the two readers is crucial not only for making a more informed reader selection in practice but also for developing a deeper understanding to foster further research on improving readers in a principled manner. Motivated by this goal, we make the first attempt to systematically study the comparison of extractive and generative readers for question answering. To be aligned with the state-of-the-art, we explore nine transformer-based large pre-trained language models ({PrLMs}) as backbone architectures. Furthermore, we organize our findings under two main categories: (1) keeping the architecture invariant, and (2) varying the underlying {PrLMs}. Among several interesting findings, it is important to highlight that (1) the generative readers perform better in long context {QA}, (2) the extractive readers perform better in short context while also showing better out-of-domain generalization, and (3) the encoder of encoder-decoder {PrLMs} (e.g., T5) turns out to be a strong extractive reader and outperforms the standard choice of encoder-only {PrLMs} (e.g., {RoBERTa}). We also study the effect of multi-task learning on the two types of readers varying the underlying {PrLMs} and perform qualitative and quantitative diagnosis to provide further insights into future directions in modeling better readers.},
	number = {{arXiv}:2203.07522},
	publisher = {{arXiv}},
	author = {Luo, Man and Hashimoto, Kazuma and Yavuz, Semih and Liu, Zhiwei and Baral, Chitta and Zhou, Yingbo},
	urldate = {2023-09-23},
	date = {2022-03-14},
	eprinttype = {arxiv},
	eprint = {2203.07522 [cs]},
	keywords = {Background, Computer Science - Computation and Language, {QA}, Reader},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/FTI4FGHF/Luo et al. - 2022 - Choose Your QA Model Wisely A Systematic Study of.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/43WS28GI/2203.html:text/html},
}


@misc{liu_roberta_2019,
	title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	shorttitle = {{RoBERTa}},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	number = {{arXiv}:1907.11692},
	publisher = {{arXiv}},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	urldate = {2023-09-23},
	date = {2019-07-26},
	eprinttype = {arxiv},
	eprint = {1907.11692 [cs]},
	keywords = {Background, Computer Science - Computation and Language, {LLM}, {QA}, Reader},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/T28KCBPX/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/4WZXBN7L/1907.html:text/html},
}


@misc{raffel_exploring_2023,
	title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing ({NLP}). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for {NLP} by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for {NLP}, we release our data set, pre-trained models, and code.},
	number = {{arXiv}:1910.10683},
	publisher = {{arXiv}},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	urldate = {2023-09-23},
	date = {2023-09-19},
	eprinttype = {arxiv},
	eprint = {1910.10683 [cs, stat]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, {LLM}, {QA}, Reader, Statistics - Machine Learning},
}


@misc{lewis_bart_2019,
	title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
	url = {http://arxiv.org/abs/1910.13461},
	doi = {10.48550/arXiv.1910.13461},
	shorttitle = {{BART}},
	abstract = {We present {BART}, a denoising autoencoder for pretraining sequence-to-sequence models. {BART} is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing {BERT} (due to the bidirectional encoder), {GPT} (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. {BART} is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of {RoBERTa} with comparable training resources on {GLUE} and {SQuAD}, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 {ROUGE}. {BART} also provides a 1.1 {BLEU} increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the {BART} framework, to better measure which factors most influence end-task performance.},
	number = {{arXiv}:1910.13461},
	publisher = {{arXiv}},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	urldate = {2023-09-23},
	date = {2019-10-29},
	eprinttype = {arxiv},
	eprint = {1910.13461 [cs, stat]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, {LLM}, {QA}, Reader, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/HL6T5MMZ/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/7LDXX3A9/1910.html:text/html},
}


@misc{pereira_visconde_2022,
	title = {Visconde: Multi-document {QA} with {GPT}-3 and Neural Reranking},
	url = {http://arxiv.org/abs/2212.09656},
	doi = {10.48550/arXiv.2212.09656},
	shorttitle = {Visconde},
	abstract = {This paper proposes a question-answering system that can answer questions whose supporting evidence is spread over multiple (potentially long) documents. The system, called Visconde, uses a three-step pipeline to perform the task: decompose, retrieve, and aggregate. The first step decomposes the question into simpler questions using a few-shot large language model ({LLM}). Then, a state-of-the-art search engine is used to retrieve candidate passages from a large collection for each decomposed question. In the final step, we use the {LLM} in a few-shot setting to aggregate the contents of the passages into the final answer. The system is evaluated on three datasets: {IIRC}, Qasper, and {StrategyQA}. Results suggest that current retrievers are the main bottleneck and that readers are already performing at the human level as long as relevant passages are provided. The system is also shown to be more effective when the model is induced to give explanations before answering a question. Code is available at {\textbackslash}url\{https://github.com/neuralmind-ai/visconde\}.},
	number = {{arXiv}:2212.09656},
	publisher = {{arXiv}},
	author = {Pereira, Jayr and Fidalgo, Robson and Lotufo, Roberto and Nogueira, Rodrigo},
	urldate = {2023-09-23},
	date = {2022-12-19},
	eprinttype = {arxiv},
	eprint = {2212.09656 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, {LLM}, {QA}, Reader},
}


@inproceedings{serban_generating_2016,
	location = {Berlin, Germany},
	title = {Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus},
	url = {https://aclanthology.org/P16-1056},
	doi = {10.18653/v1/P16-1056},
	shorttitle = {Generating Factoid Questions With Recurrent Neural Networks},
	eventtitle = {{ACL} 2016},
	pages = {588--598},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Serban, Iulian Vlad and García-Durán, Alberto and Gulcehre, Caglar and Ahn, Sungjin and Chandar, Sarath and Courville, Aaron and Bengio, Yoshua},
	urldate = {2023-09-23},
	date = {2016-08},
	keywords = {Background, Limits, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/F3HMB6DH/Serban et al. - 2016 - Generating Factoid Questions With Recurrent Neural.pdf:application/pdf},
}


@article{kwiatkowski_natural_2019,
	title = {Natural Questions: A Benchmark for Question Answering Research},
	volume = {7},
	url = {https://aclanthology.org/Q19-1026},
	doi = {10.1162/tacl_a_00276},
	shorttitle = {Natural Questions},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	pages = {452--466},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	urldate = {2023-09-23},
	date = {2019},
	note = {Place: Cambridge, {MA}
Publisher: {MIT} Press},
	keywords = {Background, Dataset, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/4DA5FW2C/Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answer.pdf:application/pdf},
}


@misc{ding_v-doc_2022,
	title = {V-Doc : Visual questions answers with Documents},
	url = {http://arxiv.org/abs/2205.13724},
	doi = {10.48550/arXiv.2205.13724},
	shorttitle = {V-Doc},
	abstract = {We propose V-Doc, a question-answering tool using document images and {PDF}, mainly for researchers and general non-deep learning experts looking to generate, process, and understand the document visual question answering tasks. The V-Doc supports generating and using both extractive and abstractive question-answer pairs using documents images. The extractive {QA} selects a subset of tokens or phrases from the document contents to predict the answers, while the abstractive {QA} recognises the language in the content and generates the answer based on the trained model. Both aspects are crucial to understanding the documents, especially in an image format. We include a detailed scenario of question generation for the abstractive {QA} task. V-Doc supports a wide range of datasets and models, and is highly extensible through a declarative, framework-agnostic platform.},
	number = {{arXiv}:2205.13724},
	publisher = {{arXiv}},
	author = {Ding, Yihao and Huang, Zhe and Wang, Runlin and Zhang, Yanhang and Chen, Xianru and Ma, Yuzhong and Chung, Hyunsuk and Han, Soyeon Caren},
	urldate = {2023-09-24},
	date = {2022-05-30},
	eprinttype = {arxiv},
	eprint = {2205.13724 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, {QA}, related work, {VQA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/HQ9KE4KM/Ding et al. - 2022 - V-Doc  Visual questions answers with Documents.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/CUNC2YQQ/2205.html:text/html},
}


@misc{neelakantan_text_2022,
	title = {Text and Code Embeddings by Contrastive Pre-Training},
	url = {http://arxiv.org/abs/2201.10005},
	doi = {10.48550/arXiv.2201.10005},
	abstract = {Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4\% and 1.8\% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4\%, 14.7\%, and 10.6\% over previous best unsupervised methods on {MSMARCO}, Natural Questions and {TriviaQA} benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8\% relative improvement over prior best work on code search.},
	number = {{arXiv}:2201.10005},
	publisher = {{arXiv}},
	author = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and Heidecke, Johannes and Shyam, Pranav and Power, Boris and Nekoul, Tyna Eloundou and Sastry, Girish and Krueger, Gretchen and Schnurr, David and Such, Felipe Petroski and Hsu, Kenny and Thompson, Madeleine and Khan, Tabarak and Sherbakov, Toki and Jang, Joanne and Welinder, Peter and Weng, Lilian},
	urldate = {2023-09-24},
	date = {2022-01-24},
	eprinttype = {arxiv},
	eprint = {2201.10005 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, {LLM}, {QA}, related work, Retrieval},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7FD3EEJ5/Neelakantan et al. - 2022 - Text and Code Embeddings by Contrastive Pre-Traini.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/7J8TTGML/2201.html:text/html},
}

@misc{ni_large_2021,
	title = {Large Dual Encoders Are Generalizable Retrievers},
	url = {http://arxiv.org/abs/2112.07899},
	doi = {10.48550/arXiv.2112.07899},
	abstract = {It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited to make dual encoders an effective retrieval model for out-of-domain generalization. In this paper, we challenge this belief by scaling up the size of the dual encoder model \{{\textbackslash}em while keeping the bottleneck embedding size fixed.\} With multi-stage training, surprisingly, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. Experimental results show that our dual encoders, {\textbackslash}textbf\{G\}eneralizable {\textbackslash}textbf\{T\}5-based dense {\textbackslash}textbf\{R\}etrievers ({GTR}), outperform \%{ColBERT}{\textasciitilde}{\textbackslash}cite\{khattab2020colbert\} and existing sparse and dense retrievers on the {BEIR} dataset{\textasciitilde}{\textbackslash}cite\{thakur2021beir\} significantly. Most surprisingly, our ablation study finds that {GTR} is very data efficient, as it only needs 10{\textbackslash}\% of {MS} Marco supervised data to achieve the best out-of-domain performance. All the {GTR} models are released at https://tfhub.dev/google/collections/gtr/1.},
	number = {{arXiv}:2112.07899},
	publisher = {{arXiv}},
	author = {Ni, Jianmo and Qu, Chen and Lu, Jing and Dai, Zhuyun and Abrego, Gustavo Hernandez and Ma, Ji and Zhao, Vincent Y. and Luan, Yi and Hall, Keith B. and Chang, Ming-Wei and Yang, Yinfei},
	urldate = {2023-09-24},
	date = {2021-12-15},
	eprinttype = {arxiv},
	eprint = {2112.07899 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, {OOD}, {QA}, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/TFLHD85U/Ni et al. - 2021 - Large Dual Encoders Are Generalizable Retrievers.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/8CB8QQ5J/2112.html:text/html},
}

@inproceedings{izacard_leveraging_2021,
	location = {Online},
	title = {Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
	url = {https://aclanthology.org/2021.eacl-main.74},
	doi = {10.18653/v1/2021.eacl-main.74},
	abstract = {Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and {TriviaQA} open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.},
	eventtitle = {{EACL} 2021},
	pages = {874--880},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	publisher = {Association for Computational Linguistics},
	author = {Izacard, Gautier and Grave, Edouard},
	urldate = {2023-09-24},
	date = {2021-04},
	keywords = {Background, {LLM}, {QA}, related work},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/8RPVFNAT/Izacard und Grave - 2021 - Leveraging Passage Retrieval with Generative Model.pdf:application/pdf},
}


@misc{lyu_extending_2022,
	title = {Extending the Scope of Out-of-Domain: Examining {QA} models in multiple subdomains},
	url = {http://arxiv.org/abs/2204.04534},
	doi = {10.48550/arXiv.2204.04534},
	shorttitle = {Extending the Scope of Out-of-Domain},
	abstract = {Past works that investigate out-of-domain performance of {QA} systems have mainly focused on general domains (e.g. news domain, wikipedia domain), underestimating the importance of subdomains defined by the internal characteristics of {QA} datasets. In this paper, we extend the scope of "out-of-domain" by splitting {QA} examples into different subdomains according to their several internal characteristics including question type, text length, answer position. We then examine the performance of {QA} systems trained on the data from different subdomains. Experimental results show that the performance of {QA} systems can be significantly reduced when the train data and test data come from different subdomains. These results question the generalizability of current {QA} systems in multiple subdomains, suggesting the need to combat the bias introduced by the internal characteristics of {QA} datasets.},
	number = {{arXiv}:2204.04534},
	publisher = {{arXiv}},
	author = {Lyu, Chenyang and Foster, Jennifer and Graham, Yvette},
	urldate = {2023-09-24},
	date = {2022-04-09},
	eprinttype = {arxiv},
	eprint = {2204.04534 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, {OOD}, overview, {QA}, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/Z46K4ZJF/Lyu et al. - 2022 - Extending the Scope of Out-of-Domain Examining QA.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZI8NHCAV/2204.html:text/html},
}


@misc{dai_promptagator_2022,
	title = {Promptagator: Few-shot Dense Retrieval From 8 Examples},
	url = {http://arxiv.org/abs/2209.11755},
	doi = {10.48550/arXiv.2209.11755},
	shorttitle = {Promptagator},
	abstract = {Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models ({LLM}) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by {LLM}'s generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples \{without\} using Natural Questions or {MS} {MARCO} to train \%question generators or dual encoders. Surprisingly, {LLM} prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on {MS} {MARCO} like {ColBERT} v2 by more than 1.2 {nDCG} on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point {nDCG} improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.},
	number = {{arXiv}:2209.11755},
	publisher = {{arXiv}},
	author = {Dai, Zhuyun and Zhao, Vincent Y. and Ma, Ji and Luan, Yi and Ni, Jianmo and Lu, Jing and Bakalov, Anton and Guu, Kelvin and Hall, Keith B. and Chang, Ming-Wei},
	urldate = {2023-09-24},
	date = {2022-09-23},
	eprinttype = {arxiv},
	eprint = {2209.11755 [cs]},
	keywords = {Computer Science - Computation and Language, Background, Computer Science - Information Retrieval, {QA}, related work, {OOD}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/6DV4A7KK/Dai et al. - 2022 - Promptagator Few-shot Dense Retrieval From 8 Exam.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ACSLQCES/2209.html:text/html},
}


@misc{sachan_questions_2023,
	title = {Questions Are All You Need to Train a Dense Passage Retriever},
	url = {http://arxiv.org/abs/2206.10658},
	doi = {10.48550/arXiv.2206.10658},
	abstract = {We introduce {ART}, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open {QA}, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. {ART}, in contrast, only requires access to unpaired inputs and outputs (e.g. questions and potential answer documents). It uses a new document-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence documents, and (2) the documents are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both document and question encoders, which can be later incorporated into complete Open {QA} systems without any further finetuning. Extensive experiments demonstrate that {ART} obtains state-of-the-art results on multiple {QA} retrieval benchmarks with only generic initialization from a pre-trained language model, removing the need for labeled data and task-specific losses.},
	number = {{arXiv}:2206.10658},
	publisher = {{arXiv}},
	author = {Sachan, Devendra Singh and Lewis, Mike and Yogatama, Dani and Zettlemoyer, Luke and Pineau, Joelle and Zaheer, Manzil},
	urldate = {2023-09-24},
	date = {2023-04-02},
	eprinttype = {arxiv},
	eprint = {2206.10658 [cs]},
	keywords = {Computer Science - Computation and Language, Background, Computer Science - Information Retrieval, {QA}, related work, {OOD}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/F7ZWF6MC/Sachan et al. - 2023 - Questions Are All You Need to Train a Dense Passag.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/47359T5Z/2206.html:text/html},
}


@inproceedings{chen_improving_2021,
	title = {Improving Out-of-Domain Question Answering with Mixture of Experts},
	url = {https://www.semanticscholar.org/paper/Improving-Out-of-Domain-Question-Answering-with-of-Chen/646be1b52a97c415f299eb333f2b97d51421be46#related-papers},
	abstract = {Question answering ({QA}) is an important problem with numerous application in real life. Sometimes, the resource of certain {QA} task is limited. This work aims to build a robust {QA} system that can generalize to novel {QA} tasks with few examples and gradient steps. We propose a Mixture-of-Experts({MoE}) style training framework, and use meta-learning methods for domain adaptation. We also explored data augmentation techniques, and successfully improve out-of-domain {QA} performance of baseline models on F-1 score from 50.81 to 53.84 and exact match ({EM}) score from 34.82 to 39.27. Our approach achieves a F-1 score of 60.8 and {EM} score of 42.2 on the out-of-domain {QA} testing leaderboard.},
	author = {Chen, Haofeng},
	urldate = {2023-09-24},
	date = {2021},
	keywords = {Background, {QA}, related work, {OOD}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/89BQMZAT/Chen - 2021 - Improving Out-of-Domain Question Answering with Mi.pdf:application/pdf},
}

@misc{gholami_zero-shot_2021,
	title = {Zero-Shot Open-Book Question Answering},
	url = {http://arxiv.org/abs/2111.11520},
	doi = {10.48550/arXiv.2111.11520},
	abstract = {Open book question answering is a subset of question answering tasks where the system aims to find answers in a given set of documents (open-book) and common knowledge about a topic. This article proposes a solution for answering natural language questions from a corpus of Amazon Web Services ({AWS}) technical documents with no domain-specific labeled data (zero-shot). These questions can have yes-no-none answers, short answers, long answers, or any combination of the above. This solution comprises a two-step architecture in which a retriever finds the right document and an extractor finds the answers in the retrieved document. We are introducing a new test dataset for open-book {QA} based on real customer questions on {AWS} technical documentation. After experimenting with several information retrieval systems and extractor models based on extractive language models, the solution attempts to find the yes-no-none answers and text answers in the same pass. The model is trained on the The Stanford Question Answering Dataset - {SQuAD} (Rajpurkaret al., 2016) and Natural Questions (Kwiatkowski et al., 2019) datasets. We were able to achieve 49\% F1 and 39\% exact match score ({EM}) end-to-end with no domain-specific training.},
	number = {{arXiv}:2111.11520},
	publisher = {{arXiv}},
	author = {Gholami, Sia and Noori, Mehdi},
	urldate = {2023-09-24},
	date = {2021-11-22},
	eprinttype = {arxiv},
	eprint = {2111.11520 [cs]},
	keywords = {Computer Science - Computation and Language, Background, Computer Science - Information Retrieval, {QA}, Computer Science - Machine Learning, related work, {OOD}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/X3RMPTIS/Gholami und Noori - 2021 - Zero-Shot Open-Book Question Answering.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/H8VYJZPG/2111.html:text/html},
}


@misc{khashabi_unifiedqa-v2_2022,
	title = {{UnifiedQA}-v2: Stronger Generalization via Broader Cross-Format Training},
	url = {http://arxiv.org/abs/2202.12359},
	doi = {10.48550/arXiv.2202.12359},
	shorttitle = {{UnifiedQA}-v2},
	abstract = {We present {UnifiedQA}-v2, a {QA} model built with the same process as {UnifiedQA}, except that it utilizes more supervision -- roughly 3x the number of datasets used for {UnifiedQA}. This generally leads to better in-domain and cross-domain results.},
	number = {{arXiv}:2202.12359},
	publisher = {{arXiv}},
	author = {Khashabi, Daniel and Kordi, Yeganeh and Hajishirzi, Hannaneh},
	urldate = {2023-09-23},
	date = {2022-02-23},
	eprinttype = {arxiv},
	eprint = {2202.12359 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Background, {QA}, Reader, Open-Source, Model},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/ZG3ZNYQC/Khashabi et al. - 2022 - UnifiedQA-v2 Stronger Generalization via Broader .pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/UVT95SB9/2202.html:text/html},
}


@misc{johnson_billion-scale_2017,
	title = {Billion-scale similarity search with {GPUs}},
	url = {http://arxiv.org/abs/1702.08734},
	doi = {10.48550/arXiv.1702.08734},
	abstract = {Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing {GPUs} for this task. While {GPUs} excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a design for k-selection that operates at up to 55\% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior {GPU} state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-{NN} graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X {GPUs}. We have open-sourced our approach for the sake of comparison and reproducibility.},
	number = {{arXiv}:1702.08734},
	publisher = {{arXiv}},
	author = {Johnson, Jeff and Douze, Matthijs and Jégou, Hervé},
	urldate = {2023-09-24},
	date = {2017-02-28},
	eprinttype = {arxiv},
	eprint = {1702.08734 [cs]},
	keywords = {Background, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Data Structures and Algorithms, Computer Science - Databases, Computer Science - Information Retrieval, Limits, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/K3BW38BG/Johnson et al. - 2017 - Billion-scale similarity search with GPUs.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZVZE9GC5/1702.html:text/html},
}


@misc{reddy_synthetic_2022,
	title = {Synthetic Target Domain Supervision for Open Retrieval {QA}},
	url = {http://arxiv.org/abs/2204.09248},
	doi = {10.48550/arXiv.2204.09248},
	abstract = {Neural passage retrieval is a new and promising approach in open retrieval question answering. In this work, we stress-test the Dense Passage Retriever ({DPR}) -- a state-of-the-art ({SOTA}) open domain neural retrieval model -- on closed and specialized target domains such as {COVID}-19, and find that it lags behind standard {BM}25 in this important real-world setting. To make {DPR} more robust under domain shift, we explore its fine-tuning with synthetic training examples, which we generate from unlabeled target domain text using a text-to-text generator. In our experiments, this noisy but fully automated target domain supervision gives {DPR} a sizable advantage over {BM}25 in out-of-domain settings, making it a more viable model in practice. Finally, an ensemble of {BM}25 and our improved {DPR} model yields the best results, further pushing the {SOTA} for open retrieval {QA} on multiple out-of-domain test sets.},
	number = {{arXiv}:2204.09248},
	publisher = {{arXiv}},
	author = {Reddy, Revanth Gangi and Iyer, Bhavani and Sultan, Md Arafat and Zhang, Rong and Sil, Avirup and Castelli, Vittorio and Florian, Radu and Roukos, Salim},
	urldate = {2023-09-24},
	date = {2022-04-20},
	eprinttype = {arxiv},
	eprint = {2204.09248 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, {OOD}, {QA}, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/KA87P9I9/Reddy et al. - 2022 - Synthetic Target Domain Supervision for Open Retri.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/9S585Y9E/2204.html:text/html},
}


@inproceedings{muller_bioasq_2015,
	location = {Cham},
	title = {{BioASQ}: A Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering},
	volume = {9059},
	isbn = {978-3-319-24470-9 978-3-319-24471-6},
	url = {http://link.springer.com/10.1007/978-3-319-24471-6_3},
	doi = {10.1007/978-3-319-24471-6_3},
	shorttitle = {{BioASQ}},
	abstract = {{BioASQ} is a series of challenges that aims to assess the performance of information systems in supporting two tasks that are central to the biomedical question answering process: a the indexing of large volumes of unlabelled data, primarily scientific articles, with biomedical concepts, b the processing of biomedical questions and the generation of answers and supporting material. In this paper, the main results of the first two {BioASQ} challenges are presented.},
	pages = {26--39},
	publisher = {Springer International Publishing},
	author = {Balikas, Georgios and Krithara, Anastasia and Partalas, Ioannis and Paliouras, George},
	editor = {Müller, Henning and Jimenez Del Toro, Oscar Alfonso and Hanbury, Allan and Langs, Georg and Foncubierta Rodriguez, Antonio},
	urldate = {2023-09-24},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-24471-6_3},
	note = {Book Title: Multimodal Retrieval in the Medical Domain
Series Title: Lecture Notes in Computer Science},
	keywords = {Background, dataset, {QA}, related work},
}

@article{gururangan_dont_2020,
	title = {Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
	url = {https://www.aclweb.org/anthology/2020.acl-main.740},
	doi = {10.18653/v1/2020.acl-main.740},
	shorttitle = {Don't Stop Pretraining},
	abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's {NLP}. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
	pages = {8342--8360},
	journaltitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	author = {Gururangan, Suchin and Marasovic, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
	urldate = {2023-09-24},
	date = {2020},
	langid = {english},
	note = {Conference Name: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
Place: Online
Publisher: Association for Computational Linguistics},
	keywords = {Background, {OOD}, {QA}, related work},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/6KXDI3ZI/Gururangan et al. - 2020 - Don't Stop Pretraining Adapt Language Models to D.pdf:application/pdf},
}
