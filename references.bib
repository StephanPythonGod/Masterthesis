
@misc{zhu_retrieving_2021,
	title = {Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering},
	url = {http://arxiv.org/abs/2101.00774},
	doi = {10.48550/arXiv.2101.00774},
	shorttitle = {Retrieving and Reading},
	abstract = {Open-domain Question Answering ({OpenQA}) is an important task in Natural Language Processing ({NLP}), which aims to answer a question in the form of natural language based on large-scale unstructured documents. Recently, there has been a surge in the amount of research literature on {OpenQA}, particularly on techniques that integrate with neural Machine Reading Comprehension ({MRC}). While these research works have advanced performance to new heights on benchmark datasets, they have been rarely covered in existing surveys on {QA} systems. In this work, we review the latest research trends in {OpenQA}, with particular attention to systems that incorporate neural {MRC} techniques. Specifically, we begin with revisiting the origin and development of {OpenQA} systems. We then introduce modern {OpenQA} architecture named "Retriever-Reader" and analyze the various systems that follow this architecture as well as the specific techniques adopted in each of the components. We then discuss key challenges to developing {OpenQA} systems and offer an analysis of benchmarks that are commonly used. We hope our work would enable researchers to be informed of the recent advancement and also the open challenges in {OpenQA} research, so as to stimulate further progress in this field.},
	number = {{arXiv}:2101.00774},
	publisher = {{arXiv}},
	author = {Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
	urldate = {2023-08-27},
	date = {2021-05-08},
	eprinttype = {arxiv},
	eprint = {2101.00774 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Example Architectures, overview, {QA}, {RRS}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7JSH329N/Zhu et al. - 2021 - Retrieving and Reading A Comprehensive Survey on .pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/6WWFWPPE/2101.html:text/html},
}

@misc{zamani_conversational_2023,
	title = {Conversational Information Seeking},
	url = {http://arxiv.org/abs/2201.08808},
	doi = {10.48550/arXiv.2201.08808},
	abstract = {Conversational information seeking ({CIS}) is concerned with a sequence of interactions between one or more users and an information system. Interactions in {CIS} are primarily based on natural language dialogue, while they may include other types of interactions, such as click, touch, and body gestures. This monograph provides a thorough overview of {CIS} definitions, applications, interactions, interfaces, design, implementation, and evaluation. This monograph views {CIS} applications as including conversational search, conversational question answering, and conversational recommendation. Our aim is to provide an overview of past research related to {CIS}, introduce the current state-of-the-art in {CIS}, highlight the challenges still being faced in the community. and suggest future directions.},
	number = {{arXiv}:2201.08808},
	publisher = {{arXiv}},
	author = {Zamani, Hamed and Trippas, Johanne R. and Dalton, Jeff and Radlinski, Filip},
	urldate = {2023-08-27},
	date = {2023-01-25},
	eprinttype = {arxiv},
	eprint = {2201.08808 [cs]},
	keywords = {Background, {CIS}, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval, overview, {QA}, {SvD}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7WFGQWLT/Zamani et al. - 2023 - Conversational Information Seeking.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/MBB85CRQ/2201.html:text/html},
}

@misc{gao_neural_2022,
	title = {Neural Approaches to Conversational Information Retrieval},
	url = {http://arxiv.org/abs/2201.05176},
	doi = {10.48550/arXiv.2201.05176},
	abstract = {A conversational information retrieval ({CIR}) system is an information retrieval ({IR}) system with a conversational interface which allows users to interact with the system to seek information via multi-turn conversations of natural language, in spoken or written form. Recent progress in deep learning has brought tremendous improvements in natural language processing ({NLP}) and conversational {AI}, leading to a plethora of commercial conversational services that allow naturally spoken and typed interaction, increasing the need for more human-centric interactions in {IR}. As a result, we have witnessed a resurgent interest in developing modern {CIR} systems in both research communities and industry. This book surveys recent advances in {CIR}, focusing on neural approaches that have been developed in the last few years. This book is based on the authors' tutorial at {SIGIR}'2020 (Gao et al., 2020b), with {IR} and {NLP} communities as the primary target audience. However, audiences with other background, such as machine learning and human-computer interaction, will also find it an accessible introduction to {CIR}. We hope that this book will prove a valuable resource for students, researchers, and software developers. This manuscript is a working draft. Comments are welcome.},
	number = {{arXiv}:2201.05176},
	publisher = {{arXiv}},
	author = {Gao, Jianfeng and Xiong, Chenyan and Bennett, Paul and Craswell, Nick},
	urldate = {2023-08-27},
	date = {2022-01-13},
	eprinttype = {arxiv},
	eprint = {2201.05176 [cs]},
	keywords = {Background, {CIS}, Computer Science - Computation and Language, Computer Science - Information Retrieval, overview, {QA}, {SvD}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/RZ38AQSJ/Gao et al. - 2022 - Neural Approaches to Conversational Information Re.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/5UH2JCTT/2201.html:text/html},
}

@misc{pandya_question_2021,
	title = {Question Answering Survey: Directions, Challenges, Datasets, Evaluation Matrices},
	url = {http://arxiv.org/abs/2112.03572},
	doi = {10.48550/arXiv.2112.03572},
	shorttitle = {Question Answering Survey},
	abstract = {The usage and amount of information available on the internet increase over the past decade. This digitization leads to the need for automated answering system to extract fruitful information from redundant and transitional knowledge sources. Such systems are designed to cater the most prominent answer from this giant knowledge source to the user query using natural language understanding ({NLU}) and thus eminently depends on the Question-answering({QA}) field. Question answering involves but not limited to the steps like mapping of user question to pertinent query, retrieval of relevant information, finding the best suitable answer from the retrieved information etc. The current improvement of deep learning models evince compelling performance improvement in all these tasks. In this review work, the research directions of {QA} field are analyzed based on the type of question, answer type, source of evidence-answer, and modeling approach. This detailing followed by open challenges of the field like automatic question generation, similarity detection and, low resource availability for a language. In the end, a survey of available datasets and evaluation measures is presented.},
	number = {{arXiv}:2112.03572},
	publisher = {{arXiv}},
	author = {Pandya, Hariom A. and Bhatt, Brijesh S.},
	urldate = {2023-09-19},
	date = {2021-12-07},
	eprinttype = {arxiv},
	eprint = {2112.03572 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval, Computer Science - Machine Learning, overview, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/88TJBBSV/Pandya und Bhatt - 2021 - Question Answering Survey Directions, Challenges,.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/87EC3RU7/2112.html:text/html},
}

@article{hao_recent_2022,
	title = {Recent progress in leveraging deep learning methods for question answering},
	volume = {34},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-021-06748-3},
	doi = {10.1007/s00521-021-06748-3},
	abstract = {Question answering, serving as one of important tasks in natural language processing, enables machines to understand questions in natural language and answer the questions concisely. From web search to expert systems, question answering systems are widely applied to various domains in assisting information seeking. Deep learning methods have boosted various tasks of question answering and have demonstrated dramatic effects in performance improvement for essential steps of question answering. Thus, leveraging deep learning methods for question answering has drawn much attention from both academia and industry in recent years. This paper provides a systematic review of the recent development of deep learning methods for question answering. The survey covers the scope including methods, datasets, and applications. The methods are discussed in terms of network structure characteristics, methodology innovations, and their effectiveness. The survey is expected to be a contribution to the summarization of recent research progress and future directions of deep learning methods for question answering.},
	pages = {2765--2783},
	number = {4},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Hao, Tianyong and Li, Xinxin and He, Yulan and Wang, Fu Lee and Qu, Yingying},
	urldate = {2023-09-19},
	date = {2022-02},
	langid = {english},
	keywords = {Background, basics, overview, {QA}},
	file = {Hao et al. - 2022 - Recent progress in leveraging deep learning method.pdf:/Users/lenert/Zotero/storage/247GDDR2/Hao et al. - 2022 - Recent progress in leveraging deep learning method.pdf:application/pdf},
}

@article{etezadi_state_2023,
	title = {The state of the art in open domain complex question answering: a survey},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-022-03732-9},
	doi = {10.1007/s10489-022-03732-9},
	shorttitle = {The state of the art in open domain complex question answering},
	abstract = {Research on question answering ({QA}) systems has a long tradition. {QA} systems, as widely used systems in various applications, seek to find the answers to the given questions through the available resources. These systems are expected to be capable of answering various types of questions, including simple questions whose answers can be found in a single passage or sentence and complex questions which need more complicated reasoning to find the answer or their answer should be found by traversing several relations. Nowadays, answering complex questions from texts or structured data is a challenge in {QA} systems. In this paper, we have a comparative study on {QA} approaches and systems for answering complex questions. For this purpose, firstly, this paper discusses what a complex question is and surveys different types of constraints that may appear in complex questions. Furthermore, it addresses the challenges of these types of questions, the methods proposed to deal with them, and benchmark datasets used to evaluate their strengths and weaknesses.},
	pages = {4124--4144},
	number = {4},
	journaltitle = {Applied Intelligence},
	shortjournal = {Appl Intell},
	author = {Etezadi, Romina and Shamsfard, Mehrnoush},
	urldate = {2023-09-19},
	date = {2023-02},
	langid = {english},
	keywords = {Background, basics, overview, {QA}},
	file = {Etezadi und Shamsfard - 2023 - The state of the art in open domain complex questi.pdf:/Users/lenert/Zotero/storage/TTQ32SLA/Etezadi und Shamsfard - 2023 - The state of the art in open domain complex questi.pdf:application/pdf},
}

@book{jurafsky_speech_2023,
	location = {Palo Alto},
	edition = {3},
	title = {Speech and Language Processing},
	abstract = {An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
	author = {Jurafsky, Dan and Martin, James H.},
	date = {2023-01-03},
	file = {Speech and Language Processing.pdf:/Users/lenert/Zotero/storage/3K7TMW7N/14.pdf:application/pdf},
}

@misc{farea_evaluation_2022,
	title = {Evaluation of Question Answering Systems: Complexity of judging a natural language},
	url = {http://arxiv.org/abs/2209.12617},
	doi = {10.48550/arXiv.2209.12617},
	shorttitle = {Evaluation of Question Answering Systems},
	abstract = {Question answering ({QA}) systems are among the most important and rapidly developing research topics in natural language processing ({NLP}). A reason, therefore, is that a {QA} system allows humans to interact more naturally with a machine, e.g., via a virtual assistant or search engine. In the last decades, many {QA} systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a {QA} system. This survey attempts to provide a systematic overview of the general framework of {QA}, {QA} paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of {QA} systems. The latter is particularly important because not only is the construction of a {QA} system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.},
	number = {{arXiv}:2209.12617},
	publisher = {{arXiv}},
	author = {Farea, Amer and Yang, Zhen and Duong, Kien and Perera, Nadeesha and Emmert-Streib, Frank},
	urldate = {2023-09-20},
	date = {2022-09-10},
	eprinttype = {arxiv},
	eprint = {2209.12617 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, overview, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/UATP8ZJ9/Farea et al. - 2022 - Evaluation of Question Answering Systems Complexi.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/XP44XJ2E/2209.html:text/html;Full Text PDF:/Users/lenert/Zotero/storage/ISM7H746/Farea et al. - 2022 - Evaluation of Question Answering Systems Complexi.pdf:application/pdf},
}


@inproceedings{voorhees_trec-8_1999,
	title = {The {TREC}-8 Question Answering Track Report},
	url = {https://www.semanticscholar.org/paper/The-TREC-8-Question-Answering-Track-Report-Voorhees/646d4888871aca2a25111eb2520e4c47e253b014},
	abstract = {The {TREC}-8 Question Answering track was the  first large-scale evaluation of domain-independent question answering systems. This paper summarizes the results of the track by giving a brief overview of the different approaches taken to solve the problem. The most accurate systems found a correct response for more than 2/3 of the questions. Relatively simple bag-of-words approaches were adequate for  finding answers when responses could be as long as a paragraph (250 bytes), but more sophisticated processing was necessary for more direct responses (50 bytes). 
 
The {TREC}-8 Question Answering track was an initial e ort to bring the bene ts of large-scale evaluation to bear on a question answering ({QA}) task. The goal in the {QA} task is to retrieve small snippets of text that contain the actual answer to a question rather than the document lists traditionally returned by text retrieval systems. The assumption is that users would usually prefer to be given the answer rather than  and the answer themselves in a document. 
 
This paper summarizes the retrieval results of the track; a companion paper ({\textbackslash}The {TREC}-8 Question Answering Track Evaluation") gives details about how the evaluation was implemented. By necessity, a track report can give only an overview of the different approaches used in the track. Readers are urged to consult the participants' papers elsewhere in the Proceedings for details regarding a particular approach.},
	eventtitle = {Text Retrieval Conference},
	author = {Voorhees, E.},
	urldate = {2023-09-19},
	date = {1999},
	keywords = {Background, basics, groundwork, historical, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/RSDRLSMP/Voorhees - 1999 - The TREC-8 Question Answering Track Report.pdf:application/pdf},
}

@inproceedings{green_baseball_1961,
	location = {New York, {NY}, {USA}},
	title = {Baseball: an automatic question-answerer},
	isbn = {978-1-4503-7872-7},
	url = {https://dl.acm.org/doi/10.1145/1460690.1460714},
	doi = {10.1145/1460690.1460714},
	series = {{IRE}-{AIEE}-{ACM} '61 (Western)},
	shorttitle = {Baseball},
	abstract = {{\textless}u{\textgreater}Baseball{\textless}/u{\textgreater} is a computer program that answers questions phrased in ordinary English about stored data. The program reads the question from punched cards. After the words and idioms are looked up in a dictionary, the phrase structure and other syntactic facts are determined for a content analysis, which lists attribute-value pairs specifying the information given and the information requested. The requested information is then extracted from the data matching the specifications, and any necessary processing is done. Finally, the answer is printed. The program's present context is baseball games; it answers such questions as "Where did each team play on July 7?"},
	pages = {219--224},
	booktitle = {Papers presented at the May 9-11, 1961, western joint {IRE}-{AIEE}-{ACM} computer conference},
	publisher = {Association for Computing Machinery},
	author = {Green, Bert F. and Wolf, Alice K. and Chomsky, Carol and Laughery, Kenneth},
	urldate = {2023-09-19},
	date = {1961-05-09},
	keywords = {Background, groundwork, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/A9B5I6B8/Green et al. - 1961 - Baseball an automatic question-answerer.pdf:application/pdf},
}

@article{ferrucci_introduction_2012,
	title = {Introduction to This is Watson},
	volume = {56},
	issn = {0018-8646},
	doi = {10.1147/JRD.2012.2184356},
	abstract = {In 2007, {IBM} Research took on the grand challenge of building a computer system that could compete with champions at the game of Jeopardy!™. In 2011, the open-domain question-answering ({QA}) system, dubbed Watson, beat the two highest ranked players in a nationally televised two-game Jeopardy! match. This paper provides a brief history of the events and ideas that positioned our team to take on the Jeopardy! challenge, build Watson, {IBM} Watson™, and ultimately triumph. It describes both the nature of the {QA} challenge represented by Jeopardy! and our overarching technical approach. The main body of this paper provides a narrative of the {DeepQA} processing pipeline to introduce the articles in this special issue and put them in context of the overall system. Finally, this paper summarizes our main results, describing how the system, as a holistic combination of many diverse algorithmic techniques, performed at champion levels, and it briefly discusses the team's future research plans.},
	pages = {1:1--1:15},
	number = {3},
	journaltitle = {{IBM} Journal of Research and Development},
	author = {Ferrucci, D. A.},
	date = {2012-05},
	note = {Conference Name: {IBM} Journal of Research and Development},
	keywords = {Algorithm design and analysis, Background, Computer architecture, Computers, Games, groundwork, History, {QA}, Semantics},
	file = {IEEE Xplore Abstract Record:/Users/lenert/Zotero/storage/9LBMHVTC/6177724.html:text/html},
}

@inproceedings{zhang_survey_2023,
	location = {Toronto, Canada},
	title = {A Survey for Efficient Open Domain Question Answering},
	url = {https://aclanthology.org/2023.acl-long.808},
	doi = {10.18653/v1/2023.acl-long.808},
	abstract = {Open domain question answering ({ODQA}) is a longstanding task aimed at answering factual questions from a large knowledge corpus without any explicit evidence in natural language processing ({NLP}). Recent works have predominantly focused on improving the answering accuracy and have achieved promising progress. However, higher accuracy often requires more memory consumption and inference latency, which might not necessarily be efficient enough for direct deployment in the real world. Thus, a trade-off between accuracy, memory consumption and processing speed is pursued. In this paper, we will survey recent advancements in the efficiency of {ODQA} models and conclude core techniques for achieving efficiency. Additionally, we will provide a quantitative analysis of memory cost, query speed, accuracy, and overall performance comparison. Our goal is to keep scholars informed of the latest advancements and open challenges in {ODQA} efficiency research and contribute to the further development of {ODQA} efficiency.},
	eventtitle = {{ACL} 2023},
	pages = {14447--14465},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Qin and Chen, Shangsi and Xu, Dongkuan and Cao, Qingqing and Chen, Xiaojun and Cohn, Trevor and Fang, Meng},
	urldate = {2023-08-27},
	date = {2023-07},
	keywords = {Background, Evaluation, Limits, Memory, Metrics, overview, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/4B43W6HL/Zhang et al. - 2023 - A Survey for Efficient Open Domain Question Answer.pdf:application/pdf},
}


@misc{dasigi_dataset_2021,
	title = {A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},
	url = {http://arxiv.org/abs/2105.03011},
	doi = {10.48550/arXiv.2105.03011},
	abstract = {Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present {QASPER}, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an {NLP} practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of {NLP} practitioners who also provide supporting evidence to answers. We find that existing models that do well on other {QA} tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking {QA}, which our dataset is designed to facilitate.},
	number = {{arXiv}:2105.03011},
	publisher = {{arXiv}},
	author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt},
	urldate = {2023-08-27},
	date = {2021-05-06},
	eprinttype = {arxiv},
	eprint = {2105.03011 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Dataset, Evaluation, groundwork, Indexing, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/WTHDV2NM/Dasigi et al. - 2021 - A Dataset of Information-Seeking Questions and Ans.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/BC2HI6RJ/2105.html:text/html},
}

@misc{rajpurkar_squad_2016,
	title = {{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
	url = {http://arxiv.org/abs/1606.05250},
	doi = {10.48550/arXiv.1606.05250},
	shorttitle = {{SQuAD}},
	abstract = {We present the Stanford Question Answering Dataset ({SQuAD}), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
	number = {{arXiv}:1606.05250},
	publisher = {{arXiv}},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	urldate = {2023-09-21},
	date = {2016-10-10},
	eprinttype = {arxiv},
	eprint = {1606.05250 [cs]},
	keywords = {Background, Computer Science - Computation and Language, dataset, groundwork, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/EIBICFD5/Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/HQLULCXF/1606.html:text/html},
}


@article{mishra_survey_2016,
	title = {A survey on question answering systems with classification},
	volume = {28},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157815000890},
	doi = {10.1016/j.jksuci.2014.10.007},
	abstract = {Question answering systems ({QASs}) generate answers of questions asked in natural languages. Early {QASs} were developed for restricted domains and have limited capabilities. Current {QASs} focus on types of questions generally asked by users, characteristics of data sources consulted, and forms of correct answers generated. Research in the area of {QASs} began in 1960s and since then, a large number of {QASs} have been developed. To identify the future scope of research in this area, the need of a comprehensive survey on {QASs} arises naturally. This paper surveys {QASs} and classifies them based on different criteria. We identify the current status of the research in the each category of {QASs}, and suggest future scope of the research.},
	pages = {345--361},
	number = {3},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	shortjournal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Mishra, Amit and Jain, Sanjay Kumar},
	urldate = {2023-09-21},
	date = {2016-07-01},
	keywords = {Background, basics, Information retrieval, Natural language processing, Natural language understanding, overview, {QA}, Question answering system, Search engine},
	file = {ScienceDirect Snapshot:/Users/lenert/Zotero/storage/ZGAGW9F2/S1319157815000890.html:text/html},
}
