
@misc{zhu_retrieving_2021,
	title = {Retrieving and {Reading}: {A} {Comprehensive} {Survey} on {Open}-domain {Question} {Answering}},
	shorttitle = {Retrieving and {Reading}},
	url = {http://arxiv.org/abs/2101.00774},
	doi = {10.48550/arXiv.2101.00774},
	abstract = {Open-domain Question Answering (OpenQA) is an important task in Natural Language Processing (NLP), which aims to answer a question in the form of natural language based on large-scale unstructured documents. Recently, there has been a surge in the amount of research literature on OpenQA, particularly on techniques that integrate with neural Machine Reading Comprehension (MRC). While these research works have advanced performance to new heights on benchmark datasets, they have been rarely covered in existing surveys on QA systems. In this work, we review the latest research trends in OpenQA, with particular attention to systems that incorporate neural MRC techniques. Specifically, we begin with revisiting the origin and development of OpenQA systems. We then introduce modern OpenQA architecture named "Retriever-Reader" and analyze the various systems that follow this architecture as well as the specific techniques adopted in each of the components. We then discuss key challenges to developing OpenQA systems and offer an analysis of benchmarks that are commonly used. We hope our work would enable researchers to be informed of the recent advancement and also the open challenges in OpenQA research, so as to stimulate further progress in this field.},
	urldate = {2023-08-27},
	publisher = {arXiv},
	author = {Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
	month = may,
	year = {2021},
	note = {arXiv:2101.00774 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Example Architectures, overview, QA, RRS},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7JSH329N/Zhu et al. - 2021 - Retrieving and Reading A Comprehensive Survey on .pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/6WWFWPPE/2101.html:text/html},
}

@misc{zamani_conversational_2023,
	title = {Conversational {Information} {Seeking}},
	url = {http://arxiv.org/abs/2201.08808},
	doi = {10.48550/arXiv.2201.08808},
	abstract = {Conversational information seeking (CIS) is concerned with a sequence of interactions between one or more users and an information system. Interactions in CIS are primarily based on natural language dialogue, while they may include other types of interactions, such as click, touch, and body gestures. This monograph provides a thorough overview of CIS definitions, applications, interactions, interfaces, design, implementation, and evaluation. This monograph views CIS applications as including conversational search, conversational question answering, and conversational recommendation. Our aim is to provide an overview of past research related to CIS, introduce the current state-of-the-art in CIS, highlight the challenges still being faced in the community. and suggest future directions.},
	urldate = {2023-08-27},
	publisher = {arXiv},
	author = {Zamani, Hamed and Trippas, Johanne R. and Dalton, Jeff and Radlinski, Filip},
	month = jan,
	year = {2023},
	note = {arXiv:2201.08808 [cs]},
	keywords = {Background, CIS, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval, overview, QA, SvD},
	annote = {Comment: Draft Version 1.2},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7WFGQWLT/Zamani et al. - 2023 - Conversational Information Seeking.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/MBB85CRQ/2201.html:text/html},
}

@misc{gao_neural_2022,
	title = {Neural {Approaches} to {Conversational} {Information} {Retrieval}},
	url = {http://arxiv.org/abs/2201.05176},
	doi = {10.48550/arXiv.2201.05176},
	abstract = {A conversational information retrieval (CIR) system is an information retrieval (IR) system with a conversational interface which allows users to interact with the system to seek information via multi-turn conversations of natural language, in spoken or written form. Recent progress in deep learning has brought tremendous improvements in natural language processing (NLP) and conversational AI, leading to a plethora of commercial conversational services that allow naturally spoken and typed interaction, increasing the need for more human-centric interactions in IR. As a result, we have witnessed a resurgent interest in developing modern CIR systems in both research communities and industry. This book surveys recent advances in CIR, focusing on neural approaches that have been developed in the last few years. This book is based on the authors' tutorial at SIGIR'2020 (Gao et al., 2020b), with IR and NLP communities as the primary target audience. However, audiences with other background, such as machine learning and human-computer interaction, will also find it an accessible introduction to CIR. We hope that this book will prove a valuable resource for students, researchers, and software developers. This manuscript is a working draft. Comments are welcome.},
	urldate = {2023-08-27},
	publisher = {arXiv},
	author = {Gao, Jianfeng and Xiong, Chenyan and Bennett, Paul and Craswell, Nick},
	month = jan,
	year = {2022},
	note = {arXiv:2201.05176 [cs]},
	keywords = {Background, CIS, Computer Science - Computation and Language, Computer Science - Information Retrieval, overview, QA, SvD},
	annote = {Comment: Book Draft},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/RZ38AQSJ/Gao et al. - 2022 - Neural Approaches to Conversational Information Re.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/5UH2JCTT/2201.html:text/html},
}

@misc{liu_dense_2021,
	title = {Dense {Hierarchical} {Retrieval} for {Open}-{Domain} {Question} {Answering}},
	url = {http://arxiv.org/abs/2110.15439},
	doi = {10.48550/arXiv.2110.15439},
	abstract = {Dense neural text retrieval has achieved promising results on open-domain Question Answering (QA), where latent representations of questions and passages are exploited for maximum inner product search in the retrieval process. However, current dense retrievers require splitting documents into short passages that usually contain local, partial, and sometimes biased context, and highly depend on the splitting process. As a consequence, it may yield inaccurate and misleading hidden representations, thus deteriorating the final retrieval result. In this work, we propose Dense Hierarchical Retrieval (DHR), a hierarchical framework that can generate accurate dense representations of passages by utilizing both macroscopic semantics in the document and microscopic semantics specific to each passage. Specifically, a document-level retriever first identifies relevant documents, among which relevant passages are then retrieved by a passage-level retriever. The ranking of the retrieved passages will be further calibrated by examining the document-level relevance. In addition, hierarchical title structure and two negative sampling strategies (i.e., In-Doc and In-Sec negatives) are investigated. We apply DHR to large-scale open-domain QA datasets. DHR significantly outperforms the original dense passage retriever and helps an end-to-end QA system outperform the strong baselines on multiple open-domain QA benchmarks.},
	urldate = {2023-08-27},
	publisher = {arXiv},
	author = {Liu, Ye and Hashimoto, Kazuma and Zhou, Yingbo and Yavuz, Semih and Xiong, Caiming and Yu, Philip S.},
	month = oct,
	year = {2021},
	note = {arXiv:2110.15439 [cs]},
	keywords = {Background, Computer Science - Information Retrieval, Indexing, QA, Retrieval},
	annote = {Comment: EMNLP 2021 Findings},
	annote = {Create a hierarchical tree over Wikipedia articles. Title + Abstract + ToC are the Document Level and will be searched for at first, after it will dive into the Passage level
},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/JWPAQPNM/Liu et al. - 2021 - Dense Hierarchical Retrieval for Open-Domain Quest.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZWUJ8UF5/2110.html:text/html},
}

@inproceedings{zhang_survey_2023,
	address = {Toronto, Canada},
	title = {A {Survey} for {Efficient} {Open} {Domain} {Question} {Answering}},
	url = {https://aclanthology.org/2023.acl-long.808},
	doi = {10.18653/v1/2023.acl-long.808},
	abstract = {Open domain question answering (ODQA) is a longstanding task aimed at answering factual questions from a large knowledge corpus without any explicit evidence in natural language processing (NLP). Recent works have predominantly focused on improving the answering accuracy and have achieved promising progress. However, higher accuracy often requires more memory consumption and inference latency, which might not necessarily be efficient enough for direct deployment in the real world. Thus, a trade-off between accuracy, memory consumption and processing speed is pursued. In this paper, we will survey recent advancements in the efficiency of ODQA models and conclude core techniques for achieving efficiency. Additionally, we will provide a quantitative analysis of memory cost, query speed, accuracy, and overall performance comparison. Our goal is to keep scholars informed of the latest advancements and open challenges in ODQA efficiency research and contribute to the further development of ODQA efficiency.},
	urldate = {2023-08-27},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Qin and Chen, Shangsi and Xu, Dongkuan and Cao, Qingqing and Chen, Xiaojun and Cohn, Trevor and Fang, Meng},
	month = jul,
	year = {2023},
	keywords = {Background, Evaluation, Limits, Memory, Metrics, overview, QA},
	pages = {14447--14465},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/4B43W6HL/Zhang et al. - 2023 - A Survey for Efficient Open Domain Question Answer.pdf:application/pdf},
}

@misc{karpukhin_dense_2020,
	title = {Dense {Passage} {Retrieval} for {Open}-{Domain} {Question} {Answering}},
	url = {http://arxiv.org/abs/2004.04906},
	doi = {10.48550/arXiv.2004.04906},
	abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
	urldate = {2023-08-27},
	publisher = {arXiv},
	author = {Karpukhin, Vladimir and Oğuz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
	month = sep,
	year = {2020},
	note = {arXiv:2004.04906 [cs]},
	keywords = {Background, Computer Science - Computation and Language, groundwork, Indexing, LLM, QA, Retrieval},
	annote = {Comment: EMNLP 2020},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/VP8YAJD5/Karpukhin et al. - 2020 - Dense Passage Retrieval for Open-Domain Question A.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/R25C9NEX/2004.html:text/html},
}

@misc{mcdonald_detect_2022,
	title = {Detect, {Retrieve}, {Comprehend}: {A} {Flexible} {Framework} for {Zero}-{Shot} {Document}-{Level} {Question} {Answering}},
	shorttitle = {Detect, {Retrieve}, {Comprehend}},
	url = {http://arxiv.org/abs/2210.01959},
	doi = {10.48550/arXiv.2210.01959},
	abstract = {Researchers produce thousands of scholarly documents containing valuable technical knowledge. The community faces the laborious task of reading these documents to identify, extract, and synthesize information. To automate information gathering, document-level question answering (QA) offers a flexible framework where human-posed questions can be adapted to extract diverse knowledge. Finetuning QA systems requires access to labeled data (tuples of context, question and answer). However, data curation for document QA is uniquely challenging because the context (i.e. answer evidence passage) needs to be retrieved from potentially long, ill-formatted documents. Existing QA datasets sidestep this challenge by providing short, well-defined contexts that are unrealistic in real-world applications. We present a three-stage document QA approach: (1) text extraction from PDF; (2) evidence retrieval from extracted texts to form well-posed contexts; (3) QA to extract knowledge from contexts to return high-quality answers -- extractive, abstractive, or Boolean. Using QASPER for evaluation, our detect-retrieve-comprehend (DRC) system achieves a +7.19 improvement in Answer-F1 over existing baselines while delivering superior context selection. Our results demonstrate that DRC holds tremendous promise as a flexible framework for practical scientific document QA.},
	urldate = {2023-08-27},
	publisher = {arXiv},
	author = {McDonald, Tavish and Tsan, Brian and Saini, Amar and Ordonez, Juanita and Gutierrez, Luis and Nguyen, Phan and Mason, Blake and Ng, Brenda},
	month = dec,
	year = {2022},
	note = {arXiv:2210.01959 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Indexing, pdfs, QA, related work, Retrieval},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/2TUGS7SU/McDonald et al. - 2022 - Detect, Retrieve, Comprehend A Flexible Framework.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/6Z9ZSPFB/2210.html:text/html},
}

@misc{dasigi_dataset_2021,
	title = {A {Dataset} of {Information}-{Seeking} {Questions} and {Answers} {Anchored} in {Research} {Papers}},
	url = {http://arxiv.org/abs/2105.03011},
	doi = {10.48550/arXiv.2105.03011},
	abstract = {Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.},
	urldate = {2023-08-27},
	publisher = {arXiv},
	author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt},
	month = may,
	year = {2021},
	note = {arXiv:2105.03011 [cs]},
	keywords = {Computer Science - Computation and Language, Background, Evaluation, Indexing, groundwork, related work, Dataset},
	annote = {Basically just have a Transformer which can take all tokens in and no need to split the text.

},
	annote = {Comment: Accepted at NAACL 2021; Project page: https://allenai.org/project/qasper},
	annote = {Have a lot regarding how to classify questions you ask to a document.
},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/WTHDV2NM/Dasigi et al. - 2021 - A Dataset of Information-Seeking Questions and Ans.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/BC2HI6RJ/2105.html:text/html},
}

@misc{pandya_question_2021,
	title = {Question {Answering} {Survey}: {Directions}, {Challenges}, {Datasets}, {Evaluation} {Matrices}},
	shorttitle = {Question {Answering} {Survey}},
	url = {http://arxiv.org/abs/2112.03572},
	doi = {10.48550/arXiv.2112.03572},
	abstract = {The usage and amount of information available on the internet increase over the past decade. This digitization leads to the need for automated answering system to extract fruitful information from redundant and transitional knowledge sources. Such systems are designed to cater the most prominent answer from this giant knowledge source to the user query using natural language understanding (NLU) and thus eminently depends on the Question-answering(QA) field. Question answering involves but not limited to the steps like mapping of user question to pertinent query, retrieval of relevant information, finding the best suitable answer from the retrieved information etc. The current improvement of deep learning models evince compelling performance improvement in all these tasks. In this review work, the research directions of QA field are analyzed based on the type of question, answer type, source of evidence-answer, and modeling approach. This detailing followed by open challenges of the field like automatic question generation, similarity detection and, low resource availability for a language. In the end, a survey of available datasets and evaluation measures is presented.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Pandya, Hariom A. and Bhatt, Brijesh S.},
	month = dec,
	year = {2021},
	note = {arXiv:2112.03572 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval, Computer Science - Machine Learning, overview, QA},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/88TJBBSV/Pandya und Bhatt - 2021 - Question Answering Survey Directions, Challenges,.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/87EC3RU7/2112.html:text/html},
}

@inproceedings{voorhees_trec-8_1999,
	title = {The {TREC}-8 {Question} {Answering} {Track} {Report}},
	url = {https://www.semanticscholar.org/paper/The-TREC-8-Question-Answering-Track-Report-Voorhees/646d4888871aca2a25111eb2520e4c47e253b014},
	abstract = {The TREC-8 Question Answering track was the  first large-scale evaluation of domain-independent question answering systems. This paper summarizes the results of the track by giving a brief overview of the different approaches taken to solve the problem. The most accurate systems found a correct response for more than 2/3 of the questions. Relatively simple bag-of-words approaches were adequate for  finding answers when responses could be as long as a paragraph (250 bytes), but more sophisticated processing was necessary for more direct responses (50 bytes). 
 
The TREC-8 Question Answering track was an initial e ort to bring the bene ts of large-scale evaluation to bear on a question answering (QA) task. The goal in the QA task is to retrieve small snippets of text that contain the actual answer to a question rather than the document lists traditionally returned by text retrieval systems. The assumption is that users would usually prefer to be given the answer rather than  and the answer themselves in a document. 
 
This paper summarizes the retrieval results of the track; a companion paper ({\textbackslash}The TREC-8 Question Answering Track Evaluation") gives details about how the evaluation was implemented. By necessity, a track report can give only an overview of the different approaches used in the track. Readers are urged to consult the participants' papers elsewhere in the Proceedings for details regarding a particular approach.},
	urldate = {2023-09-19},
	author = {Voorhees, E.},
	year = {1999},
	keywords = {Background, QA, historical, groundwork, basics},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/RSDRLSMP/Voorhees - 1999 - The TREC-8 Question Answering Track Report.pdf:application/pdf},
}

@article{hao_recent_2022,
	title = {Recent progress in leveraging deep learning methods for question answering},
	volume = {34},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-021-06748-3},
	doi = {10.1007/s00521-021-06748-3},
	abstract = {Question answering, serving as one of important tasks in natural language processing, enables machines to understand questions in natural language and answer the questions concisely. From web search to expert systems, question answering systems are widely applied to various domains in assisting information seeking. Deep learning methods have boosted various tasks of question answering and have demonstrated dramatic effects in performance improvement for essential steps of question answering. Thus, leveraging deep learning methods for question answering has drawn much attention from both academia and industry in recent years. This paper provides a systematic review of the recent development of deep learning methods for question answering. The survey covers the scope including methods, datasets, and applications. The methods are discussed in terms of network structure characteristics, methodology innovations, and their effectiveness. The survey is expected to be a contribution to the summarization of recent research progress and future directions of deep learning methods for question answering.},
	language = {en},
	number = {4},
	urldate = {2023-09-19},
	journal = {Neural Computing and Applications},
	author = {Hao, Tianyong and Li, Xinxin and He, Yulan and Wang, Fu Lee and Qu, Yingying},
	month = feb,
	year = {2022},
	keywords = {Background, basics, overview, QA},
	pages = {2765--2783},
	annote = {[TLDR] This paper provides a systematic review of the recent development of deep learning methods for question answering in terms of network structure characteristics, methodology innovations, and their effectiveness.},
	file = {Hao et al. - 2022 - Recent progress in leveraging deep learning method.pdf:/Users/lenert/Zotero/storage/247GDDR2/Hao et al. - 2022 - Recent progress in leveraging deep learning method.pdf:application/pdf},
}

@article{etezadi_state_2023,
	title = {The state of the art in open domain complex question answering: a survey},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	shorttitle = {The state of the art in open domain complex question answering},
	url = {https://link.springer.com/10.1007/s10489-022-03732-9},
	doi = {10.1007/s10489-022-03732-9},
	abstract = {Research on question answering (QA) systems has a long tradition. QA systems, as widely used systems in various applications, seek to find the answers to the given questions through the available resources. These systems are expected to be capable of answering various types of questions, including simple questions whose answers can be found in a single passage or sentence and complex questions which need more complicated reasoning to find the answer or their answer should be found by traversing several relations. Nowadays, answering complex questions from texts or structured data is a challenge in QA systems. In this paper, we have a comparative study on QA approaches and systems for answering complex questions. For this purpose, firstly, this paper discusses what a complex question is and surveys different types of constraints that may appear in complex questions. Furthermore, it addresses the challenges of these types of questions, the methods proposed to deal with them, and benchmark datasets used to evaluate their strengths and weaknesses.},
	language = {en},
	number = {4},
	urldate = {2023-09-19},
	journal = {Applied Intelligence},
	author = {Etezadi, Romina and Shamsfard, Mehrnoush},
	month = feb,
	year = {2023},
	keywords = {Background, basics, overview, QA},
	pages = {4124--4144},
	annote = {[TLDR] What a complex question is, the challenges of these types of questions, the methods proposed to deal with them, and benchmark datasets used to evaluate their strengths and weaknesses are addressed.},
	file = {Etezadi und Shamsfard - 2023 - The state of the art in open domain complex questi.pdf:/Users/lenert/Zotero/storage/TTQ32SLA/Etezadi und Shamsfard - 2023 - The state of the art in open domain complex questi.pdf:application/pdf},
}

@book{jurafsky_speech_2023,
	address = {Palo Alto},
	edition = {3},
	title = {Speech and {Language} {Processing}},
	abstract = {An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
	author = {Jurafsky, Dan and Martin, James H.},
	month = jan,
	year = {2023},
	file = {Speech and Language Processing.pdf:/Users/lenert/Zotero/storage/3K7TMW7N/14.pdf:application/pdf},
}

@misc{farea_evaluation_2022,
	title = {Evaluation of {Question} {Answering} {Systems}: {Complexity} of judging a natural language},
	shorttitle = {Evaluation of {Question} {Answering} {Systems}},
	url = {http://arxiv.org/abs/2209.12617},
	doi = {10.48550/arXiv.2209.12617},
	abstract = {Question answering (QA) systems are among the most important and rapidly developing research topics in natural language processing (NLP). A reason, therefore, is that a QA system allows humans to interact more naturally with a machine, e.g., via a virtual assistant or search engine. In the last decades, many QA systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a QA system. This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of QA systems. The latter is particularly important because not only is the construction of a QA system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Farea, Amer and Yang, Zhen and Duong, Kien and Perera, Nadeesha and Emmert-Streib, Frank},
	month = sep,
	year = {2022},
	note = {arXiv:2209.12617 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, overview, QA},
	annote = {[TLDR] This survey attempts to provide a systematic overview of the general framework of QA, QA paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of Q a system, and hypothesize that the quantitative formalization of human judgment is an open problem.},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/UATP8ZJ9/Farea et al. - 2022 - Evaluation of Question Answering Systems Complexi.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/XP44XJ2E/2209.html:text/html;Full Text PDF:/Users/lenert/Zotero/storage/ISM7H746/Farea et al. - 2022 - Evaluation of Question Answering Systems Complexi.pdf:application/pdf},
}

@inproceedings{green_baseball_1961,
	address = {New York, NY, USA},
	series = {{IRE}-{AIEE}-{ACM} '61 ({Western})},
	title = {Baseball: an automatic question-answerer},
	isbn = {978-1-4503-7872-7},
	shorttitle = {Baseball},
	url = {https://dl.acm.org/doi/10.1145/1460690.1460714},
	doi = {10.1145/1460690.1460714},
	abstract = {{\textless}u{\textgreater}Baseball{\textless}/u{\textgreater} is a computer program that answers questions phrased in ordinary English about stored data. The program reads the question from punched cards. After the words and idioms are looked up in a dictionary, the phrase structure and other syntactic facts are determined for a content analysis, which lists attribute-value pairs specifying the information given and the information requested. The requested information is then extracted from the data matching the specifications, and any necessary processing is done. Finally, the answer is printed. The program's present context is baseball games; it answers such questions as "Where did each team play on July 7?"},
	urldate = {2023-09-19},
	booktitle = {Papers presented at the {May} 9-11, 1961, western joint {IRE}-{AIEE}-{ACM} computer conference},
	publisher = {Association for Computing Machinery},
	author = {Green, Bert F. and Wolf, Alice K. and Chomsky, Carol and Laughery, Kenneth},
	month = may,
	year = {1961},
	keywords = {Background, QA, groundwork},
	pages = {219--224},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/A9B5I6B8/Green et al. - 1961 - Baseball an automatic question-answerer.pdf:application/pdf},
}

@article{ferrucci_introduction_2012,
	title = {Introduction to “{This} is {Watson}”},
	volume = {56},
	issn = {0018-8646},
	doi = {10.1147/JRD.2012.2184356},
	abstract = {In 2007, IBM Research took on the grand challenge of building a computer system that could compete with champions at the game of Jeopardy!™. In 2011, the open-domain question-answering (QA) system, dubbed Watson, beat the two highest ranked players in a nationally televised two-game Jeopardy! match. This paper provides a brief history of the events and ideas that positioned our team to take on the Jeopardy! challenge, build Watson, IBM Watson™, and ultimately triumph. It describes both the nature of the QA challenge represented by Jeopardy! and our overarching technical approach. The main body of this paper provides a narrative of the DeepQA processing pipeline to introduce the articles in this special issue and put them in context of the overall system. Finally, this paper summarizes our main results, describing how the system, as a holistic combination of many diverse algorithmic techniques, performed at champion levels, and it briefly discusses the team's future research plans.},
	number = {3.4},
	journal = {IBM Journal of Research and Development},
	author = {Ferrucci, D. A.},
	month = may,
	year = {2012},
	note = {Conference Name: IBM Journal of Research and Development},
	keywords = {Background, QA, groundwork, Algorithm design and analysis, Computer architecture, Computers, Games, History, Semantics},
	pages = {1:1--1:15},
	file = {IEEE Xplore Abstract Record:/Users/lenert/Zotero/storage/9LBMHVTC/6177724.html:text/html},
}

@misc{rajpurkar_squad_2016,
	title = {{SQuAD}: 100,000+ {Questions} for {Machine} {Comprehension} of {Text}},
	shorttitle = {{SQuAD}},
	url = {http://arxiv.org/abs/1606.05250},
	doi = {10.48550/arXiv.1606.05250},
	abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	month = oct,
	year = {2016},
	note = {arXiv:1606.05250 [cs]},
	keywords = {Computer Science - Computation and Language, Background, QA, groundwork, Dataset},
	annote = {Comment: To appear in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/EIBICFD5/Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/HQLULCXF/1606.html:text/html},
}

@article{mishra_survey_2016,
	title = {A survey on question answering systems with classification},
	volume = {28},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157815000890},
	doi = {10.1016/j.jksuci.2014.10.007},
	abstract = {Question answering systems (QASs) generate answers of questions asked in natural languages. Early QASs were developed for restricted domains and have limited capabilities. Current QASs focus on types of questions generally asked by users, characteristics of data sources consulted, and forms of correct answers generated. Research in the area of QASs began in 1960s and since then, a large number of QASs have been developed. To identify the future scope of research in this area, the need of a comprehensive survey on QASs arises naturally. This paper surveys QASs and classifies them based on different criteria. We identify the current status of the research in the each category of QASs, and suggest future scope of the research.},
	number = {3},
	urldate = {2023-09-21},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Mishra, Amit and Jain, Sanjay Kumar},
	month = jul,
	year = {2016},
	keywords = {Background, QA, overview, basics, Information retrieval, Natural language processing, Natural language understanding, Question answering system, Search engine},
	pages = {345--361},
	file = {ScienceDirect Snapshot:/Users/lenert/Zotero/storage/ZGAGW9F2/S1319157815000890.html:text/html},
}

@article{dimitrakis_survey_2020,
	title = {A survey on question answering systems over linked data and documents},
	volume = {55},
	issn = {1573-7675},
	url = {https://doi.org/10.1007/s10844-019-00584-7},
	doi = {10.1007/s10844-019-00584-7},
	abstract = {Question Answering (QA) systems aim at supplying precise answers to questions, posed by users in a natural language form. They are used in a wide range of application areas, from bio-medicine to tourism. Their underlying knowledge source can be structured data (e.g. RDF graphs and SQL databases), unstructured data in the form of plain text (e.g. textual excerpts from Wikipedia), or combinations of the above. In this paper we survey the recent work that has been done in the area of stateless QA systems with emphasis on methods that have been applied in RDF and Linked Data, documents, and mixtures of these. We identify the main challenges, we categorize the existing approaches according to various aspects, we review 21 recent systems, and 23 evaluation and training datasets that are most commonly used in the literature categorized according to the type of the domain, the underlying knowledge source, the provided tasks, and the associated evaluation metrics.},
	language = {en},
	number = {2},
	urldate = {2023-09-21},
	journal = {Journal of Intelligent Information Systems},
	author = {Dimitrakis, Eleftherios and Sgontzos, Konstantinos and Tzitzikas, Yannis},
	month = oct,
	year = {2020},
	keywords = {Background, QA, overview, Question answering, dialogue systems, evaluation collections, RDF and linked data, Example, KBQA},
	pages = {233--259},
}

@inproceedings{roberts_how_2020,
	address = {Online},
	title = {How {Much} {Knowledge} {Can} {You} {Pack} {Into} the {Parameters} of a {Language} {Model}?},
	url = {https://aclanthology.org/2020.emnlp-main.437},
	doi = {10.18653/v1/2020.emnlp-main.437},
	abstract = {It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.},
	urldate = {2023-09-21},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Roberts, Adam and Raffel, Colin and Shazeer, Noam},
	month = nov,
	year = {2020},
	keywords = {Background, Example Architectures, generative, QA},
	pages = {5418--5426},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/B98FDWN7/Roberts et al. - 2020 - How Much Knowledge Can You Pack Into the Parameter.pdf:application/pdf},
}

@article{harabagiu_open-domain_2003,
	title = {Open-domain textual question answering techniques},
	volume = {9},
	issn = {1469-8110, 1351-3249},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/opendomain-textual-question-answering-techniques/31811E06D60998460030D304F6869D7E},
	doi = {10.1017/S1351324903003176},
	abstract = {Textual question answering is a technique of extracting a sentence or text snippet from a document or document collection that responds directly to a query. Open-domain textual question answering presupposes that questions are natural and unrestricted with respect to topic. The question answering (Q/A) techniques, as embodied in today's systems, can be roughly divided into two types: (1) techniques for Information Seeking (IS), which localize the answer in vast document collections; and (2) techniques for Reading Comprehension (RC) that answer a series of questions related to a given document. Although these two types of techniques and systems are different, it is desirable to combine them for enabling more advanced forms of Q/A. This paper discusses an approach that successfully enhanced an existing IS system with RC capabilities. This enhancement is important because advanced Q/A, as exemplified by the ARDA AQUAINT program, is moving towards Q/A systems that incorporate semantic and pragmatic knowledge enabling dialogue-based Q/A. Because today's RC systems involve a short series of questions in context, they represent a rudimentary form of interactive Q/A which constitutes a possible foundation for more advanced forms of dialogue-based Q/A.},
	language = {en},
	number = {3},
	urldate = {2023-09-21},
	journal = {Natural Language Engineering},
	author = {Harabagiu, Sanda M. and Maiorano, Steven J. and Paşca, Marius A.},
	month = sep,
	year = {2003},
	note = {Publisher: Cambridge University Press},
	keywords = {Background, Example Architectures, QA},
	pages = {231--267},
}

@article{nassiri_transformer_2023,
	title = {Transformer models used for text-based question answering systems},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-022-04052-8},
	doi = {10.1007/s10489-022-04052-8},
	abstract = {The question answering system is frequently applied in the area of natural language processing (NLP) because of the wide variety of applications. It consists of answering questions using natural language. The problem is, in general, solved by employing a dataset that consists of an input text, a query, and the text segment or span from the input text that provides the question’s answer. The ability to make human-level predictions from data has improved significantly thanks to deep learning models, particularly the Transformer architecture, which has been state-of-the-art in text-based models in recent years. This paper reviews studies related to the use of transformer models in the implementation of question-answering (QA) systems. The paper’s first focus is on the attention and transformer models. A brief description of the architectures is presented by classifying them into models based on encoders, decoders, and on both Encoder-Decoder. Following that, we examine the most recent research trends in textual QA datasets by highlighting the architecture of QA systems and categorizing them according to various criteria. We survey also a significant set of evaluation metrics that have been developed in order to evaluate the models’ performance. Finally, we highlight solutions built to simplify the implementation of Transformer models.},
	language = {en},
	number = {9},
	urldate = {2023-09-21},
	journal = {Applied Intelligence},
	author = {Nassiri, Khalid and Akhloufi, Moulay},
	month = may,
	year = {2023},
	keywords = {Background, basics, Example Architectures, overview, QA},
	pages = {10602--10635},
	annote = {[TLDR] This paper reviews studies related to the use of transformer models in the implementation of question-answering (QA) systems and examines the most recent research trends in textual QA datasets by highlighting the architecture of QA systems and categorizing them according to various criteria.},
	file = {Nassiri und Akhloufi - 2023 - Transformer models used for text-based question an.pdf:/Users/lenert/Zotero/storage/CBW5CYTP/Nassiri und Akhloufi - 2023 - Transformer models used for text-based question an.pdf:application/pdf},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, Example Architectures, QA},
	annote = {Comment: Accepted at NeurIPS 2020},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/W8IWGTEN/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Inten.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/AWEXY5RF/2005.html:text/html},
}

@misc{guu_realm_2020,
	title = {{REALM}: {Retrieval}-{Augmented} {Language} {Model} {Pre}-{Training}},
	shorttitle = {{REALM}},
	url = {http://arxiv.org/abs/2002.08909},
	doi = {10.48550/arXiv.2002.08909},
	abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
	month = feb,
	year = {2020},
	note = {arXiv:2002.08909 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, Example Architectures, QA},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/2HCGEMTA/Guu et al. - 2020 - REALM Retrieval-Augmented Language Model Pre-Trai.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZBUCDVGK/2002.html:text/html},
}

@inproceedings{nishida_retrieve-and-read_2018,
	title = {Retrieve-and-{Read}: {Multi}-task {Learning} of {Information} {Retrieval} and {Reading} {Comprehension}},
	shorttitle = {Retrieve-and-{Read}},
	url = {http://arxiv.org/abs/1808.10628},
	doi = {10.1145/3269206.3271702},
	abstract = {This study considers the task of machine reading at scale (MRS) wherein, given a question, a system first performs the information retrieval (IR) task of finding relevant passages in a knowledge source and then carries out the reading comprehension (RC) task of extracting an answer span from the passages. Previous MRS studies, in which the IR component was trained without considering answer spans, struggled to accurately find a small number of relevant passages from a large set of passages. In this paper, we propose a simple and effective approach that incorporates the IR and RC tasks by using supervised multi-task learning in order that the IR component can be trained by considering answer spans. Experimental results on the standard benchmark, answering SQuAD questions using the full Wikipedia as the knowledge source, showed that our model achieved state-of-the-art performance. Moreover, we thoroughly evaluated the individual contributions of our model components with our new Japanese dataset and SQuAD. The results showed significant improvements in the IR task and provided a new perspective on IR for RC: it is effective to teach which part of the passage answers the question rather than to give only a relevance score to the whole passage.},
	urldate = {2023-09-21},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	author = {Nishida, Kyosuke and Saito, Itsumi and Otsuka, Atsushi and Asano, Hisako and Tomita, Junji},
	month = oct,
	year = {2018},
	note = {arXiv:1808.10628 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, Example Architectures, QA},
	pages = {647--656},
	annote = {Comment: 10 pages, 6 figure. Accepted as a full paper at CIKM 2018},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/YZ4J87GJ/Nishida et al. - 2018 - Retrieve-and-Read Multi-task Learning of Informat.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/TRPAWYK9/1808.html:text/html},
}

@article{wang_modern_2022,
	title = {Modern {Question} {Answering} {Datasets} and {Benchmarks}: {A} {Survey}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Modern {Question} {Answering} {Datasets} and {Benchmarks}},
	url = {https://arxiv.org/abs/2206.15030},
	doi = {10.48550/ARXIV.2206.15030},
	abstract = {Question Answering (QA) is one of the most important natural language processing (NLP) tasks. It aims using NLP technologies to generate a corresponding answer to a given question based on the massive unstructured corpus. With the development of deep learning, more and more challenging QA datasets are being proposed, and lots of new methods for solving them are also emerging. In this paper, we investigate influential QA datasets that have been released in the era of deep learning. Specifically, we begin with introducing two of the most common QA tasks - textual question answer and visual question answering - separately, covering the most representative datasets, and then give some current challenges of QA research.},
	urldate = {2023-09-22},
	author = {Wang, Zhen},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Background, Computation and Language (cs.CL), Dataset, FOS: Computer and information sciences, overview, QA},
	annote = {[TLDR] Two of the most common QA tasks - textual question answer and visual question answering - are introduced separately, covering the most representative datasets, and some current challenges of QA research are given.},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/8SLQGJSR/Wang - 2022 - Modern Question Answering Datasets and Benchmarks.pdf:application/pdf},
}

@incollection{tito_document_2021,
	title = {Document {Collection} {Visual} {Question} {Answering}},
	volume = {12822},
	url = {http://arxiv.org/abs/2104.14336},
	abstract = {Current tasks and methods in Document Understanding aims to process documents as single elements. However, documents are usually organized in collections (historical records, purchase invoices), that provide context useful for their interpretation. To address this problem, we introduce Document Collection Visual Question Answering (DocCVQA) a new dataset and related task, where questions are posed over a whole collection of document images and the goal is not only to provide the answer to the given question, but also to retrieve the set of documents that contain the information needed to infer the answer. Along with the dataset we propose a new evaluation metric and baselines which provide further insights to the new dataset and task.},
	urldate = {2023-09-22},
	author = {Tito, Rubèn and Karatzas, Dimosthenis and Valveny, Ernest},
	year = {2021},
	doi = {10.1007/978-3-030-86331-9_50},
	note = {arXiv:2104.14336 [cs]},
	keywords = {Background, Computer Science - Information Retrieval, Indexing, QA, VQA},
	pages = {778--792},
	file = {2104.14336.pdf:/Users/lenert/Zotero/storage/UKBJDGUY/2104.14336.pdf:application/pdf;arXiv Fulltext PDF:/Users/lenert/Zotero/storage/AC44N9FM/Tito et al. - 2021 - Document Collection Visual Question Answering.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/2HDBBTH3/2104.html:text/html},
}

@misc{wang_multi-passage_2019,
	title = {Multi-passage {BERT}: {A} {Globally} {Normalized} {BERT} {Model} for {Open}-domain {Question} {Answering}},
	shorttitle = {Multi-passage {BERT}},
	url = {http://arxiv.org/abs/1908.08167},
	doi = {10.48550/arXiv.1908.08167},
	abstract = {BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4\%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2\%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4\% EM and 21.5\% \$F\_1\$ over all non-BERT models, and 5.8\% EM and 6.5\% \$F\_1\$ over BERT-based models.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Wang, Zhiguo and Ng, Patrick and Ma, Xiaofei and Nallapati, Ramesh and Xiang, Bing},
	month = oct,
	year = {2019},
	note = {arXiv:1908.08167 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Indexing, LLM, QA},
	annote = {Comment: To appear in EMNLP 2019},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/MAQMI4J2/Wang et al. - 2019 - Multi-passage BERT A Globally Normalized BERT Mod.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/HLTCZN8I/1908.html:text/html},
}

@misc{noauthor_langchain-ailangchain_nodate,
	title = {langchain-ai/langchain: {Building} applications with {LLMs} through composability},
	author = {{Langchain}},
	shorttitle = {langchain-ai/langchain},
	url = {https://github.com/langchain-ai/langchain},
	language = {en},
	urldate = {2023-09-22},
	keywords = {Background, Indexing, LLM, Open-Source, QA},
	file = {Snapshot:/Users/lenert/Zotero/storage/625ZNSPF/master.html:text/html},
	year = {2023}
}

@misc{noauthor_chatgpt_2023,
	title = {{ChatGPT} {Retrieval} {Plugin}},
	author = {{OpenAI}},
	copyright = {MIT},
	url = {https://github.com/openai/chatgpt-retrieval-plugin},
	abstract = {The ChatGPT Retrieval Plugin lets you easily find personal or work documents by asking questions in natural language.},
	urldate = {2023-09-22},
	publisher = {OpenAI},
	month = sep,
	year = {2023},
	note = {original-date: 2023-03-23T06:06:22Z},
	keywords = {Background, chatgpt, chatgpt-plugins, Indexing, LLM, Open-Source, QA},
}

@misc{mathew_document_2021,
	title = {Document {Visual} {Question} {Answering} {Challenge} 2020},
	url = {http://arxiv.org/abs/2008.08899},
	doi = {10.48550/arXiv.2008.08899},
	abstract = {This paper presents results of Document Visual Question Answering Challenge organized as part of "Text and Documents in the Deep Learning Era" workshop, in CVPR 2020. The challenge introduces a new problem - Visual Question Answering on document images. The challenge comprised two tasks. The first task concerns with asking questions on a single document image. On the other hand, the second task is set as a retrieval task where the question is posed over a collection of images. For the task 1 a new dataset is introduced comprising 50,000 questions-answer(s) pairs defined over 12,767 document images. For task 2 another dataset has been created comprising 20 questions over 14,362 document images which share the same document template.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Mathew, Minesh and Tito, Ruben and Karatzas, Dimosthenis and Manmatha, R. and Jawahar, C. V.},
	month = jul,
	year = {2021},
	note = {arXiv:2008.08899 [cs]},
	keywords = {Background, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Indexing, QA, VQA},
	annote = {Comment: to be published as a short paper in DAS 2020},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/6A5UEC82/Mathew et al. - 2021 - Document Visual Question Answering Challenge 2020.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/MYGTHFBZ/2008.html:text/html},
}

@misc{li_dit_2022,
	title = {{DiT}: {Self}-supervised {Pre}-training for {Document} {Image} {Transformer}},
	shorttitle = {{DiT}},
	url = {http://arxiv.org/abs/2203.02378},
	doi = {10.48550/arXiv.2203.02378},
	abstract = {Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose {\textbackslash}textbf\{DiT\}, a self-supervised pre-trained {\textbackslash}textbf\{D\}ocument {\textbackslash}textbf\{I\}mage {\textbackslash}textbf\{T\}ransformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human-labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 \${\textbackslash}rightarrow\$ 92.69), document layout analysis (91.0 \${\textbackslash}rightarrow\$ 94.9), table detection (94.23 \${\textbackslash}rightarrow\$ 96.55) and text detection for OCR (93.07 \${\textbackslash}rightarrow\$ 94.29). The code and pre-trained models are publicly available at {\textbackslash}url\{https://aka.ms/msdit\}.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Li, Junlong and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
	month = jul,
	year = {2022},
	note = {arXiv:2203.02378 [cs]},
	keywords = {Background, Computer Science - Computer Vision and Pattern Recognition, Indexing, QA},
	annote = {Comment: ACM Multimedia 2022},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/3C3CXZ5I/Li et al. - 2022 - DiT Self-supervised Pre-training for Document Ima.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/SRUD7BX8/2203.html:text/html},
}

@incollection{meuschke_benchmark_2023,
	title = {A {Benchmark} of {PDF} {Information} {Extraction} {Tools} using a {Multi}-{Task} and {Multi}-{Domain} {Evaluation} {Framework} for {Academic} {Documents}},
	volume = {13972},
	url = {http://arxiv.org/abs/2303.09957},
	abstract = {Extracting information from academic PDF documents is crucial for numerous indexing, retrieval, and analysis use cases. Choosing the best tool to extract specific content elements is difficult because many, technically diverse tools are available, but recent performance benchmarks are rare. Moreover, such benchmarks typically cover only a few content elements like header metadata or bibliographic references and use smaller datasets from specific academic disciplines. We provide a large and diverse evaluation framework that supports more extraction tasks than most related datasets. Our framework builds upon DocBank, a multi-domain dataset of 1.5M annotated content elements extracted from 500K pages of research papers on arXiv. Using the new framework, we benchmark ten freely available tools in extracting document metadata, bibliographic references, tables, and other content elements from academic PDF documents. GROBID achieves the best metadata and reference extraction results, followed by CERMINE and Science Parse. For table extraction, Adobe Extract outperforms other tools, even though the performance is much lower than for other content elements. All tools struggle to extract lists, footers, and equations. We conclude that more research on improving and combining tools is necessary to achieve satisfactory extraction quality for most content elements. Evaluation datasets and frameworks like the one we present support this line of research. We make our data and code publicly available to contribute toward this goal.},
	urldate = {2023-09-22},
	author = {Meuschke, Norman and Jagdale, Apurva and Spinde, Timo and Mitrović, Jelena and Gipp, Bela},
	year = {2023},
	doi = {10.1007/978-3-031-28032-0_31},
	note = {arXiv:2303.09957 [cs]},
	keywords = {Background, Computer Science - Information Retrieval, Indexing, QA},
	pages = {383--405},
	annote = {Comment: iConference 2023},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/P8TZJEYY/Meuschke et al. - 2023 - A Benchmark of PDF Information Extraction Tools us.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/TZI8SLKP/2303.html:text/html},
}

@article{robertson_probabilistic_2009,
	title = {The {Probabilistic} {Relevance} {Framework}: {BM25} and {Beyond}},
	volume = {3},
	shorttitle = {The {Probabilistic} {Relevance} {Framework}},
	doi = {10.1561/1500000019},
	abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
	journal = {Foundations and Trends in Information Retrieval},
	author = {Robertson, Stephen and Zaragoza, Hugo},
	month = jan,
	year = {2009},
	keywords = {Background, historical, QA, Retrieval},
	pages = {333--389},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/T6AIFA9G/Robertson und Zaragoza - 2009 - The Probabilistic Relevance Framework BM25 and Be.pdf:application/pdf},
}

@misc{thakur_beir_2021,
	title = {{BEIR}: {A} {Heterogenous} {Benchmark} for {Zero}-shot {Evaluation} of {Information} {Retrieval} {Models}},
	shorttitle = {{BEIR}},
	url = {http://arxiv.org/abs/2104.08663},
	doi = {10.48550/arXiv.2104.08663},
	abstract = {Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Thakur, Nandan and Reimers, Nils and Rücklé, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
	month = oct,
	year = {2021},
	note = {arXiv:2104.08663 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, overview, QA, Retrieval},
	annote = {Comment: Accepted at NeurIPS 2021 Dataset and Benchmark Track},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/L4LKFL5E/Thakur et al. - 2021 - BEIR A Heterogenous Benchmark for Zero-shot Evalu.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/YS9JFTEK/2104.html:text/html},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Background, Computer Science - Computation and Language, groundwork, LLM, QA, Retrieval},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/2LWVZYFE/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/RTNVN3RG/1810.html:text/html},
}

@misc{khattab_colbert_2020,
	title = {{ColBERT}: {Efficient} and {Effective} {Passage} {Search} via {Contextualized} {Late} {Interaction} over {BERT}},
	shorttitle = {{ColBERT}},
	url = {http://arxiv.org/abs/2004.12832},
	doi = {10.48550/arXiv.2004.12832},
	abstract = {Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Khattab, Omar and Zaharia, Matei},
	month = jun,
	year = {2020},
	note = {arXiv:2004.12832 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, LLM, QA, Retrieval},
	annote = {Comment: Accepted at SIGIR 2020},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/45TREETC/Khattab und Zaharia - 2020 - ColBERT Efficient and Effective Passage Search vi.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/58N5BN5K/2004.html:text/html;Volltext:/Users/lenert/Zotero/storage/QM48M9W8/Khattab und Zaharia - 2020 - ColBERT Efficient and Effective Passage Search vi.pdf:application/pdf},
}

@misc{yang_hotpotqa_2018,
	title = {{HotpotQA}: {A} {Dataset} for {Diverse}, {Explainable} {Multi}-hop {Question} {Answering}},
	shorttitle = {{HotpotQA}},
	url = {http://arxiv.org/abs/1809.09600},
	doi = {10.48550/arXiv.1809.09600},
	abstract = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
	month = sep,
	year = {2018},
	note = {arXiv:1809.09600 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Dataset, QA, Retrieval},
	annote = {Comment: EMNLP 2018 long paper. The first three authors contribute equally. Data, code, and blog posts available at https://hotpotqa.github.io/},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/STYG4CHM/Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZRU8LJS5/1809.html:text/html},
}

@misc{zhang_beam_2023,
	title = {Beam {Retrieval}: {General} {End}-to-{End} {Retrieval} for {Multi}-{Hop} {Question} {Answering}},
	shorttitle = {Beam {Retrieval}},
	url = {http://arxiv.org/abs/2308.08973},
	doi = {10.48550/arXiv.2308.08973},
	abstract = {Multi-hop QA involves finding multiple relevant passages and step-by-step reasoning to answer complex questions. While previous approaches have developed retrieval modules for selecting relevant passages, they face challenges in scenarios beyond two hops, owing to the limited performance of one-step methods and the failure of two-step methods when selecting irrelevant passages in earlier stages. In this work, we introduce Beam Retrieval, a general end-to-end retrieval framework for multi-hop QA. This approach maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. Moreover, Beam Retrieval jointly optimizes an encoder and two classification heads by minimizing the combined loss across all hops. To establish a complete QA system, we incorporate a supervised reader or a zero-shot GPT-3.5. Experimental results demonstrate that Beam Retrieval achieves a nearly 50\% improvement compared with baselines on challenging MuSiQue-Ans, and it also surpasses all previous retrievers on HotpotQA and 2WikiMultiHopQA. Providing high-quality context, Beam Retrieval helps our supervised reader achieve new state-of-the-art performance and substantially improves (up to 28.8 points) the QA performance of zero-shot GPT-3.5.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Zhang, Jiahao and Zhang, Haiyang and Zhang, Dongmei and Liu, Yong and Huang, Shen},
	month = aug,
	year = {2023},
	note = {arXiv:2308.08973 [cs]},
	keywords = {Background, Computer Science - Computation and Language, LLM, QA, Retrieval},
	annote = {Comment: Code is available at https://github.com/canghongjian/beam\_retriever},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/CRKYSGUB/Zhang et al. - 2023 - Beam Retrieval General End-to-End Retrieval for M.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/HKGI5N7M/2308.html:text/html},
}

@inproceedings{he_deberta_2020,
	title = {{DEBERTA}: {DECODING}-{ENHANCED} {BERT} {WITH} {DISENTANGLED} {ATTENTION}},
	shorttitle = {{DEBERTA}},
	url = {https://openreview.net/forum?id=XPZIaotutsD},
	abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models’ generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understand(NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9\% (90.2\% vs. 91.1\%), on SQuAD v2.0 by +2.3\% (88.4\% vs. 90.7\%) and RACE by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, outperforming the human baseline by a decent margin (90.3 versus 89.8). The pre-trained DeBERTa models and the source code were released at: https://github.com/microsoft/DeBERTa.},
	language = {en},
	urldate = {2023-09-23},
	author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
	month = oct,
	year = {2020},
	keywords = {Background, LLM, QA, Retrieval},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/ICFYT6KS/He et al. - 2020 - DEBERTA DECODING-ENHANCED BERT WITH DISENTANGLED .pdf:application/pdf},
}

@misc{luo_choose_2022,
	title = {Choose {Your} {QA} {Model} {Wisely}: {A} {Systematic} {Study} of {Generative} and {Extractive} {Readers} for {Question} {Answering}},
	shorttitle = {Choose {Your} {QA} {Model} {Wisely}},
	url = {http://arxiv.org/abs/2203.07522},
	doi = {10.48550/arXiv.2203.07522},
	abstract = {While both extractive and generative readers have been successfully applied to the Question Answering (QA) task, little attention has been paid toward the systematic comparison of them. Characterizing the strengths and weaknesses of the two readers is crucial not only for making a more informed reader selection in practice but also for developing a deeper understanding to foster further research on improving readers in a principled manner. Motivated by this goal, we make the first attempt to systematically study the comparison of extractive and generative readers for question answering. To be aligned with the state-of-the-art, we explore nine transformer-based large pre-trained language models (PrLMs) as backbone architectures. Furthermore, we organize our findings under two main categories: (1) keeping the architecture invariant, and (2) varying the underlying PrLMs. Among several interesting findings, it is important to highlight that (1) the generative readers perform better in long context QA, (2) the extractive readers perform better in short context while also showing better out-of-domain generalization, and (3) the encoder of encoder-decoder PrLMs (e.g., T5) turns out to be a strong extractive reader and outperforms the standard choice of encoder-only PrLMs (e.g., RoBERTa). We also study the effect of multi-task learning on the two types of readers varying the underlying PrLMs and perform qualitative and quantitative diagnosis to provide further insights into future directions in modeling better readers.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Luo, Man and Hashimoto, Kazuma and Yavuz, Semih and Liu, Zhiwei and Baral, Chitta and Zhou, Yingbo},
	month = mar,
	year = {2022},
	note = {arXiv:2203.07522 [cs]},
	keywords = {Background, Computer Science - Computation and Language, QA, Reader},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/FTI4FGHF/Luo et al. - 2022 - Choose Your QA Model Wisely A Systematic Study of.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/43WS28GI/2203.html:text/html},
}

@misc{khashabi_unifiedqa-v2_2022,
	title = {{UnifiedQA}-v2: {Stronger} {Generalization} via {Broader} {Cross}-{Format} {Training}},
	shorttitle = {{UnifiedQA}-v2},
	url = {http://arxiv.org/abs/2202.12359},
	doi = {10.48550/arXiv.2202.12359},
	abstract = {We present UnifiedQA-v2, a QA model built with the same process as UnifiedQA, except that it utilizes more supervision -- roughly 3x the number of datasets used for UnifiedQA. This generally leads to better in-domain and cross-domain results.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Khashabi, Daniel and Kordi, Yeganeh and Hajishirzi, Hannaneh},
	month = feb,
	year = {2022},
	note = {arXiv:2202.12359 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Model, Open-Source, QA, Reader},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/ZG3ZNYQC/Khashabi et al. - 2022 - UnifiedQA-v2 Stronger Generalization via Broader .pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/UVT95SB9/2202.html:text/html},
}

@inproceedings{serban_generating_2016,
	address = {Berlin, Germany},
	title = {Generating {Factoid} {Questions} {With} {Recurrent} {Neural} {Networks}: {The} {30M} {Factoid} {Question}-{Answer} {Corpus}},
	shorttitle = {Generating {Factoid} {Questions} {With} {Recurrent} {Neural} {Networks}},
	url = {https://aclanthology.org/P16-1056},
	doi = {10.18653/v1/P16-1056},
	urldate = {2023-09-23},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Serban, Iulian Vlad and García-Durán, Alberto and Gulcehre, Caglar and Ahn, Sungjin and Chandar, Sarath and Courville, Aaron and Bengio, Yoshua},
	month = aug,
	year = {2016},
	keywords = {Background, Limits, QA},
	pages = {588--598},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/F3HMB6DH/Serban et al. - 2016 - Generating Factoid Questions With Recurrent Neural.pdf:application/pdf},
}

@article{kwiatkowski_natural_2019,
	title = {Natural {Questions}: {A} {Benchmark} for {Question} {Answering} {Research}},
	volume = {7},
	shorttitle = {Natural {Questions}},
	url = {https://aclanthology.org/Q19-1026},
	doi = {10.1162/tacl_a_00276},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	urldate = {2023-09-23},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	year = {2019},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	keywords = {Background, Dataset, QA},
	pages = {452--466},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/4DA5FW2C/Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answer.pdf:application/pdf},
}

@misc{neelakantan_text_2022,
	title = {Text and {Code} {Embeddings} by {Contrastive} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2201.10005},
	doi = {10.48550/arXiv.2201.10005},
	abstract = {Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4\% and 1.8\% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4\%, 14.7\%, and 10.6\% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8\% relative improvement over prior best work on code search.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and Heidecke, Johannes and Shyam, Pranav and Power, Boris and Nekoul, Tyna Eloundou and Sastry, Girish and Krueger, Gretchen and Schnurr, David and Such, Felipe Petroski and Hsu, Kenny and Thompson, Madeleine and Khan, Tabarak and Sherbakov, Toki and Jang, Joanne and Welinder, Peter and Weng, Lilian},
	month = jan,
	year = {2022},
	note = {arXiv:2201.10005 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, LLM, QA, related work, Retrieval},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7FD3EEJ5/Neelakantan et al. - 2022 - Text and Code Embeddings by Contrastive Pre-Traini.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/7J8TTGML/2201.html:text/html},
}

@misc{sachan_questions_2023,
	title = {Questions {Are} {All} {You} {Need} to {Train} a {Dense} {Passage} {Retriever}},
	url = {http://arxiv.org/abs/2206.10658},
	doi = {10.48550/arXiv.2206.10658},
	abstract = {We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g. questions and potential answer documents). It uses a new document-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence documents, and (2) the documents are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both document and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtains state-of-the-art results on multiple QA retrieval benchmarks with only generic initialization from a pre-trained language model, removing the need for labeled data and task-specific losses.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Sachan, Devendra Singh and Lewis, Mike and Yogatama, Dani and Zettlemoyer, Luke and Pineau, Joelle and Zaheer, Manzil},
	month = apr,
	year = {2023},
	note = {arXiv:2206.10658 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, OOD, QA, related work},
	annote = {Comment: Accepted to TACL, pre MIT Press publication version},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/F7ZWF6MC/Sachan et al. - 2023 - Questions Are All You Need to Train a Dense Passag.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/47359T5Z/2206.html:text/html},
}

@misc{ni_large_2021,
	title = {Large {Dual} {Encoders} {Are} {Generalizable} {Retrievers}},
	url = {http://arxiv.org/abs/2112.07899},
	doi = {10.48550/arXiv.2112.07899},
	abstract = {It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited to make dual encoders an effective retrieval model for out-of-domain generalization. In this paper, we challenge this belief by scaling up the size of the dual encoder model \{{\textbackslash}em while keeping the bottleneck embedding size fixed.\} With multi-stage training, surprisingly, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. Experimental results show that our dual encoders, {\textbackslash}textbf\{G\}eneralizable {\textbackslash}textbf\{T\}5-based dense {\textbackslash}textbf\{R\}etrievers (GTR), outperform \%ColBERT{\textasciitilde}{\textbackslash}cite\{khattab2020colbert\} and existing sparse and dense retrievers on the BEIR dataset{\textasciitilde}{\textbackslash}cite\{thakur2021beir\} significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10{\textbackslash}\% of MS Marco supervised data to achieve the best out-of-domain performance. All the GTR models are released at https://tfhub.dev/google/collections/gtr/1.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Ni, Jianmo and Qu, Chen and Lu, Jing and Dai, Zhuyun and Ábrego, Gustavo Hernández and Ma, Ji and Zhao, Vincent Y. and Luan, Yi and Hall, Keith B. and Chang, Ming-Wei and Yang, Yinfei},
	month = dec,
	year = {2021},
	note = {arXiv:2112.07899 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, OOD, QA, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/TFLHD85U/Ni et al. - 2021 - Large Dual Encoders Are Generalizable Retrievers.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/8CB8QQ5J/2112.html:text/html},
}

@misc{dai_promptagator_2022,
	title = {Promptagator: {Few}-shot {Dense} {Retrieval} {From} 8 {Examples}},
	shorttitle = {Promptagator},
	url = {http://arxiv.org/abs/2209.11755},
	doi = {10.48550/arXiv.2209.11755},
	abstract = {Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples \{without\} using Natural Questions or MS MARCO to train \%question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Dai, Zhuyun and Zhao, Vincent Y. and Ma, Ji and Luan, Yi and Ni, Jianmo and Lu, Jing and Bakalov, Anton and Guu, Kelvin and Hall, Keith B. and Chang, Ming-Wei},
	month = sep,
	year = {2022},
	note = {arXiv:2209.11755 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, OOD, QA, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/6DV4A7KK/Dai et al. - 2022 - Promptagator Few-shot Dense Retrieval From 8 Exam.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ACSLQCES/2209.html:text/html},
}

@misc{lyu_extending_2022,
	title = {Extending the {Scope} of {Out}-of-{Domain}: {Examining} {QA} models in multiple subdomains},
	shorttitle = {Extending the {Scope} of {Out}-of-{Domain}},
	url = {http://arxiv.org/abs/2204.04534},
	doi = {10.48550/arXiv.2204.04534},
	abstract = {Past works that investigate out-of-domain performance of QA systems have mainly focused on general domains (e.g. news domain, wikipedia domain), underestimating the importance of subdomains defined by the internal characteristics of QA datasets. In this paper, we extend the scope of "out-of-domain" by splitting QA examples into different subdomains according to their several internal characteristics including question type, text length, answer position. We then examine the performance of QA systems trained on the data from different subdomains. Experimental results show that the performance of QA systems can be significantly reduced when the train data and test data come from different subdomains. These results question the generalizability of current QA systems in multiple subdomains, suggesting the need to combat the bias introduced by the internal characteristics of QA datasets.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Lyu, Chenyang and Foster, Jennifer and Graham, Yvette},
	month = apr,
	year = {2022},
	note = {arXiv:2204.04534 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, OOD, overview, QA, related work},
	annote = {Comment: 14 pages, 6 figures, 29 tables, to appear at ACL 2022 Workshop on Insights from Negative Results in NLP, code available in https://github.com/lyuchenyang/Analysing-Question-Answering-Data},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/Z46K4ZJF/Lyu et al. - 2022 - Extending the Scope of Out-of-Domain Examining QA.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZI8NHCAV/2204.html:text/html},
}

@misc{gholami_zero-shot_2021,
	title = {Zero-{Shot} {Open}-{Book} {Question} {Answering}},
	url = {http://arxiv.org/abs/2111.11520},
	doi = {10.48550/arXiv.2111.11520},
	abstract = {Open book question answering is a subset of question answering tasks where the system aims to find answers in a given set of documents (open-book) and common knowledge about a topic. This article proposes a solution for answering natural language questions from a corpus of Amazon Web Services (AWS) technical documents with no domain-specific labeled data (zero-shot). These questions can have yes-no-none answers, short answers, long answers, or any combination of the above. This solution comprises a two-step architecture in which a retriever finds the right document and an extractor finds the answers in the retrieved document. We are introducing a new test dataset for open-book QA based on real customer questions on AWS technical documentation. After experimenting with several information retrieval systems and extractor models based on extractive language models, the solution attempts to find the yes-no-none answers and text answers in the same pass. The model is trained on the The Stanford Question Answering Dataset - SQuAD (Rajpurkaret al., 2016) and Natural Questions (Kwiatkowski et al., 2019) datasets. We were able to achieve 49\% F1 and 39\% exact match score (EM) end-to-end with no domain-specific training.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Gholami, Sia and Noori, Mehdi},
	month = nov,
	year = {2021},
	note = {arXiv:2111.11520 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning, OOD, QA, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/X3RMPTIS/Gholami und Noori - 2021 - Zero-Shot Open-Book Question Answering.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/H8VYJZPG/2111.html:text/html},
}

@misc{ding_v-doc_2022,
	title = {V-{Doc} : {Visual} questions answers with {Documents}},
	shorttitle = {V-{Doc}},
	url = {http://arxiv.org/abs/2205.13724},
	doi = {10.48550/arXiv.2205.13724},
	abstract = {We propose V-Doc, a question-answering tool using document images and PDF, mainly for researchers and general non-deep learning experts looking to generate, process, and understand the document visual question answering tasks. The V-Doc supports generating and using both extractive and abstractive question-answer pairs using documents images. The extractive QA selects a subset of tokens or phrases from the document contents to predict the answers, while the abstractive QA recognises the language in the content and generates the answer based on the trained model. Both aspects are crucial to understanding the documents, especially in an image format. We include a detailed scenario of question generation for the abstractive QA task. V-Doc supports a wide range of datasets and models, and is highly extensible through a declarative, framework-agnostic platform.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Ding, Yihao and Huang, Zhe and Wang, Runlin and Zhang, Yanhang and Chen, Xianru and Ma, Yuzhong and Chung, Hyunsuk and Han, Soyeon Caren},
	month = may,
	year = {2022},
	note = {arXiv:2205.13724 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, QA, related work, VQA},
	annote = {Comment: Accepted by CVPR 2022},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/HQ9KE4KM/Ding et al. - 2022 - V-Doc  Visual questions answers with Documents.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/CUNC2YQQ/2205.html:text/html},
}

@misc{noauthor_question_nodate,
	title = {Question {Answering} {\textbar} {Langchain}},
	author = {Langchain},
	url = {https://python.langchain.com/docs/use_cases/question_answering/},
	abstract = {Open In Collab},
	language = {en},
	urldate = {2023-09-24},
	keywords = {Background, LLM, QA, related work},
	file = {Snapshot:/Users/lenert/Zotero/storage/XMNN5KJ9/question_answering.html:text/html},
	year = {2023},
}

@inproceedings{izacard_leveraging_2021,
	address = {Online},
	title = {Leveraging {Passage} {Retrieval} with {Generative} {Models} for {Open} {Domain} {Question} {Answering}},
	url = {https://aclanthology.org/2021.eacl-main.74},
	doi = {10.18653/v1/2021.eacl-main.74},
	abstract = {Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.},
	urldate = {2023-09-24},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {Izacard, Gautier and Grave, Edouard},
	month = apr,
	year = {2021},
	keywords = {Background, LLM, QA, related work},
	pages = {874--880},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/8RPVFNAT/Izacard und Grave - 2021 - Leveraging Passage Retrieval with Generative Model.pdf:application/pdf},
}

@inproceedings{chen_improving_2021,
	title = {Improving {Out}-of-{Domain} {Question} {Answering} with {Mixture} of {Experts}},
	url = {https://www.semanticscholar.org/paper/Improving-Out-of-Domain-Question-Answering-with-of-Chen/646be1b52a97c415f299eb333f2b97d51421be46#related-papers},
	abstract = {Question answering (QA) is an important problem with numerous application in real life. Sometimes, the resource of certain QA task is limited. This work aims to build a robust QA system that can generalize to novel QA tasks with few examples and gradient steps. We propose a Mixture-of-Experts(MoE) style training framework, and use meta-learning methods for domain adaptation. We also explored data augmentation techniques, and successfully improve out-of-domain QA performance of baseline models on F-1 score from 50.81 to 53.84 and exact match (EM) score from 34.82 to 39.27. Our approach achieves a F-1 score of 60.8 and EM score of 42.2 on the out-of-domain QA testing leaderboard.},
	urldate = {2023-09-24},
	author = {Chen, Haofeng},
	year = {2021},
	keywords = {Background, OOD, QA, related work},
	annote = {[TLDR] This work aims to build a robust QA system that can generalize to novel QA tasks with few examples and gradient steps, and proposes a Mixture-of-Experts (MoE) style training framework, and uses meta-learning methods for domain adaptation.},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/89BQMZAT/Chen - 2021 - Improving Out-of-Domain Question Answering with Mi.pdf:application/pdf},
}

@misc{raffel_exploring_2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, LLM, QA, related work, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/S34MIIBM/Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/MHCRXLZU/1910.html:text/html},
}

@misc{reddy_synthetic_2022,
	title = {Synthetic {Target} {Domain} {Supervision} for {Open} {Retrieval} {QA}},
	url = {http://arxiv.org/abs/2204.09248},
	doi = {10.48550/arXiv.2204.09248},
	abstract = {Neural passage retrieval is a new and promising approach in open retrieval question answering. In this work, we stress-test the Dense Passage Retriever (DPR) -- a state-of-the-art (SOTA) open domain neural retrieval model -- on closed and specialized target domains such as COVID-19, and find that it lags behind standard BM25 in this important real-world setting. To make DPR more robust under domain shift, we explore its fine-tuning with synthetic training examples, which we generate from unlabeled target domain text using a text-to-text generator. In our experiments, this noisy but fully automated target domain supervision gives DPR a sizable advantage over BM25 in out-of-domain settings, making it a more viable model in practice. Finally, an ensemble of BM25 and our improved DPR model yields the best results, further pushing the SOTA for open retrieval QA on multiple out-of-domain test sets.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Reddy, Revanth Gangi and Iyer, Bhavani and Sultan, Md Arafat and Zhang, Rong and Sil, Avirup and Castelli, Vittorio and Florian, Radu and Roukos, Salim},
	month = apr,
	year = {2022},
	note = {arXiv:2204.09248 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, OOD, QA, related work},
	annote = {Comment: Published at SIGIR 2021},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/KA87P9I9/Reddy et al. - 2022 - Synthetic Target Domain Supervision for Open Retri.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/9S585Y9E/2204.html:text/html},
}

@misc{johnson_billion-scale_2017,
	title = {Billion-scale similarity search with {GPUs}},
	url = {http://arxiv.org/abs/1702.08734},
	doi = {10.48550/arXiv.1702.08734},
	abstract = {Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a design for k-selection that operates at up to 55\% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Johnson, Jeff and Douze, Matthijs and Jégou, Hervé},
	month = feb,
	year = {2017},
	note = {arXiv:1702.08734 [cs]},
	keywords = {Background, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Data Structures and Algorithms, Computer Science - Databases, Computer Science - Information Retrieval, Limits, QA},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/K3BW38BG/Johnson et al. - 2017 - Billion-scale similarity search with GPUs.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZVZE9GC5/1702.html:text/html},
}

@inproceedings{muller_bioasq_2015,
	address = {Cham},
	title = {{BioASQ}: {A} {Challenge} on {Large}-{Scale} {Biomedical} {Semantic} {Indexing} and {Question} {Answering}},
	volume = {9059},
	isbn = {978-3-319-24470-9 978-3-319-24471-6},
	shorttitle = {{BioASQ}},
	url = {http://link.springer.com/10.1007/978-3-319-24471-6_3},
	doi = {10.1007/978-3-319-24471-6_3},
	abstract = {BioASQ is a series of challenges that aims to assess the performance of information systems in supporting two tasks that are central to the biomedical question answering process: a the indexing of large volumes of unlabelled data, primarily scientific articles, with biomedical concepts, b the processing of biomedical questions and the generation of answers and supporting material. In this paper, the main results of the first two BioASQ challenges are presented.},
	language = {en},
	urldate = {2023-09-24},
	publisher = {Springer International Publishing},
	author = {Balikas, Georgios and Krithara, Anastasia and Partalas, Ioannis and Paliouras, George},
	editor = {Müller, Henning and Jimenez Del Toro, Oscar Alfonso and Hanbury, Allan and Langs, Georg and Foncubierta Rodriguez, Antonio},
	year = {2015},
	doi = {10.1007/978-3-319-24471-6_3},
	note = {Book Title: Multimodal Retrieval in the Medical Domain
Series Title: Lecture Notes in Computer Science},
	keywords = {Background, Dataset, QA, related work},
	pages = {26--39},
	annote = {[TLDR] The main results of the first two BioASQ challenges are presented and the performance of information systems in supporting two tasks that are central to the biomedical question answering process are assessed.},
}

@article{gururangan_dont_2020,
	title = {Don’t {Stop} {Pretraining}: {Adapt} {Language} {Models} to {Domains} and {Tasks}},
	shorttitle = {Don’t {Stop} {Pretraining}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.740},
	doi = {10.18653/v1/2020.acl-main.740},
	abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
	language = {en},
	urldate = {2023-09-24},
	journal = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	author = {Gururangan, Suchin and Marasović, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
	year = {2020},
	note = {Conference Name: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
Place: Online
Publisher: Association for Computational Linguistics},
	keywords = {Background, OOD, QA, related work},
	pages = {8342--8360},
	annote = {[TLDR] It is consistently found that multi-phase adaptive pretraining offers large gains in task performance, and it is shown that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable.},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/6KXDI3ZI/Gururangan et al. - 2020 - Don’t Stop Pretraining Adapt Language Models to D.pdf:application/pdf},
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to {PyPDF2} — {PyPDF2} documentation},
	author = {PyPDF2},
	url = {https://pypdf2.readthedocs.io/en/3.0.0/index.html},
	urldate = {2023-09-25},
	keywords = {Background, Extract, Main, Tools},
	file = {Welcome to PyPDF2 — PyPDF2 documentation:/Users/lenert/Zotero/storage/RJXCJ9XB/index.html:text/html},
}

@misc{santhanam_colbertv2_2022,
	title = {{ColBERTv2}: {Effective} and {Efficient} {Retrieval} via {Lightweight} {Late} {Interaction}},
	shorttitle = {{ColBERTv2}},
	url = {http://arxiv.org/abs/2112.01488},
	doi = {10.48550/arXiv.2112.01488},
	abstract = {Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10\${\textbackslash}times\$.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei},
	month = jul,
	year = {2022},
	note = {arXiv:2112.01488 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, LLM, Main, OOD, Retrieval},
	annote = {Comment: NAACL 2022. Omar and Keshav contributed equally to this work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/4HQME92U/Santhanam et al. - 2022 - ColBERTv2 Effective and Efficient Retrieval via L.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/2X7NJX6P/2112.html:text/html},
}

@misc{bajaj_ms_2018,
	title = {{MS} {MARCO}: {A} {Human} {Generated} {MAchine} {Reading} {COmprehension} {Dataset}},
	shorttitle = {{MS} {MARCO}},
	url = {http://arxiv.org/abs/1611.09268},
	doi = {10.48550/arXiv.1611.09268},
	abstract = {We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and Rosenberg, Mir and Song, Xia and Stoica, Alina and Tiwary, Saurabh and Wang, Tong},
	month = oct,
	year = {2018},
	note = {arXiv:1611.09268 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, Dataset},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/TSL3TE8Q/Bajaj et al. - 2018 - MS MARCO A Human Generated MAchine Reading COmpre.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/R9LWCB5M/1611.html:text/html},
}

@misc{wang_minilm_2020,
	title = {{MiniLM}: {Deep} {Self}-{Attention} {Distillation} for {Task}-{Agnostic} {Compression} of {Pre}-{Trained} {Transformers}},
	shorttitle = {{MiniLM}},
	url = {http://arxiv.org/abs/2002.10957},
	doi = {10.48550/arXiv.2002.10957},
	abstract = {Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99\% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50\% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
	month = apr,
	year = {2020},
	note = {arXiv:2002.10957 [cs]},
	keywords = {Computer Science - Computation and Language, Main, Retrieval},
	annote = {Comment: Code and models: https://github.com/microsoft/unilm/tree/master/minilm},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/4UE3UBMD/Wang et al. - 2020 - MiniLM Deep Self-Attention Distillation for Task-.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/U4NGN2NY/2002.html:text/html},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {https://arxiv.org/abs/1907.11692v1},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	language = {en},
	urldate = {2023-09-26},
	journal = {arXiv.org},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	keywords = {Background, Model, QA},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/UVUFPPQR/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf},
}

@misc{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {https://arxiv.org/abs/1910.13461v1},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
	language = {en},
	urldate = {2023-09-26},
	journal = {arXiv.org},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	month = oct,
	year = {2019},
	keywords = {Background, Model, QA},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/9QX5RLJ2/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:application/pdf},
}

@misc{pereira_visconde_2022,
	title = {Visconde: {Multi}-document {QA} with {GPT}-3 and {Neural} {Reranking}},
	shorttitle = {Visconde},
	url = {https://arxiv.org/abs/2212.09656v1},
	abstract = {This paper proposes a question-answering system that can answer questions whose supporting evidence is spread over multiple (potentially long) documents. The system, called Visconde, uses a three-step pipeline to perform the task: decompose, retrieve, and aggregate. The first step decomposes the question into simpler questions using a few-shot large language model (LLM). Then, a state-of-the-art search engine is used to retrieve candidate passages from a large collection for each decomposed question. In the final step, we use the LLM in a few-shot setting to aggregate the contents of the passages into the final answer. The system is evaluated on three datasets: IIRC, Qasper, and StrategyQA. Results suggest that current retrievers are the main bottleneck and that readers are already performing at the human level as long as relevant passages are provided. The system is also shown to be more effective when the model is induced to give explanations before answering a question. Code is available at {\textbackslash}url\{https://github.com/neuralmind-ai/visconde\}.},
	language = {en},
	urldate = {2023-09-26},
	journal = {arXiv.org},
	author = {Pereira, Jayr and Fidalgo, Robson and Lotufo, Roberto and Nogueira, Rodrigo},
	month = dec,
	year = {2022},
	keywords = {Background, Model, QA},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/DLZMVUSW/Pereira et al. - 2022 - Visconde Multi-document QA with GPT-3 and Neural .pdf:application/pdf},
}


@inproceedings{fajcik_r2-d2_2021,
	address = {Punta Cana, Dominican Republic},
	title = {R2-{D2}: {A} {Modular} {Baseline} for {Open}-{Domain} {Question} {Answering}},
	shorttitle = {R2-{D2}},
	url = {https://aclanthology.org/2021.findings-emnlp.73},
	doi = {10.18653/v1/2021.findings-emnlp.73},
	abstract = {This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank twice, reaD twice). The pipeline is composed of a retriever, passage reranker, extractive reader, generative reader and a mechanism that aggregates the final prediction from all system's components. We demonstrate its strength across three open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA, surpassing state-of-the-art on the first two. Our analysis demonstrates that: (i) combining extractive and generative reader yields absolute improvements up to 5 exact match and it is at least twice as effective as the posterior averaging ensemble of the same models with different parameters, (ii) the extractive reader with fewer parameters can match the performance of the generative reader on extractive QA datasets.},
	urldate = {2023-09-26},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Fajcik, Martin and Docekal, Martin and Ondrej, Karel and Smrz, Pavel},
	month = nov,
	year = {2021},
	keywords = {Main, Reader, Retrieval},
	pages = {854--870},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/BFJ55W2M/Fajcik et al. - 2021 - R2-D2 A Modular Baseline for Open-Domain Question.pdf:application/pdf},
}


@inproceedings{xu_laprador_2022,
	title = {{LaPraDoR}: {Unsupervised} {Pretrained} {Dense} {Retriever} for {Zero}-{Shot} {Text} {Retrieval}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{LaPraDoR}},
	url = {https://arxiv.org/abs/2203.06169},
	doi = {10.48550/ARXIV.2203.06169},
	abstract = {In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training. Specifically, we first present Iterative Contrastive Learning (ICoL) that iteratively trains the query and document encoders with a cache mechanism. ICoL not only enlarges the number of negative instances but also keeps representations of cached examples in the same hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a simple yet effective way to enhance dense retrieval with lexical matching. We evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18 datasets of 9 zero-shot text retrieval tasks. Experimental results show that LaPraDoR achieves state-of-the-art performance compared with supervised dense retrieval models, and further analysis reveals the effectiveness of our training strategy and objectives. Compared to re-ranking, our lexicon-enhanced approach can be run in milliseconds (22.5x faster) while achieving superior performance.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
	year = {2022},
	note = {Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Background, Computation and Language (cs.CL), FOS: Computer and information sciences, Information Retrieval (cs.IR), Machine Learning (cs.LG), Main, OOD, QA, Retrieval},
	annote = {Other
ACL 2022 (Findings)},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/4FIY9ESH/Xu et al. - 2022 - LaPraDoR Unsupervised Pretrained Dense Retriever .pdf:application/pdf},
}

@misc{zaib_conversational_2021,
	title = {Conversational {Question} {Answering}: {A} {Survey}},
	shorttitle = {Conversational {Question} {Answering}},
	url = {https://arxiv.org/abs/2106.00874v2},
	abstract = {Question answering (QA) systems provide a way of querying the information available in various formats including, but not limited to, unstructured and structured data in natural languages. It constitutes a considerable part of conversational artificial intelligence (AI) which has led to the introduction of a special research topic on Conversational Question Answering (CQA), wherein a system is required to understand the given context and then engages in multi-turn QA to satisfy the user's information needs. Whilst the focus of most of the existing research work is subjected to single-turn QA, the field of multi-turn QA has recently grasped attention and prominence owing to the availability of large-scale, multi-turn QA datasets and the development of pre-trained language models. With a good amount of models and research papers adding to the literature every year recently, there is a dire need of arranging and presenting the related work in a unified manner to streamline future research. This survey, therefore, is an effort to present a comprehensive review of the state-of-the-art research trends of CQA primarily based on reviewed papers from 2016-2021. Our findings show that there has been a trend shift from single-turn to multi-turn QA which empowers the field of Conversational AI from different perspectives. This survey is intended to provide an epitome for the research community with the hope of laying a strong foundation for the field of CQA.},
	language = {en},
	urldate = {2023-09-27},
	journal = {arXiv.org},
	author = {Zaib, Munazza and Zhang, Wei Emma and Sheng, Quan Z. and Mahmood, Adnan and Zhang, Yang},
	month = jun,
	year = {2021},
	keywords = {Background, CQA, overview},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/P42UR6GW/Zaib et al. - 2021 - Conversational Question Answering A Survey.pdf:application/pdf},
}


@misc{reddy_coqa_2018,
	title = {{CoQA}: {A} {Conversational} {Question} {Answering} {Challenge}},
	shorttitle = {{CoQA}},
	url = {https://arxiv.org/abs/1808.07042v2},
	abstract = {Humans gather information by engaging in conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong conversational and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4\%, which is 23.4 points behind human performance (88.8\%), indicating there is ample room for improvement. We launch CoQA as a challenge to the community at http://stanfordnlp.github.io/coqa/},
	language = {en},
	urldate = {2023-09-28},
	journal = {arXiv.org},
	author = {Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
	month = aug,
	year = {2018},
	keywords = {Background, basics, CQA, dataset},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/VK7GSNA3/Reddy et al. - 2018 - CoQA A Conversational Question Answering Challeng.pdf:application/pdf},
}


@inproceedings{gupta_conversational_2020,
	address = {Barcelona, Spain (Online)},
	title = {Conversational {Machine} {Comprehension}: a {Literature} {Review}},
	shorttitle = {Conversational {Machine} {Comprehension}},
	url = {https://aclanthology.org/2020.coling-main.247},
	doi = {10.18653/v1/2020.coling-main.247},
	abstract = {Conversational Machine Comprehension (CMC), a research track in conversational AI, expects the machine to understand an open-domain natural language text and thereafter engage in a multi-turn conversation to answer questions related to the text. While most of the research in Machine Reading Comprehension (MRC) revolves around single-turn question answering (QA), multi-turn CMC has recently gained prominence, thanks to the advancement in natural language understanding via neural language models such as BERT and the introduction of large-scale conversational datasets such as CoQA and QuAC. The rise in interest has, however, led to a flurry of concurrent publications, each with a different yet structurally similar modeling approach and an inconsistent view of the surrounding literature. With the volume of model submissions to conversational datasets increasing every year, there exists a need to consolidate the scattered knowledge in this domain to streamline future research. This literature review attempts at providing a holistic overview of CMC with an emphasis on the common trends across recently published models, specifically in their approach to tackling conversational history. The review synthesizes a generic framework for CMC models while highlighting the differences in recent approaches and intends to serve as a compendium of CMC for future researchers.},
	urldate = {2023-09-28},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Gupta, Somil and Rawat, Bhanu Pratap Singh and Yu, Hong},
	month = dec,
	year = {2020},
	keywords = {Background, basics, CQA},
	pages = {2739--2753},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/BEBVP43P/Gupta et al. - 2020 - Conversational Machine Comprehension a Literature.pdf:application/pdf},
}


@article{owoicho_trec_2022,
	title = {{TREC} {CAsT} 2022: {Going} {Beyond} {User} {Ask} and {System} {Retrieve} with {Initiative} and {Response} {Generation}},
	language = {en},
	author = {Owoicho, Paul and Dalton, Jeffrey and Aliannejadi, Mohammad and Azzopardi, Leif and Trippas, Johanne R and Vakulenko, Svitlana},
	year = {2022},
	keywords = {Background, CQA, groundwork},
	file = {Owoicho et al. - 2022 - TREC CAsT 2022 Going Beyond User Ask and System R.pdf:/Users/lenert/Zotero/storage/AEKWQX27/Owoicho et al. - 2022 - TREC CAsT 2022 Going Beyond User Ask and System R.pdf:application/pdf},
}


@misc{rastogi_schema-guided_2020,
	title = {Schema-{Guided} {Dialogue} {State} {Tracking} {Task} at {DSTC8}},
	url = {https://arxiv.org/abs/2002.01359v1},
	abstract = {This paper gives an overview of the Schema-Guided Dialogue State Tracking task of the 8th Dialogue System Technology Challenge. The goal of this task is to develop dialogue state tracking models suitable for large-scale virtual assistants, with a focus on data-efficient joint modeling across domains and zero-shot generalization to new APIs. This task provided a new dataset consisting of over 16000 dialogues in the training set spanning 16 domains to highlight these challenges, and a baseline model capable of zero-shot generalization to new APIs. Twenty-five teams participated, developing a range of neural network models, exceeding the performance of the baseline model by a very high margin. The submissions incorporated a variety of pre-trained encoders and data augmentation techniques. This paper describes the task definition, dataset and evaluation methodology. We also summarize the approach and results of the submitted systems to highlight the overall trends in the state-of-the-art.},
	language = {en},
	urldate = {2023-09-29},
	journal = {arXiv.org},
	author = {Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
	month = feb,
	year = {2020},
	keywords = {Background, basics, CQA},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/ID76YVS7/Rastogi et al. - 2020 - Schema-Guided Dialogue State Tracking Task at DSTC.pdf:application/pdf},
}


@inproceedings{voskarides_query_2020,
	title = {Query {Resolution} for {Conversational} {Search} with {Limited} {Supervision}},
	url = {http://arxiv.org/abs/2005.11723},
	doi = {10.1145/3397271.3401130},
	abstract = {In this work we focus on multi-turn passage retrieval as a crucial component of conversational search. One of the key challenges in multi-turn passage retrieval comes from the fact that the current turn query is often underspecified due to zero anaphora, topic change, or topic return. Context from the conversational history can be used to arrive at a better expression of the current turn query, defined as the task of query resolution. In this paper, we model the query resolution task as a binary term classification problem: for each term appearing in the previous turns of the conversation decide whether to add it to the current turn query or not. We propose QuReTeC (Query Resolution by Term Classification), a neural query resolution model based on bidirectional transformers. We propose a distant supervision method to automatically generate training data by using query-passage relevance labels. Such labels are often readily available in a collection either as human annotations or inferred from user interactions. We show that QuReTeC outperforms state-of-the-art models, and furthermore, that our distant supervision method can be used to substantially reduce the amount of human-curated data required to train QuReTeC. We incorporate QuReTeC in a multi-turn, multi-stage passage retrieval architecture and demonstrate its effectiveness on the TREC CAsT dataset.},
	urldate = {2023-09-29},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Voskarides, Nikos and Li, Dan and Ren, Pengjie and Kanoulas, Evangelos and de Rijke, Maarten},
	month = jul,
	year = {2020},
	note = {arXiv:2005.11723 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, context, CQA},
	pages = {921--930},
	annote = {Comment: SIGIR 2020 full conference paper},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/UXFU4WJK/Voskarides et al. - 2020 - Query Resolution for Conversational Search with Li.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/9ZNVM853/2005.html:text/html},
}


@article{yang_query_2019,
	title = {Query and {Answer} {Expansion} from {Conversation} {History}},
	abstract = {In this paper, we present our methods, experimental analysis, and final submissions for the Conversational Assistance Track (CAsT) at TREC 2019. In addition to language understanding, extracting knowledge from historical dialogues (e.g., previous queries, searching results) is a key to the conversational IR task. However, limited annotated data in the CAsT task makes machine learning or other data-driven approaches infeasible. Along this line, we propose two ad hoc and intuitive approaches: Historical Query Expansion and Historical Answer Expansion, to improve the performance of the conversational IR system with limited training data. Our empirical result on the CAsT training set shows that the proposed methods significantly improve the quality of conversational search in terms of retrieval (recall@1000: 0.774 → 0.844) and ranking (mAP: 0.187 → 0.197) compared to our strong baseline. As a result, our submitted entries outperform the median performance of all the 21 teams.},
	language = {en},
	author = {Yang, Jheng-Hong and Lin, Sheng-Chieh and Lin, Jimmy and Tsai, Ming-Feng and Wang, Chuan-Ju},
	year = {2019},
	keywords = {Background, context, CQA},
	file = {Yang et al. - 2019 - Query and Answer Expansion from Conversation Histo.pdf:/Users/lenert/Zotero/storage/R6QI87IJ/Yang et al. - 2019 - Query and Answer Expansion from Conversation Histo.pdf:application/pdf},
}


@misc{dalton_trec_2020,
	title = {{TREC} {CAsT} 2019: {The} {Conversational} {Assistance} {Track} {Overview}},
	shorttitle = {{TREC} {CAsT} 2019},
	url = {http://arxiv.org/abs/2003.13624},
	doi = {10.48550/arXiv.2003.13624},
	abstract = {The Conversational Assistance Track (CAsT) is a new track for TREC 2019 to facilitate Conversational Information Seeking (CIS) research and to create a large-scale reusable test collection for conversational search systems. The document corpus is 38,426,252 passages from the TREC Complex Answer Retrieval (CAR) and Microsoft MAchine Reading COmprehension (MARCO) datasets. Eighty information seeking dialogues (30 train, 50 test) are an average of 9 to 10 questions long. Relevance assessments are provided for 30 training topics and 20 test topics. This year 21 groups submitted a total of 65 runs using varying methods for conversational query understanding and ranking. Methods include traditional retrieval based methods, feature based learning-to-rank, neural models, and knowledge enhanced methods. A common theme through the runs is the use of BERT-based neural reranking methods. Leading methods also employed document expansion, conversational query expansion, and generative language models for conversational query rewriting (GPT-2). The results show a gap between automatic systems and those using the manually resolved utterances, with a 35\% relative improvement of manual rewrites over the best automatic system.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Dalton, Jeffrey and Xiong, Chenyan and Callan, Jamie},
	month = mar,
	year = {2020},
	note = {arXiv:2003.13624 [cs]},
	keywords = {Background, basics, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning, CQA},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/AFR23643/Dalton et al. - 2020 - TREC CAsT 2019 The Conversational Assistance Trac.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/4CWK8NTS/2003.html:text/html},
}


@article{mao_large_2023,
	title = {Large {Language} {Models} {Know} {Your} {Contextual} {Search} {Intent}: {A} {Prompting} {Framework} for {Conversational} {Search}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Large {Language} {Models} {Know} {Your} {Contextual} {Search} {Intent}},
	url = {https://arxiv.org/abs/2303.06573},
	doi = {10.48550/ARXIV.2303.06573},
	abstract = {In this paper, we present a prompting framework called LLMCS that leverages large language models, such as code-davinci-002 of GPT-3, to perform few-shot conversational query rewriting for conversational search. We explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose aggregating them into an integrated representation that can robustly represent the user's real contextual search intent. Experimental results on two conversational search datasets, including CAst-19 and CAsT-20, show that our approach achieves significant improvements in search effectiveness over existing baselines and manual rewrites. Notably, LLMCS can significantly outperform the state-of-the-art baselines by up to +5.9{\textbackslash}\% and +32.9{\textbackslash}\% w.r.t. NDCG@3 on CAsT-19 and CAsT-20, highlighting the vast potential of large language models for conversational search. Our code will be released at https://github.com/kyriemao/LLMCS.},
	urldate = {2023-09-29},
	author = {Mao, Kelong and Dou, Zhicheng and Chen, Haonan and Mo, Fengran and Qian, Hongjin},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Background, FOS: Computer and information sciences, Information Retrieval (cs.IR), CQA, context},
	annote = {Other
Work in progress},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/MRFP5LSC/Mao et al. - 2023 - Large Language Models Know Your Contextual Search .pdf:application/pdf},
}


@article{dai_dialog_2022,
	title = {Dialog {Inpainting}: {Turning} {Documents} into {Dialogs}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Dialog {Inpainting}},
	url = {https://arxiv.org/abs/2205.09073},
	doi = {10.48550/ARXIV.2205.09073},
	abstract = {Many important questions (e.g. "How to eat healthier?") require conversation to establish context and explore in depth. However, conversational question answering (ConvQA) systems have long been stymied by scarce training data that is expensive to collect. To address this problem, we propose a new technique for synthetically generating diverse and high-quality dialog data: dialog inpainting. Our approach takes the text of any document and transforms it into a two-person dialog between the writer and an imagined reader: we treat sentences from the article as utterances spoken by the writer, and then use a dialog inpainter to predict what the imagined reader asked or said in between each of the writer's utterances. By applying this approach to passages from Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets totalling 19 million diverse information-seeking dialogs -- 1,000x larger than the largest existing ConvQA dataset. Furthermore, human raters judge the answer adequacy and conversationality of WikiDialog to be as good or better than existing manually-collected datasets. Using our inpainted data to pre-train ConvQA retrieval systems, we significantly advance state-of-the-art across three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40\% relative gains on standard evaluation metrics.},
	urldate = {2023-09-28},
	author = {Dai, Zhuyun and Chaganty, Arun Tejasvi and Zhao, Vincent and Amini, Aida and Rashid, Qazi Mamunur and Green, Mike and Guu, Kelvin},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Background, basics, Computation and Language (cs.CL), CQA, FOS: Computer and information sciences, OOD},
	annote = {[TLDR] Using inpainted data to pre-train ConvQA retrieval systems, this work significantly advance state-of-the-art across three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40\% relative gains on standard evaluation metrics.},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/MZCXY4DC/Dai et al. - 2022 - Dialog Inpainting Turning Documents into Dialogs.pdf:application/pdf},
}


@article{liusie_university_nodate,
	title = {{UNIVERSITY} {OF} {CAMBRIDGE} {AT} {TREC} {CAST} 2022},
	abstract = {Team heatwave (of the University of Cambridge) submitted 3 automatic runs to the TREC 2022 Conversational Assistance Track. This paper discusses our approach to the challenge of conversational informational retrieval. We first describe our four stage approach of query reformulation, BM25 retrieval, passage reranking, and response extraction. Our experiments then show that our multi-query approach, which uses the raw concatenated conversational context for BM25 and the rewritten query for reranking, shows considerable performance improvement over a single-query approach, where our best performing system achieves a NDCG@3 of 0.440 in the 2022 CAsT challenge.},
	language = {en},
	author = {Liusie, Adian and Qian, Mengjie and Li, Xiang and Gales, Mark},
	keywords = {Background, context, CQA},
	file = {Liusie et al. - UNIVERSITY OF CAMBRIDGE AT TREC CAST 2022.pdf:/Users/lenert/Zotero/storage/BIN984IL/Liusie et al. - UNIVERSITY OF CAMBRIDGE AT TREC CAST 2022.pdf:application/pdf},
	year = {2022}
}


@inproceedings{elgohary_can_2019,
	address = {Hong Kong, China},
	title = {Can {You} {Unpack} {That}? {Learning} to {Rewrite} {Questions}-in-{Context}},
	shorttitle = {Can {You} {Unpack} {That}?},
	url = {https://aclanthology.org/D19-1605},
	doi = {10.18653/v1/D19-1605},
	abstract = {Question answering is an AI-complete problem, but existing datasets lack key elements of language understanding such as coreference and ellipsis resolution. We consider sequential question answering: multiple questions are asked one-by-one in a conversation between a questioner and an answerer. Answering these questions is only possible through understanding the conversation history. We introduce the task of question-in-context rewriting: given the context of a conversation's history, rewrite a context-dependent into a self-contained question with the same answer. We construct, CANARD, a dataset of 40,527 questions based on QuAC (Choi et al., 2018) and train Seq2Seq models for incorporating context into standalone questions.},
	urldate = {2023-09-29},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Elgohary, Ahmed and Peskov, Denis and Boyd-Graber, Jordan},
	month = nov,
	year = {2019},
	keywords = {Background, context, CQA},
	pages = {5918--5924},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/MTJJ779F/Elgohary et al. - 2019 - Can You Unpack That Learning to Rewrite Questions.pdf:application/pdf},
}


@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, CQA, LLM},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/BUUX82SU/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/5VH9M63P/2307.html:text/html},
}


@misc{anil_palm_2023,
	title = {{PaLM} 2 {Technical} {Report}},
	url = {http://arxiv.org/abs/2305.10403},
	doi = {10.48550/arXiv.2305.10403},
	abstract = {We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities. When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Anil, Rohan and Dai, Andrew M. and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and Chu, Eric and Clark, Jonathan H. and Shafey, Laurent El and Huang, Yanping and Meier-Hellstern, Kathy and Mishra, Gaurav and Moreira, Erica and Omernick, Mark and Robinson, Kevin and Ruder, Sebastian and Tay, Yi and Xiao, Kefan and Xu, Yuanzhong and Zhang, Yujing and Abrego, Gustavo Hernandez and Ahn, Junwhan and Austin, Jacob and Barham, Paul and Botha, Jan and Bradbury, James and Brahma, Siddhartha and Brooks, Kevin and Catasta, Michele and Cheng, Yong and Cherry, Colin and Choquette-Choo, Christopher A. and Chowdhery, Aakanksha and Crepy, Clément and Dave, Shachi and Dehghani, Mostafa and Dev, Sunipa and Devlin, Jacob and Díaz, Mark and Du, Nan and Dyer, Ethan and Feinberg, Vlad and Feng, Fangxiaoyu and Fienber, Vlad and Freitag, Markus and Garcia, Xavier and Gehrmann, Sebastian and Gonzalez, Lucas and Gur-Ari, Guy and Hand, Steven and Hashemi, Hadi and Hou, Le and Howland, Joshua and Hu, Andrea and Hui, Jeffrey and Hurwitz, Jeremy and Isard, Michael and Ittycheriah, Abe and Jagielski, Matthew and Jia, Wenhao and Kenealy, Kathleen and Krikun, Maxim and Kudugunta, Sneha and Lan, Chang and Lee, Katherine and Lee, Benjamin and Li, Eric and Li, Music and Li, Wei and Li, YaGuang and Li, Jian and Lim, Hyeontaek and Lin, Hanzhao and Liu, Zhongtao and Liu, Frederick and Maggioni, Marcello and Mahendru, Aroma and Maynez, Joshua and Misra, Vedant and Moussalem, Maysam and Nado, Zachary and Nham, John and Ni, Eric and Nystrom, Andrew and Parrish, Alicia and Pellat, Marie and Polacek, Martin and Polozov, Alex and Pope, Reiner and Qiao, Siyuan and Reif, Emily and Richter, Bryan and Riley, Parker and Ros, Alex Castro and Roy, Aurko and Saeta, Brennan and Samuel, Rajkumar and Shelby, Renee and Slone, Ambrose and Smilkov, Daniel and So, David R. and Sohn, Daniel and Tokumine, Simon and Valter, Dasha and Vasudevan, Vijay and Vodrahalli, Kiran and Wang, Xuezhi and Wang, Pidong and Wang, Zirui and Wang, Tao and Wieting, John and Wu, Yuhuai and Xu, Kelvin and Xu, Yunhan and Xue, Linting and Yin, Pengcheng and Yu, Jiahui and Zhang, Qiao and Zheng, Steven and Zheng, Ce and Zhou, Weikang and Zhou, Denny and Petrov, Slav and Wu, Yonghui},
	month = sep,
	year = {2023},
	note = {arXiv:2305.10403 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, LLM},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/WJC33IBS/Anil et al. - 2023 - PaLM 2 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/DJ8LYSRI/2305.html:text/html},
}


@article{treviso_efficient_2023,
	title = {Efficient {Methods} for {Natural} {Language} {Processing}: {A} {Survey}},
	volume = {11},
	issn = {2307-387X},
	shorttitle = {Efficient {Methods} for {Natural} {Language} {Processing}},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00577/116725/Efficient-Methods-for-Natural-Language-Processing},
	doi = {10.1162/tacl_a_00577},
	abstract = {Abstract
            Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.},
	language = {en},
	urldate = {2023-10-01},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty Van and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and Martins, Pedro H. and Martins, André F. T. and Forde, Jessica Zosa and Milder, Peter and Simpson, Edwin and Slonim, Noam and Dodge, Jesse and Strubell, Emma and Balasubramanian, Niranjan and Derczynski, Leon and Gurevych, Iryna and Schwartz, Roy},
	month = jul,
	year = {2023},
	keywords = {Background, LLM, overview},
	pages = {826--860},
	annote = {[TLDR] This survey synthesizes and relates current methods and findings in efficient NLP to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.},
	file = {Volltext:/Users/lenert/Zotero/storage/TESAP6NV/Treviso et al. - 2023 - Efficient Methods for Natural Language Processing.pdf:application/pdf},
}


@misc{ling_domain_2023,
	title = {Domain {Specialization} as the {Key} to {Make} {Large} {Language} {Models} {Disruptive}: {A} {Comprehensive} {Survey}},
	shorttitle = {Domain {Specialization} as the {Key} to {Make} {Large} {Language} {Models} {Disruptive}},
	url = {http://arxiv.org/abs/2305.18703},
	doi = {10.48550/arXiv.2305.18703},
	abstract = {Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Ling, Chen and Zhao, Xujiang and Lu, Jiaying and Deng, Chengyuan and Zheng, Can and Wang, Junxiang and Chowdhury, Tanmoy and Li, Yun and Cui, Hejie and Zhang, Xuchao and Zhao, Tianjiao and Panalkar, Amit and Cheng, Wei and Wang, Haoyu and Liu, Yanchi and Chen, Zhengzhang and Chen, Haifeng and White, Chris and Gu, Quanquan and Pei, Jian and Zhao, Liang},
	month = aug,
	year = {2023},
	note = {arXiv:2305.18703 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, LLM, overview},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/WC7XEBER/Ling et al. - 2023 - Domain Specialization as the Key to Make Large Lan.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/5SZ74W5X/2305.html:text/html},
}

@misc{zhao_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = sep,
	year = {2023},
	note = {arXiv:2303.18223 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, LLM, overview},
	annote = {Comment: ongoing work; 97 pages, 683 citations},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7NSW6ZLY/Zhao et al. - 2023 - A Survey of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/RMH3GJLW/2303.html:text/html},
}


@article{hu_lora_nodate,
	title = {{LORA}: {LOW}-{RANK} {ADAPTATION} {OF} {LARGE} {LAN}- {GUAGE} {MODELS}},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	author = {Hu, Edward and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	keywords = {Background, LLM, PEFT},
	file = {Hu et al. - LORA LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODE.pdf:/Users/lenert/Zotero/storage/8WF4K7RM/Hu et al. - LORA LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODE.pdf:application/pdf},
	year = {2021},
}


@misc{chung_scaling_2022,
	title = {Scaling {Instruction}-{Finetuned} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.11416},
	doi = {10.48550/arXiv.2210.11416},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	month = dec,
	year = {2022},
	note = {arXiv:2210.11416 [cs]},
	keywords = {Background, basics, Computer Science - Computation and Language, Computer Science - Machine Learning, LLM},
	annote = {Comment: Public checkpoints: https://huggingface.co/docs/transformers/model\_doc/flan-t5},
}


@misc{noauthor_peft_nodate,
	title = {{PEFT}},
	url = {https://huggingface.co/docs/peft/index},
	abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-10-01},
	keywords = {Background, LLM, PEFT},
	file = {Snapshot:/Users/lenert/Zotero/storage/K557ML9H/index.html:text/html},
	author= {Huggingface},
	year = {2023}
}


@misc{houlsby_parameter-efficient_2019,
	title = {Parameter-{Efficient} {Transfer} {Learning} for {NLP}},
	url = {http://arxiv.org/abs/1902.00751},
	doi = {10.48550/arXiv.1902.00751},
	abstract = {Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4\% of the performance of full fine-tuning, adding only 3.6\% parameters per task. By contrast, fine-tuning trains 100\% of the parameters per task.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	month = jun,
	year = {2019},
	note = {arXiv:1902.00751 [cs, stat]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, LLM, PEFT, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/X9KW255P/Houlsby et al. - 2019 - Parameter-Efficient Transfer Learning for NLP.pdf:application/pdf},
}


@misc{li_prefix-tuning_2021,
	title = {Prefix-{Tuning}: {Optimizing} {Continuous} {Prompts} for {Generation}},
	shorttitle = {Prefix-{Tuning}},
	url = {http://arxiv.org/abs/2101.00190},
	doi = {10.48550/arXiv.2101.00190},
	abstract = {Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1{\textbackslash}\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Li, Xiang Lisa and Liang, Percy},
	month = jan,
	year = {2021},
	note = {arXiv:2101.00190 [cs]},
	keywords = {Background, Computer Science - Computation and Language, LLM, PEFT},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/PGBG4NUV/Li und Liang - 2021 - Prefix-Tuning Optimizing Continuous Prompts for G.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/7A2M4VRB/2101.html:text/html},
}

@inproceedings{lester_power_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
	url = {https://aclanthology.org/2021.emnlp-main.243},
	doi = {10.18653/v1/2021.emnlp-main.243},
	abstract = {In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.},
	urldate = {2023-10-01},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	month = nov,
	year = {2021},
	keywords = {Background, LLM, PEFT},
	pages = {3045--3059},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/E7UW5QLV/Lester et al. - 2021 - The Power of Scale for Parameter-Efficient Prompt .pdf:application/pdf},
}


@article{liu_few-shot_2022,
	title = {Few-{Shot} {Parameter}-{Efficient} {Fine}-{Tuning} is {Better} and {Cheaper} than {In}-{Context} {Learning}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2205.05638},
	doi = {10.48550/ARXIV.2205.05638},
	abstract = {Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)\${\textasciicircum}3\$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6\% absolute. All of the code used in our experiments is publicly available.},
	urldate = {2023-10-01},
	author = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Background, overview, Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, LLM, Machine Learning (cs.LG), PEFT},
	annote = {[TLDR] This paper rigorously compares few-shot ICL and PEFT and demonstrates that the latter offers better accuracy as well as dramatically lower computational costs, and introduces a new PEFT method called (IA)\${\textasciicircum}3\$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters.},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/AN4GV9KA/Liu et al. - 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better.pdf:application/pdf},
}


@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Background, basics, Computer Science - Computation and Language, LLM},
	annote = {Comment: 40+32 pages},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/VWFX3Y4Q/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/IPA4EMJQ/2005.html:text/html},
}

@misc{white_prompt_2023,
	title = {A {Prompt} {Pattern} {Catalog} to {Enhance} {Prompt} {Engineering} with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.11382},
	doi = {10.48550/arXiv.2302.11382},
	abstract = {Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11382 [cs]},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Software Engineering, LLM},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/AVAQJNIM/White et al. - 2023 - A Prompt Pattern Catalog to Enhance Prompt Enginee.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZQR97QQC/2302.html:text/html},
}


@misc{liu_gpt_2021,
	title = {{GPT} {Understands}, {Too}},
	url = {http://arxiv.org/abs/2103.10385},
	doi = {10.48550/arXiv.2103.10385},
	abstract = {While GPTs with traditional fine-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning -- which employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64{\textbackslash}\% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we find that P-tuning also improves BERTs' performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, P-tuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
	month = mar,
	year = {2021},
	note = {arXiv:2103.10385 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, LLM, PEFT},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/EA9277FW/Liu et al. - 2021 - GPT Understands, Too.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/7KXKDRCW/2103.html:text/html},
}


@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Background, LLM, CoT, CQA},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/X4UW5WIS/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/J7IZ65BJ/2201.html:text/html},
}


@misc{zhu_survey_2023,
	title = {A {Survey} on {Model} {Compression} for {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2308.07633v3},
	abstract = {Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of compressed LLMs. By providing insights into the latest developments and practical implications, this survey serves as an invaluable resource for both researchers and practitioners. As LLMs continue to evolve, this survey aims to facilitate enhanced efficiency and real-world applicability, establishing a foundation for future advancements in the field.},
	language = {en},
	urldate = {2023-10-02},
	journal = {arXiv.org},
	author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
	month = aug,
	year = {2023},
	keywords = {Background, compression, LLM},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/BRAAL7EM/Zhu et al. - 2023 - A Survey on Model Compression for Large Language M.pdf:application/pdf},
}



@misc{fang_depgraph_2023,
	title = {{DepGraph}: {Towards} {Any} {Structural} {Pruning}},
	shorttitle = {{DepGraph}},
	url = {http://arxiv.org/abs/2301.12900},
	doi = {10.48550/arXiv.2301.12900},
	abstract = {Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant, thereby avoiding structural issues and significant performance degradation after pruning. To address this problem, we propose a general and \{fully automatic\} method, {\textbackslash}emph\{Dependency Graph\} (DepGraph), to explicitly model the dependency between layers and comprehensively group coupled parameters for pruning. In this work, we extensively evaluate our method on several architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and Vision transformer for images, GAT for graph, DGCNN for 3D point cloud, alongside LSTM for language, and demonstrate that, even with a simple norm-based criterion, the proposed method consistently yields gratifying performances.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
	month = mar,
	year = {2023},
	note = {arXiv:2301.12900 [cs]},
	keywords = {Background, compression, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, LLM},
}


@misc{frantar_sparsegpt_2023,
	title = {{SparseGPT}: {Massive} {Language} {Models} {Can} {Be} {Accurately} {Pruned} in {One}-{Shot}},
	shorttitle = {{SparseGPT}},
	url = {https://arxiv.org/abs/2301.00774v3},
	abstract = {We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50\% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60\% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.},
	language = {en},
	urldate = {2023-10-02},
	journal = {arXiv.org},
	author = {Frantar, Elias and Alistarh, Dan},
	month = jan,
	year = {2023},
	keywords = {Background, compression, LLM},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/JM22J3PL/Frantar und Alistarh - 2023 - SparseGPT Massive Language Models Can Be Accurate.pdf:application/pdf},
}


@misc{noauthor_deepsparse_2023,
	title = {{DeepSparse}},
	copyright = {Apache-2.0},
	url = {https://github.com/neuralmagic/deepsparse},
	abstract = {Inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application},
	urldate = {2023-10-02},
	publisher = {Neural Magic},
	month = oct,
	year = {2023},
	note = {original-date: 2020-12-14T17:40:38Z},
	keywords = {auto-ml, Background, compression, computer-vision, cpu-inference-api, cpus, deepsparse-engine, inference, LLM, machinelearning, ml, nlp, object-detection, onnx, pretrained-models, pruning, pytorch, quantization, sparsification},
}



@misc{ma_llm-pruner_2023,
	title = {{LLM}-{Pruner}: {On} the {Structural} {Pruning} of {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Pruner}},
	url = {http://arxiv.org/abs/2305.11627},
	doi = {10.48550/arXiv.2305.11627},
	abstract = {Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
	month = sep,
	year = {2023},
	note = {arXiv:2305.11627 [cs]},
	keywords = {Background, compression, Computer Science - Computation and Language, LLM},
	annote = {Comment: Accepted at NeurIPS 2023},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/HX6KEILZ/Ma et al. - 2023 - LLM-Pruner On the Structural Pruning of Large Lan.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/GX3SH3NB/2305.html:text/html},
}


@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	doi = {10.48550/arXiv.1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Background, compression, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, LLM, Statistics - Machine Learning},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/S38Z4JPW/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/JDKYW9IV/1503.html:text/html},
}


@article{zhu_teach_nodate,
	title = {Teach {Less}, {Learn} {More}: {On} the {Undistillable} {Classes} in {Knowledge} {Distillation}},
	abstract = {Knowledge distillation (KD) can effectively compress neural networks by training a smaller network (student) to simulate the behavior of a larger one (teacher). A counter-intuitive observation is that a more expansive teacher does not make a better student, but the reasons for this phenomenon remain unclear. In this paper, we demonstrate that this is directly attributed to the presence of undistillable classes: when trained with distillation, the teacher’s knowledge of some classes is incomprehensible to the student model. We observe that while KD improves the overall accuracy, it is at the cost of the model becoming inaccurate in these undistillable classes. After establishing their widespread existence in state-ofthe-art distillation methods, we illustrate their correlation with the capacity gap between teacher and student models. Finally, we present a simple “Teach Less Learn More” (TLLM) framework to identify and discard the undistillable classes during training. We validate the effectiveness of our approach on multiple datasets with varying network architectures. In all settings, our proposed method is able to exceed the performance of competitive state-of-the-art techniques.},
	language = {en},
	author = {Zhu, Yichen and Liu, Ning and Xu, Zhiyuan and Liu, Xin and Meng, Weibing and Wang, Yi},
	keywords = {Background, compression, LLM},
	file = {Zhu et al. - Teach Less, Learn More On the Undistillable Class.pdf:/Users/lenert/Zotero/storage/Y5D8CS6Y/Zhu et al. - Teach Less, Learn More On the Undistillable Class.pdf:application/pdf},
	year = {2022}
}



@misc{gu_knowledge_2023,
	title = {Knowledge {Distillation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2306.08543},
	doi = {10.48550/arXiv.2306.08543},
	abstract = {Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge from white-box generative LLMs is still under-explored, which becomes more and more important with the prosperity of LLMs. In this work, we propose MiniLLM that distills smaller language models from generative larger language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. Extensive experiments in the instruction-following setting show that the MiniLLM models generate more precise responses with the higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance. Our method is also scalable for different model families with 120M to 13B parameters. We will release our code and model checkpoints at https://aka.ms/MiniLLM.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
	month = jun,
	year = {2023},
	note = {arXiv:2306.08543 [cs]},
	keywords = {Background, compression, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, LLM},
	annote = {Comment: 20 pages, 12 figures},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/32BTGCA9/Gu et al. - 2023 - Knowledge Distillation of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/AUR3UWX6/2306.html:text/html},
}


@misc{huang_-context_2022,
	title = {In-context {Learning} {Distillation}: {Transferring} {Few}-shot {Learning} {Ability} of {Pre}-trained {Language} {Models}},
	shorttitle = {In-context {Learning} {Distillation}},
	url = {http://arxiv.org/abs/2212.10670},
	doi = {10.48550/arXiv.2212.10670},
	abstract = {Given the success with in-context learning of large pre-trained language models, we introduce in-context learning distillation to transfer in-context few-shot learning ability from large models to smaller models. We propose to combine in-context learning objectives with language modeling objectives to distill both the ability to read in-context examples and task knowledge to the smaller models. We perform in-context learning distillation under two different few-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and Multitask In-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitask few-shot learning but also requires more computation than Meta-ICT. Our method shows consistent improvements for both Meta-ICT and Multitask-ICT on two benchmarks: LAMA and CrossFit. Our extensive experiments and analysis reveal that in-context learning objectives and language modeling objectives are complementary under the Multitask-ICT paradigm. In-context learning objectives achieve the best performance when combined with language modeling objectives.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Huang, Yukun and Chen, Yanda and Yu, Zhou and McKeown, Kathleen},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10670 [cs]},
	keywords = {Background, compression, Computer Science - Computation and Language, Computer Science - Machine Learning, LLM},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7QJDGN4J/Huang et al. - 2022 - In-context Learning Distillation Transferring Few.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/H9ZJYS6N/2212.html:text/html},
}


@misc{li_explanations_2022,
	title = {Explanations from {Large} {Language} {Models} {Make} {Small} {Reasoners} {Better}},
	url = {http://arxiv.org/abs/2210.06726},
	doi = {10.48550/arXiv.2210.06726},
	abstract = {Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5\% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Li, Shiyang and Chen, Jianshu and Shen, Yelong and Chen, Zhiyu and Zhang, Xinlu and Li, Zekun and Wang, Hong and Qian, Jing and Peng, Baolin and Mao, Yi and Chen, Wenhu and Yan, Xifeng},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06726 [cs]},
	keywords = {Background, compression, Computer Science - Computation and Language, LLM},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/9ELILQFW/Li et al. - 2022 - Explanations from Large Language Models Make Small.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/5I8W66V2/2210.html:text/html},
}



@misc{jiang_lion_2023,
	title = {Lion: {Adversarial} {Distillation} of {Closed}-{Source} {Large} {Language} {Model}},
	shorttitle = {Lion},
	url = {http://arxiv.org/abs/2305.12870},
	doi = {10.48550/arXiv.2305.12870},
	abstract = {The practice of transferring knowledge from a sophisticated, closed-source large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any reciprocal "feedback"--identifying challenging instructions where the student model's performance falls short--to boost the student model's proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the closed-source model to identify "hard" instructions and generate new "hard" instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a 7B student model (named Lion), achieving nearly 95\% capability approximation using a mere 70k training data. We aspire that this proposed model may serve as the baseline to reflect the performance of ChatGPT, especially the open-source instruction-following language model baseline for our community.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Jiang, Yuxin and Chan, Chunkit and Chen, Mingyang and Wang, Wei},
	month = may,
	year = {2023},
	note = {arXiv:2305.12870 [cs]},
	keywords = {Background, compression, Computer Science - Computation and Language, LLM},
	annote = {Comment: work in progress},
}

@misc{frantar_gptq_2023,
	title = {{GPTQ}: {Accurate} {Post}-{Training} {Quantization} for {Generative} {Pre}-trained {Transformers}},
	shorttitle = {{GPTQ}},
	url = {http://arxiv.org/abs/2210.17323},
	doi = {10.48550/arXiv.2210.17323},
	abstract = {Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.17323 [cs]},
	keywords = {Background, compression, Computer Science - Machine Learning, LLM},
	annote = {Comment: ICLR 2023},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/B8AEBAXP/Frantar et al. - 2023 - GPTQ Accurate Post-Training Quantization for Gene.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/6V53VLXP/2210.html:text/html},
}


@misc{gholami_survey_2021,
	title = {A {Survey} of {Quantization} {Methods} for {Efficient} {Neural} {Network} {Inference}},
	url = {http://arxiv.org/abs/2103.13630},
	doi = {10.48550/arXiv.2103.13630},
	abstract = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
	month = jun,
	year = {2021},
	note = {arXiv:2103.13630 [cs]},
	keywords = {Background, compression, Computer Science - Computer Vision and Pattern Recognition, LLM},
	annote = {Comment: Book Chapter: Low-Power Computer Vision: Improving the Efficiency of Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/ICYJNW6D/Gholami et al. - 2021 - A Survey of Quantization Methods for Efficient Neu.pdf:application/pdf},
}


@misc{noauthor_quantize_nodate,
	title = {Quantize {Transformers} models},
	url = {https://huggingface.co/docs/transformers/main_classes/quantization},
	abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-10-02},
	keywords = {Background, compression, LLM},
	file = {Snapshot:/Users/lenert/Zotero/storage/QQ6L8NEX/quantization.html:text/html},
	year = {2023}
}


@misc{liu_llm-qat_2023,
	title = {{LLM}-{QAT}: {Data}-{Free} {Quantization} {Aware} {Training} for {Large} {Language} {Models}},
	shorttitle = {{LLM}-{QAT}},
	url = {http://arxiv.org/abs/2305.17888},
	doi = {10.48550/arXiv.2305.17888},
	abstract = {Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
	month = may,
	year = {2023},
	note = {arXiv:2305.17888 [cs]},
	keywords = {Background, compression, Computer Science - Computation and Language, LLM},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/EDYY565R/Liu et al. - 2023 - LLM-QAT Data-Free Quantization Aware Training for.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/J6A3DWHH/2305.html:text/html},
}


@misc{frantar_optimal_2023,
	title = {Optimal {Brain} {Compression}: {A} {Framework} for {Accurate} {Post}-{Training} {Quantization} and {Pruning}},
	shorttitle = {Optimal {Brain} {Compression}},
	url = {http://arxiv.org/abs/2208.11580},
	doi = {10.48550/arXiv.2208.11580},
	abstract = {We consider the problem of model compression for deep neural networks (DNNs) in the challenging one-shot/post-training setting, in which we are given an accurate trained model, and must compress it without any retraining, based only on a small amount of calibration input data. This problem has become popular in view of the emerging software and hardware support for executing models compressed via pruning and/or quantization with speedup, and well-performing solutions have been proposed independently for both compression approaches. In this paper, we introduce a new compression framework which covers both weight pruning and quantization in a unified setting, is time- and space-efficient, and considerably improves upon the practical performance of existing post-training methods. At the technical level, our approach is based on an exact and efficient realization of the classical Optimal Brain Surgeon (OBS) framework of [LeCun, Denker, and Solla, 1990] extended to also cover weight quantization at the scale of modern DNNs. From the practical perspective, our experimental results show that it can improve significantly upon the compression-accuracy trade-offs of existing post-training methods, and that it can enable the accurate compound application of both pruning and quantization in a post-training setting.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Frantar, Elias and Singh, Sidak Pal and Alistarh, Dan},
	month = jan,
	year = {2023},
	note = {arXiv:2208.11580 [cs]},
	keywords = {Background, Computer Science - Machine Learning, LLM, compression},
	annote = {Comment: Published at NeurIPS 2022},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/9XX7LUM3/Frantar et al. - 2023 - Optimal Brain Compression A Framework for Accurat.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/W2BAZCDB/2208.html:text/html},
}

@misc{william_autogptq_2023,
	title = {{AutoGPTQ}},
	copyright = {MIT},
	url = {https://github.com/PanQiWei/AutoGPTQ},
	abstract = {An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm.},
	urldate = {2023-10-02},
	author = {William},
	month = oct,
	year = {2023},
	note = {original-date: 2023-04-13T02:18:11Z},
	keywords = {Background, compression, deep-learning, inference, large-language-models, LLM, llms, nlp, pytorch, quantization, transformer, transformers},
}


@misc{noauthor_generative_nodate,
	title = {Generative {AI} applications with {Vertex} {AI} {PaLM} 2 {Models} and {LangChain}},
	url = {https://cloud.google.com/blog/products/ai-machine-learning/generative-ai-applications-with-vertex-ai-palm-2-models-and-langchain},
	abstract = {Augmenting LLMs with external systems is key to building GenAI apps. Learn how you can easily build such apps using Vertex AI PaLM API \& LangChain.},
	language = {en},
	author= {GCS},
	urldate = {2023-10-03},
	journal = {Google Cloud Blog},
	keywords = {Main},
	file = {Snapshot:/Users/lenert/Zotero/storage/W7QUS56Y/generative-ai-applications-with-vertex-ai-palm-2-models-and-langchain.html:text/html},
	year = {2023}
}


@misc{noauthor_quickly_2023,
	title = {Quickly build high-accuracy {Generative} {AI} applications on enterprise data using {Amazon} {Kendra}, {LangChain}, and large language models {\textbar} {AWS} {Machine} {Learning} {Blog}},
	url = {https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/},
	language = {en-US},
	urldate = {2023-10-03},
	month = may,
	author = {AWS},
	year = {2023},
	note = {Section: Amazon Kendra},
	keywords = {Main},
	file = {Snapshot:/Users/lenert/Zotero/storage/ST44X2HK/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-l.html:text/html},
}

