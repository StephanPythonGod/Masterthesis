
@misc{zhu_retrieving_2021,
	title = {Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering},
	url = {http://arxiv.org/abs/2101.00774},
	doi = {10.48550/arXiv.2101.00774},
	shorttitle = {Retrieving and Reading},
	abstract = {Open-domain Question Answering ({OpenQA}) is an important task in Natural Language Processing ({NLP}), which aims to answer a question in the form of natural language based on large-scale unstructured documents. Recently, there has been a surge in the amount of research literature on {OpenQA}, particularly on techniques that integrate with neural Machine Reading Comprehension ({MRC}). While these research works have advanced performance to new heights on benchmark datasets, they have been rarely covered in existing surveys on {QA} systems. In this work, we review the latest research trends in {OpenQA}, with particular attention to systems that incorporate neural {MRC} techniques. Specifically, we begin with revisiting the origin and development of {OpenQA} systems. We then introduce modern {OpenQA} architecture named "Retriever-Reader" and analyze the various systems that follow this architecture as well as the specific techniques adopted in each of the components. We then discuss key challenges to developing {OpenQA} systems and offer an analysis of benchmarks that are commonly used. We hope our work would enable researchers to be informed of the recent advancement and also the open challenges in {OpenQA} research, so as to stimulate further progress in this field.},
	number = {{arXiv}:2101.00774},
	publisher = {{arXiv}},
	author = {Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
	urldate = {2023-08-27},
	date = {2021-05-08},
	eprinttype = {arxiv},
	eprint = {2101.00774 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Example Architectures, overview, {QA}, {RRS}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7JSH329N/Zhu et al. - 2021 - Retrieving and Reading A Comprehensive Survey on .pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/6WWFWPPE/2101.html:text/html},
}

@misc{zamani_conversational_2023,
	title = {Conversational Information Seeking},
	url = {http://arxiv.org/abs/2201.08808},
	doi = {10.48550/arXiv.2201.08808},
	abstract = {Conversational information seeking ({CIS}) is concerned with a sequence of interactions between one or more users and an information system. Interactions in {CIS} are primarily based on natural language dialogue, while they may include other types of interactions, such as click, touch, and body gestures. This monograph provides a thorough overview of {CIS} definitions, applications, interactions, interfaces, design, implementation, and evaluation. This monograph views {CIS} applications as including conversational search, conversational question answering, and conversational recommendation. Our aim is to provide an overview of past research related to {CIS}, introduce the current state-of-the-art in {CIS}, highlight the challenges still being faced in the community. and suggest future directions.},
	number = {{arXiv}:2201.08808},
	publisher = {{arXiv}},
	author = {Zamani, Hamed and Trippas, Johanne R. and Dalton, Jeff and Radlinski, Filip},
	urldate = {2023-08-27},
	date = {2023-01-25},
	eprinttype = {arxiv},
	eprint = {2201.08808 [cs]},
	keywords = {Background, {CIS}, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval, overview, {QA}, {SvD}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7WFGQWLT/Zamani et al. - 2023 - Conversational Information Seeking.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/MBB85CRQ/2201.html:text/html},
}

@misc{gao_neural_2022,
	title = {Neural Approaches to Conversational Information Retrieval},
	url = {http://arxiv.org/abs/2201.05176},
	doi = {10.48550/arXiv.2201.05176},
	abstract = {A conversational information retrieval ({CIR}) system is an information retrieval ({IR}) system with a conversational interface which allows users to interact with the system to seek information via multi-turn conversations of natural language, in spoken or written form. Recent progress in deep learning has brought tremendous improvements in natural language processing ({NLP}) and conversational {AI}, leading to a plethora of commercial conversational services that allow naturally spoken and typed interaction, increasing the need for more human-centric interactions in {IR}. As a result, we have witnessed a resurgent interest in developing modern {CIR} systems in both research communities and industry. This book surveys recent advances in {CIR}, focusing on neural approaches that have been developed in the last few years. This book is based on the authors' tutorial at {SIGIR}'2020 (Gao et al., 2020b), with {IR} and {NLP} communities as the primary target audience. However, audiences with other background, such as machine learning and human-computer interaction, will also find it an accessible introduction to {CIR}. We hope that this book will prove a valuable resource for students, researchers, and software developers. This manuscript is a working draft. Comments are welcome.},
	number = {{arXiv}:2201.05176},
	publisher = {{arXiv}},
	author = {Gao, Jianfeng and Xiong, Chenyan and Bennett, Paul and Craswell, Nick},
	urldate = {2023-08-27},
	date = {2022-01-13},
	eprinttype = {arxiv},
	eprint = {2201.05176 [cs]},
	keywords = {Background, {CIS}, Computer Science - Computation and Language, Computer Science - Information Retrieval, overview, {QA}, {SvD}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/RZ38AQSJ/Gao et al. - 2022 - Neural Approaches to Conversational Information Re.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/5UH2JCTT/2201.html:text/html},
}

@misc{pandya_question_2021,
	title = {Question Answering Survey: Directions, Challenges, Datasets, Evaluation Matrices},
	url = {http://arxiv.org/abs/2112.03572},
	doi = {10.48550/arXiv.2112.03572},
	shorttitle = {Question Answering Survey},
	abstract = {The usage and amount of information available on the internet increase over the past decade. This digitization leads to the need for automated answering system to extract fruitful information from redundant and transitional knowledge sources. Such systems are designed to cater the most prominent answer from this giant knowledge source to the user query using natural language understanding ({NLU}) and thus eminently depends on the Question-answering({QA}) field. Question answering involves but not limited to the steps like mapping of user question to pertinent query, retrieval of relevant information, finding the best suitable answer from the retrieved information etc. The current improvement of deep learning models evince compelling performance improvement in all these tasks. In this review work, the research directions of {QA} field are analyzed based on the type of question, answer type, source of evidence-answer, and modeling approach. This detailing followed by open challenges of the field like automatic question generation, similarity detection and, low resource availability for a language. In the end, a survey of available datasets and evaluation measures is presented.},
	number = {{arXiv}:2112.03572},
	publisher = {{arXiv}},
	author = {Pandya, Hariom A. and Bhatt, Brijesh S.},
	urldate = {2023-09-19},
	date = {2021-12-07},
	eprinttype = {arxiv},
	eprint = {2112.03572 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval, Computer Science - Machine Learning, overview, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/88TJBBSV/Pandya und Bhatt - 2021 - Question Answering Survey Directions, Challenges,.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/87EC3RU7/2112.html:text/html},
}

@article{hao_recent_2022,
	title = {Recent progress in leveraging deep learning methods for question answering},
	volume = {34},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-021-06748-3},
	doi = {10.1007/s00521-021-06748-3},
	abstract = {Question answering, serving as one of important tasks in natural language processing, enables machines to understand questions in natural language and answer the questions concisely. From web search to expert systems, question answering systems are widely applied to various domains in assisting information seeking. Deep learning methods have boosted various tasks of question answering and have demonstrated dramatic effects in performance improvement for essential steps of question answering. Thus, leveraging deep learning methods for question answering has drawn much attention from both academia and industry in recent years. This paper provides a systematic review of the recent development of deep learning methods for question answering. The survey covers the scope including methods, datasets, and applications. The methods are discussed in terms of network structure characteristics, methodology innovations, and their effectiveness. The survey is expected to be a contribution to the summarization of recent research progress and future directions of deep learning methods for question answering.},
	pages = {2765--2783},
	number = {4},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Hao, Tianyong and Li, Xinxin and He, Yulan and Wang, Fu Lee and Qu, Yingying},
	urldate = {2023-09-19},
	date = {2022-02},
	langid = {english},
	keywords = {Background, basics, overview, {QA}},
	file = {Hao et al. - 2022 - Recent progress in leveraging deep learning method.pdf:/Users/lenert/Zotero/storage/247GDDR2/Hao et al. - 2022 - Recent progress in leveraging deep learning method.pdf:application/pdf},
}

@article{etezadi_state_2023,
	title = {The state of the art in open domain complex question answering: a survey},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-022-03732-9},
	doi = {10.1007/s10489-022-03732-9},
	shorttitle = {The state of the art in open domain complex question answering},
	abstract = {Research on question answering ({QA}) systems has a long tradition. {QA} systems, as widely used systems in various applications, seek to find the answers to the given questions through the available resources. These systems are expected to be capable of answering various types of questions, including simple questions whose answers can be found in a single passage or sentence and complex questions which need more complicated reasoning to find the answer or their answer should be found by traversing several relations. Nowadays, answering complex questions from texts or structured data is a challenge in {QA} systems. In this paper, we have a comparative study on {QA} approaches and systems for answering complex questions. For this purpose, firstly, this paper discusses what a complex question is and surveys different types of constraints that may appear in complex questions. Furthermore, it addresses the challenges of these types of questions, the methods proposed to deal with them, and benchmark datasets used to evaluate their strengths and weaknesses.},
	pages = {4124--4144},
	number = {4},
	journaltitle = {Applied Intelligence},
	shortjournal = {Appl Intell},
	author = {Etezadi, Romina and Shamsfard, Mehrnoush},
	urldate = {2023-09-19},
	date = {2023-02},
	langid = {english},
	keywords = {Background, basics, overview, {QA}},
	file = {Etezadi und Shamsfard - 2023 - The state of the art in open domain complex questi.pdf:/Users/lenert/Zotero/storage/TTQ32SLA/Etezadi und Shamsfard - 2023 - The state of the art in open domain complex questi.pdf:application/pdf},
}

@book{jurafsky_speech_2023,
	location = {Palo Alto},
	edition = {3},
	title = {Speech and Language Processing},
	abstract = {An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
	author = {Jurafsky, Dan and Martin, James H.},
	date = {2023-01-03},
	file = {Speech and Language Processing.pdf:/Users/lenert/Zotero/storage/3K7TMW7N/14.pdf:application/pdf},
}

@misc{farea_evaluation_2022,
	title = {Evaluation of Question Answering Systems: Complexity of judging a natural language},
	url = {http://arxiv.org/abs/2209.12617},
	doi = {10.48550/arXiv.2209.12617},
	shorttitle = {Evaluation of Question Answering Systems},
	abstract = {Question answering ({QA}) systems are among the most important and rapidly developing research topics in natural language processing ({NLP}). A reason, therefore, is that a {QA} system allows humans to interact more naturally with a machine, e.g., via a virtual assistant or search engine. In the last decades, many {QA} systems have been proposed to address the requirements of different question-answering tasks. Furthermore, many error scores have been introduced, e.g., based on n-gram matching, word embeddings, or contextual embeddings to measure the performance of a {QA} system. This survey attempts to provide a systematic overview of the general framework of {QA}, {QA} paradigms, benchmark datasets, and assessment techniques for a quantitative evaluation of {QA} systems. The latter is particularly important because not only is the construction of a {QA} system complex but also its evaluation. We hypothesize that a reason, therefore, is that the quantitative formalization of human judgment is an open problem.},
	number = {{arXiv}:2209.12617},
	publisher = {{arXiv}},
	author = {Farea, Amer and Yang, Zhen and Duong, Kien and Perera, Nadeesha and Emmert-Streib, Frank},
	urldate = {2023-09-20},
	date = {2022-09-10},
	eprinttype = {arxiv},
	eprint = {2209.12617 [cs]},
	keywords = {Background, basics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, overview, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/UATP8ZJ9/Farea et al. - 2022 - Evaluation of Question Answering Systems Complexi.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/XP44XJ2E/2209.html:text/html;Full Text PDF:/Users/lenert/Zotero/storage/ISM7H746/Farea et al. - 2022 - Evaluation of Question Answering Systems Complexi.pdf:application/pdf},
}


@inproceedings{voorhees_trec-8_1999,
	title = {The {TREC}-8 Question Answering Track Report},
	url = {https://www.semanticscholar.org/paper/The-TREC-8-Question-Answering-Track-Report-Voorhees/646d4888871aca2a25111eb2520e4c47e253b014},
	abstract = {The {TREC}-8 Question Answering track was the  first large-scale evaluation of domain-independent question answering systems. This paper summarizes the results of the track by giving a brief overview of the different approaches taken to solve the problem. The most accurate systems found a correct response for more than 2/3 of the questions. Relatively simple bag-of-words approaches were adequate for  finding answers when responses could be as long as a paragraph (250 bytes), but more sophisticated processing was necessary for more direct responses (50 bytes). 
 
The {TREC}-8 Question Answering track was an initial e ort to bring the bene ts of large-scale evaluation to bear on a question answering ({QA}) task. The goal in the {QA} task is to retrieve small snippets of text that contain the actual answer to a question rather than the document lists traditionally returned by text retrieval systems. The assumption is that users would usually prefer to be given the answer rather than  and the answer themselves in a document. 
 
This paper summarizes the retrieval results of the track; a companion paper ({\textbackslash}The {TREC}-8 Question Answering Track Evaluation") gives details about how the evaluation was implemented. By necessity, a track report can give only an overview of the different approaches used in the track. Readers are urged to consult the participants' papers elsewhere in the Proceedings for details regarding a particular approach.},
	eventtitle = {Text Retrieval Conference},
	author = {Voorhees, E.},
	urldate = {2023-09-19},
	date = {1999},
	keywords = {Background, basics, groundwork, historical, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/RSDRLSMP/Voorhees - 1999 - The TREC-8 Question Answering Track Report.pdf:application/pdf},
}

@inproceedings{green_baseball_1961,
	location = {New York, {NY}, {USA}},
	title = {Baseball: an automatic question-answerer},
	isbn = {978-1-4503-7872-7},
	url = {https://dl.acm.org/doi/10.1145/1460690.1460714},
	doi = {10.1145/1460690.1460714},
	series = {{IRE}-{AIEE}-{ACM} '61 (Western)},
	shorttitle = {Baseball},
	abstract = {{\textless}u{\textgreater}Baseball{\textless}/u{\textgreater} is a computer program that answers questions phrased in ordinary English about stored data. The program reads the question from punched cards. After the words and idioms are looked up in a dictionary, the phrase structure and other syntactic facts are determined for a content analysis, which lists attribute-value pairs specifying the information given and the information requested. The requested information is then extracted from the data matching the specifications, and any necessary processing is done. Finally, the answer is printed. The program's present context is baseball games; it answers such questions as "Where did each team play on July 7?"},
	pages = {219--224},
	booktitle = {Papers presented at the May 9-11, 1961, western joint {IRE}-{AIEE}-{ACM} computer conference},
	publisher = {Association for Computing Machinery},
	author = {Green, Bert F. and Wolf, Alice K. and Chomsky, Carol and Laughery, Kenneth},
	urldate = {2023-09-19},
	date = {1961-05-09},
	keywords = {Background, groundwork, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/A9B5I6B8/Green et al. - 1961 - Baseball an automatic question-answerer.pdf:application/pdf},
}

@article{ferrucci_introduction_2012,
	title = {Introduction to This is Watson},
	volume = {56},
	issn = {0018-8646},
	doi = {10.1147/JRD.2012.2184356},
	abstract = {In 2007, {IBM} Research took on the grand challenge of building a computer system that could compete with champions at the game of Jeopardy!™. In 2011, the open-domain question-answering ({QA}) system, dubbed Watson, beat the two highest ranked players in a nationally televised two-game Jeopardy! match. This paper provides a brief history of the events and ideas that positioned our team to take on the Jeopardy! challenge, build Watson, {IBM} Watson™, and ultimately triumph. It describes both the nature of the {QA} challenge represented by Jeopardy! and our overarching technical approach. The main body of this paper provides a narrative of the {DeepQA} processing pipeline to introduce the articles in this special issue and put them in context of the overall system. Finally, this paper summarizes our main results, describing how the system, as a holistic combination of many diverse algorithmic techniques, performed at champion levels, and it briefly discusses the team's future research plans.},
	pages = {1:1--1:15},
	number = {3},
	journaltitle = {{IBM} Journal of Research and Development},
	author = {Ferrucci, D. A.},
	date = {2012-05},
	note = {Conference Name: {IBM} Journal of Research and Development},
	keywords = {Algorithm design and analysis, Background, Computer architecture, Computers, Games, groundwork, History, {QA}, Semantics},
	file = {IEEE Xplore Abstract Record:/Users/lenert/Zotero/storage/9LBMHVTC/6177724.html:text/html},
}

@inproceedings{zhang_survey_2023,
	location = {Toronto, Canada},
	title = {A Survey for Efficient Open Domain Question Answering},
	url = {https://aclanthology.org/2023.acl-long.808},
	doi = {10.18653/v1/2023.acl-long.808},
	abstract = {Open domain question answering ({ODQA}) is a longstanding task aimed at answering factual questions from a large knowledge corpus without any explicit evidence in natural language processing ({NLP}). Recent works have predominantly focused on improving the answering accuracy and have achieved promising progress. However, higher accuracy often requires more memory consumption and inference latency, which might not necessarily be efficient enough for direct deployment in the real world. Thus, a trade-off between accuracy, memory consumption and processing speed is pursued. In this paper, we will survey recent advancements in the efficiency of {ODQA} models and conclude core techniques for achieving efficiency. Additionally, we will provide a quantitative analysis of memory cost, query speed, accuracy, and overall performance comparison. Our goal is to keep scholars informed of the latest advancements and open challenges in {ODQA} efficiency research and contribute to the further development of {ODQA} efficiency.},
	eventtitle = {{ACL} 2023},
	pages = {14447--14465},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Qin and Chen, Shangsi and Xu, Dongkuan and Cao, Qingqing and Chen, Xiaojun and Cohn, Trevor and Fang, Meng},
	urldate = {2023-08-27},
	date = {2023-07},
	keywords = {Background, Evaluation, Limits, Memory, Metrics, overview, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/4B43W6HL/Zhang et al. - 2023 - A Survey for Efficient Open Domain Question Answer.pdf:application/pdf},
}


@misc{dasigi_dataset_2021,
	title = {A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},
	url = {http://arxiv.org/abs/2105.03011},
	doi = {10.48550/arXiv.2105.03011},
	abstract = {Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present {QASPER}, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an {NLP} practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of {NLP} practitioners who also provide supporting evidence to answers. We find that existing models that do well on other {QA} tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking {QA}, which our dataset is designed to facilitate.},
	number = {{arXiv}:2105.03011},
	publisher = {{arXiv}},
	author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt},
	urldate = {2023-08-27},
	date = {2021-05-06},
	eprinttype = {arxiv},
	eprint = {2105.03011 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Dataset, Evaluation, groundwork, Indexing, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/WTHDV2NM/Dasigi et al. - 2021 - A Dataset of Information-Seeking Questions and Ans.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/BC2HI6RJ/2105.html:text/html},
}

@misc{rajpurkar_squad_2016,
	title = {{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
	url = {http://arxiv.org/abs/1606.05250},
	doi = {10.48550/arXiv.1606.05250},
	shorttitle = {{SQuAD}},
	abstract = {We present the Stanford Question Answering Dataset ({SQuAD}), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
	number = {{arXiv}:1606.05250},
	publisher = {{arXiv}},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	urldate = {2023-09-21},
	date = {2016-10-10},
	eprinttype = {arxiv},
	eprint = {1606.05250 [cs]},
	keywords = {Background, Computer Science - Computation and Language, dataset, groundwork, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/EIBICFD5/Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/HQLULCXF/1606.html:text/html},
}


@article{mishra_survey_2016,
	title = {A survey on question answering systems with classification},
	volume = {28},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157815000890},
	doi = {10.1016/j.jksuci.2014.10.007},
	abstract = {Question answering systems ({QASs}) generate answers of questions asked in natural languages. Early {QASs} were developed for restricted domains and have limited capabilities. Current {QASs} focus on types of questions generally asked by users, characteristics of data sources consulted, and forms of correct answers generated. Research in the area of {QASs} began in 1960s and since then, a large number of {QASs} have been developed. To identify the future scope of research in this area, the need of a comprehensive survey on {QASs} arises naturally. This paper surveys {QASs} and classifies them based on different criteria. We identify the current status of the research in the each category of {QASs}, and suggest future scope of the research.},
	pages = {345--361},
	number = {3},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	shortjournal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Mishra, Amit and Jain, Sanjay Kumar},
	urldate = {2023-09-21},
	date = {2016-07-01},
	keywords = {Background, basics, Information retrieval, Natural language processing, Natural language understanding, overview, {QA}, Question answering system, Search engine},
	file = {ScienceDirect Snapshot:/Users/lenert/Zotero/storage/ZGAGW9F2/S1319157815000890.html:text/html},
}


@inproceedings{roberts_how_2020,
	location = {Online},
	title = {How Much Knowledge Can You Pack Into the Parameters of a Language Model?},
	url = {https://aclanthology.org/2020.emnlp-main.437},
	doi = {10.18653/v1/2020.emnlp-main.437},
	abstract = {It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.},
	eventtitle = {{EMNLP} 2020},
	pages = {5418--5426},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Roberts, Adam and Raffel, Colin and Shazeer, Noam},
	urldate = {2023-09-21},
	date = {2020-11},
	keywords = {Background, Example Architectures, generative, {QA}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/B98FDWN7/Roberts et al. - 2020 - How Much Knowledge Can You Pack Into the Parameter.pdf:application/pdf},
}


@article{dimitrakis_survey_2020,
	title = {A survey on question answering systems over linked data and documents},
	volume = {55},
	issn = {1573-7675},
	url = {https://doi.org/10.1007/s10844-019-00584-7},
	doi = {10.1007/s10844-019-00584-7},
	abstract = {Question Answering ({QA}) systems aim at supplying precise answers to questions, posed by users in a natural language form. They are used in a wide range of application areas, from bio-medicine to tourism. Their underlying knowledge source can be structured data (e.g. {RDF} graphs and {SQL} databases), unstructured data in the form of plain text (e.g. textual excerpts from Wikipedia), or combinations of the above. In this paper we survey the recent work that has been done in the area of stateless {QA} systems with emphasis on methods that have been applied in {RDF} and Linked Data, documents, and mixtures of these. We identify the main challenges, we categorize the existing approaches according to various aspects, we review 21 recent systems, and 23 evaluation and training datasets that are most commonly used in the literature categorized according to the type of the domain, the underlying knowledge source, the provided tasks, and the associated evaluation metrics.},
	pages = {233--259},
	number = {2},
	journaltitle = {Journal of Intelligent Information Systems},
	shortjournal = {J Intell Inf Syst},
	author = {Dimitrakis, Eleftherios and Sgontzos, Konstantinos and Tzitzikas, Yannis},
	urldate = {2023-09-21},
	date = {2020-10-01},
	langid = {english},
	keywords = {Background, dialogue systems, evaluation collections, Example, {KBQA}, overview, {QA}, Question answering, {RDF} and linked data},
}

@article{harabagiu_open_domain_2003,
	title = {Open-domain textual question answering techniques},
	volume = {9},
	issn = {1469-8110, 1351-3249},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/opendomain-textual-question-answering-techniques/31811E06D60998460030D304F6869D7E},
	doi = {10.1017/S1351324903003176},
	pages = {231--267},
	number = {3},
	journaltitle = {Natural Language Engineering},
	author = {Harabagiu, Sanda M. and Maiorano, Steven J. and Pasca, Marius A.},
	urldate = {2023-09-21},
	date = {2003-09},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	keywords = {Background, Example Architectures, {QA}},
}


@article{nassiri_transformer_2023,
	title = {Transformer models used for text-based question answering systems},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-022-04052-8},
	doi = {10.1007/s10489-022-04052-8},
	abstract = {The question answering system is frequently applied in the area of natural language processing ({NLP}) because of the wide variety of applications. It consists of answering questions using natural language. The problem is, in general, solved by employing a dataset that consists of an input text, a query, and the text segment or span from the input text that provides the question’s answer. The ability to make human-level predictions from data has improved significantly thanks to deep learning models, particularly the Transformer architecture, which has been state-of-the-art in text-based models in recent years. This paper reviews studies related to the use of transformer models in the implementation of question-answering ({QA}) systems. The paper’s first focus is on the attention and transformer models. A brief description of the architectures is presented by classifying them into models based on encoders, decoders, and on both Encoder-Decoder. Following that, we examine the most recent research trends in textual {QA} datasets by highlighting the architecture of {QA} systems and categorizing them according to various criteria. We survey also a significant set of evaluation metrics that have been developed in order to evaluate the models’ performance. Finally, we highlight solutions built to simplify the implementation of Transformer models.},
	pages = {10602--10635},
	number = {9},
	journaltitle = {Applied Intelligence},
	shortjournal = {Appl Intell},
	author = {Nassiri, Khalid and Akhloufi, Moulay},
	urldate = {2023-09-21},
	date = {2023-05},
	langid = {english},
	keywords = {Background, basics, Example Architectures, overview, {QA}},
	file = {Nassiri und Akhloufi - 2023 - Transformer models used for text-based question an.pdf:/Users/lenert/Zotero/storage/CBW5CYTP/Nassiri und Akhloufi - 2023 - Transformer models used for text-based question an.pdf:application/pdf},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream {NLP} tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation ({RAG}) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce {RAG} models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two {RAG} formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive {NLP} tasks and set the state-of-the-art on three open domain {QA} tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that {RAG} models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	number = {{arXiv}:2005.11401},
	publisher = {{arXiv}},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	urldate = {2023-09-21},
	date = {2021-04-12},
	eprinttype = {arxiv},
	eprint = {2005.11401 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, Example Architectures, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/W8IWGTEN/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Inten.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/AWEXY5RF/2005.html:text/html},
}

@misc{guu_realm_2020,
	title = {{REALM}: Retrieval-Augmented Language Model Pre-Training},
	url = {http://arxiv.org/abs/2002.08909},
	doi = {10.48550/arXiv.2002.08909},
	shorttitle = {{REALM}},
	abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for {NLP} tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training ({REALM}) by fine-tuning on the challenging task of Open-domain Question Answering (Open-{QA}). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-{QA} benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
	number = {{arXiv}:2002.08909},
	publisher = {{arXiv}},
	author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
	urldate = {2023-09-21},
	date = {2020-02-10},
	eprinttype = {arxiv},
	eprint = {2002.08909 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Machine Learning, Example Architectures, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/2HCGEMTA/Guu et al. - 2020 - REALM Retrieval-Augmented Language Model Pre-Trai.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ZBUCDVGK/2002.html:text/html},
}

@inproceedings{nishida_retrieve-and-read_2018,
	title = {Retrieve-and-Read: Multi-task Learning of Information Retrieval and Reading Comprehension},
	url = {http://arxiv.org/abs/1808.10628},
	doi = {10.1145/3269206.3271702},
	shorttitle = {Retrieve-and-Read},
	abstract = {This study considers the task of machine reading at scale ({MRS}) wherein, given a question, a system first performs the information retrieval ({IR}) task of finding relevant passages in a knowledge source and then carries out the reading comprehension ({RC}) task of extracting an answer span from the passages. Previous {MRS} studies, in which the {IR} component was trained without considering answer spans, struggled to accurately find a small number of relevant passages from a large set of passages. In this paper, we propose a simple and effective approach that incorporates the {IR} and {RC} tasks by using supervised multi-task learning in order that the {IR} component can be trained by considering answer spans. Experimental results on the standard benchmark, answering {SQuAD} questions using the full Wikipedia as the knowledge source, showed that our model achieved state-of-the-art performance. Moreover, we thoroughly evaluated the individual contributions of our model components with our new Japanese dataset and {SQuAD}. The results showed significant improvements in the {IR} task and provided a new perspective on {IR} for {RC}: it is effective to teach which part of the passage answers the question rather than to give only a relevance score to the whole passage.},
	pages = {647--656},
	booktitle = {Proceedings of the 27th {ACM} International Conference on Information and Knowledge Management},
	author = {Nishida, Kyosuke and Saito, Itsumi and Otsuka, Atsushi and Asano, Hisako and Tomita, Junji},
	urldate = {2023-09-21},
	date = {2018-10-17},
	eprinttype = {arxiv},
	eprint = {1808.10628 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, Example Architectures, {QA}},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/YZ4J87GJ/Nishida et al. - 2018 - Retrieve-and-Read Multi-task Learning of Informat.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/TRPAWYK9/1808.html:text/html},
}
