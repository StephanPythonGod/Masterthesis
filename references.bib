@article{zhu_retrieving_2021,
  author       = {Fengbin Zhu and
                  Wenqiang Lei and
                  Chao Wang and
                  Jianming Zheng and
                  Soujanya Poria and
                  Tat{-}Seng Chua},
  title        = {Retrieving and Reading: {A} Comprehensive Survey on Open-domain Question
                  Answering},
  journal      = {CoRR},
  volume       = {abs/2101.00774},
  year         = {2021},
  url          = {https://arxiv.org/abs/2101.00774},
  eprinttype    = {arXiv},
  eprint       = {2101.00774},
  timestamp    = {Thu, 31 Aug 2023 11:16:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2101-00774.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@unpublished{zamani_conversational_2023,
    title = {Conversational {Information} {Seeking}},
    url = {http://arxiv.org/abs/2201.08808},
    doi = {10.48550/arXiv.2201.08808},
    abstract = {Conversational information seeking (CIS) is concerned with a sequence of interactions between one or more users and an information system. Interactions in CIS are primarily based on natural language dialogue, while they may include other types of interactions, such as click, touch, and body gestures. This monograph provides a thorough overview of CIS definitions, applications, interactions, interfaces, design, implementation, and evaluation. This monograph views CIS applications as including conversational search, conversational question answering, and conversational recommendation. Our aim is to provide an overview of past research related to CIS, introduce the current state-of-the-art in CIS, highlight the challenges still being faced in the community. and suggest future directions.},
    urldate = {2023-08-27},
    publisher = {arXiv},
    author = {Zamani, Hamed and Trippas, Johanne R. and Dalton, Jeff and Radlinski, Filip},
    month = jan,
    year = {2023},
    note = {arXiv. \url{http://arxiv.org/abs/2201.08808}},
    keywords = {Background, CIS, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval, overview, QA, SvD},
    annote = {Comment: Draft Version 1.2},
    file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7WFGQWLT/Zamani et al. - 2023 - Conversational Information Seeking.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/MBB85CRQ/2201.html:text/html},
}

@book{gao_neural_2022,
  author    = {Jianfeng Gao and
               Chenyan Xiong and
               Paul Bennett and
               Nick Craswell},
  title     = {Neural Approaches to Conversational Information Retrieval},
  series    = {The Information Retrieval Series},
  volume    = {44},
  publisher = {Springer},
  year      = {2023},
  url       = {https://doi.org/10.1007/978-3-031-23080-6},
  doi       = {10.1007/978-3-031-23080-6},
  isbn      = {978-3-031-23079-0},
  timestamp = {Fri, 17 Mar 2023 16:54:53 +0100},
  biburl    = {https://dblp.org/rec/series/irs/GaoXBC23.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu_dense_2021,
    title = "Dense Hierarchical Retrieval for Open-domain Question Answering",
    author = "Liu, Ye  and
      Hashimoto, Kazuma  and
      Zhou, Yingbo  and
      Yavuz, Semih  and
      Xiong, Caiming  and
      Yu, Philip",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.19",
    doi = "10.18653/v1/2021.findings-emnlp.19",
    pages = "188--200",
    abstract = "Dense neural text retrieval has achieved promising results on open-domain Question Answering (QA), where latent representations of questions and passages are exploited for maximum inner product search in the retrieval process. However, current dense retrievers require splitting documents into short passages that usually contain local, partial and sometimes biased context, and highly depend on the splitting process. As a consequence, it may yield inaccurate and misleading hidden representations, thus deteriorating the final retrieval result. In this work, we propose Dense Hierarchical Retrieval (DHR), a hierarchical framework which can generate accurate dense representations of passages by utilizing both macroscopic semantics in the document and microscopic semantics specific to each passage. Specifically, a document-level retriever first identifies relevant documents, among which relevant passages are then retrieved by a passage-level retriever. The ranking of the retrieved passages will be further calibrated by examining the document-level relevance. In addition, hierarchical title structure and two negative sampling strategies (i.e., In-Doc and In-Sec negatives) are investigated. We apply DHR to large-scale open-domain QA datasets. DHR significantly outperforms the original dense passage retriever, and helps an end-to-end QA system outperform the strong baselines on multiple open-domain QA benchmarks.",
}

@inproceedings{zhang_survey_2023,
	address = {Toronto, Canada},
	title = {A {Survey} for {Efficient} {Open} {Domain} {Question} {Answering}},
	url = {https://aclanthology.org/2023.acl-long.808},
	doi = {10.18653/v1/2023.acl-long.808},
	abstract = {Open domain question answering (ODQA) is a longstanding task aimed at answering factual questions from a large knowledge corpus without any explicit evidence in natural language processing (NLP). Recent works have predominantly focused on improving the answering accuracy and have achieved promising progress. However, higher accuracy often requires more memory consumption and inference latency, which might not necessarily be efficient enough for direct deployment in the real world. Thus, a trade-off between accuracy, memory consumption and processing speed is pursued. In this paper, we will survey recent advancements in the efficiency of ODQA models and conclude core techniques for achieving efficiency. Additionally, we will provide a quantitative analysis of memory cost, query speed, accuracy, and overall performance comparison. Our goal is to keep scholars informed of the latest advancements and open challenges in ODQA efficiency research and contribute to the further development of ODQA efficiency.},
	urldate = {2023-08-27},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Qin and Chen, Shangsi and Xu, Dongkuan and Cao, Qingqing and Chen, Xiaojun and Cohn, Trevor and Fang, Meng},
	month = jul,
	year = {2023},
	keywords = {Background, Evaluation, Limits, Memory, Metrics, overview, QA},
	pages = {14447--14465},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/4B43W6HL/Zhang et al. - 2023 - A Survey for Efficient Open Domain Question Answer.pdf:application/pdf},
}

@inproceedings{karpukhin_dense_2020,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}

@unpublished{mcdonald_detect_2022,
	title = {Detect, {Retrieve}, {Comprehend}: {A} {Flexible} {Framework} for {Zero}-{Shot} {Document}-{Level} {Question} {Answering}},
	shorttitle = {Detect, {Retrieve}, {Comprehend}},
	url = {http://arxiv.org/abs/2210.01959},
	doi = {10.48550/arXiv.2210.01959},
	abstract = {Researchers produce thousands of scholarly documents containing valuable technical knowledge. The community faces the laborious task of reading these documents to identify, extract, and synthesize information. To automate information gathering, document-level question answering (QA) offers a flexible framework where human-posed questions can be adapted to extract diverse knowledge. Finetuning QA systems requires access to labeled data (tuples of context, question and answer). However, data curation for document QA is uniquely challenging because the context (i.e. answer evidence passage) needs to be retrieved from potentially long, ill-formatted documents. Existing QA datasets sidestep this challenge by providing short, well-defined contexts that are unrealistic in real-world applications. We present a three-stage document QA approach: (1) text extraction from PDF; (2) evidence retrieval from extracted texts to form well-posed contexts; (3) QA to extract knowledge from contexts to return high-quality answers -- extractive, abstractive, or Boolean. Using QASPER for evaluation, our detect-retrieve-comprehend (DRC) system achieves a +7.19 improvement in Answer-F1 over existing baselines while delivering superior context selection. Our results demonstrate that DRC holds tremendous promise as a flexible framework for practical scientific document QA.},
	urldate = {2023-08-27},
	publisher = {arXiv},
	author = {McDonald, Tavish and Tsan, Brian and Saini, Amar and Ordonez, Juanita and Gutierrez, Luis and Nguyen, Phan and Mason, Blake and Ng, Brenda},
	month = dec,
	year = {2022},
	note = {arXiv. \url{http://arxiv.org/abs/2210.01959}},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Indexing, pdfs, QA, related work, Retrieval},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/2TUGS7SU/McDonald et al. - 2022 - Detect, Retrieve, Comprehend A Flexible Framework.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/6Z9ZSPFB/2210.html:text/html},
}

@inproceedings{dasigi_dataset_2021,
    title = "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
    author = "Dasigi, Pradeep  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Cohan, Arman  and
      Smith, Noah A.  and
      Gardner, Matt",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.365",
    doi = "10.18653/v1/2021.naacl-main.365",
    pages = "4599--4610",
    abstract = "Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.",
}

@article{pandya_question_2021,
	author = {Pandya, Hariom and Bhatt, Brijesh},
	year = {2021},
	month = {05},
	pages = {152-168},
	title = {Question Answering Survey: Directions, Challenges, Datasets, Evaluation Matrices},
	volume = {15},
	journal = {Xi'an Dianzi Keji Daxue Xuebao/Journal of Xidian University}
}

@inproceedings{voorhees_trec-8_1999,
	title = {The {TREC}-8 {Question} {Answering} {Track} {Report}},
	url = {https://www.semanticscholar.org/paper/The-TREC-8-Question-Answering-Track-Report-Voorhees/646d4888871aca2a25111eb2520e4c47e253b014},
	abstract = {The TREC-8 Question Answering track was the  first large-scale evaluation of domain-independent question answering systems. This paper summarizes the results of the track by giving a brief overview of the different approaches taken to solve the problem. The most accurate systems found a correct response for more than 2/3 of the questions. Relatively simple bag-of-words approaches were adequate for  finding answers when responses could be as long as a paragraph (250 bytes), but more sophisticated processing was necessary for more direct responses (50 bytes). 
 
The TREC-8 Question Answering track was an initial e ort to bring the bene ts of large-scale evaluation to bear on a question answering (QA) task. The goal in the QA task is to retrieve small snippets of text that contain the actual answer to a question rather than the document lists traditionally returned by text retrieval systems. The assumption is that users would usually prefer to be given the answer rather than  and the answer themselves in a document. 
 
This paper summarizes the retrieval results of the track; a companion paper ({\textbackslash}The TREC-8 Question Answering Track Evaluation") gives details about how the evaluation was implemented. By necessity, a track report can give only an overview of the different approaches used in the track. Readers are urged to consult the participants' papers elsewhere in the Proceedings for details regarding a particular approach.},
	urldate = {2023-09-19},
	author = {Voorhees, E.},
	year = {1999},
	keywords = {Background, QA, historical, groundwork, basics},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/RSDRLSMP/Voorhees - 1999 - The TREC-8 Question Answering Track Report.pdf:application/pdf},
}

@article{hao_recent_2022,
	title = {Recent progress in leveraging deep learning methods for question answering},
	volume = {34},
	issn = {0941-0643, 1433-3058},
	url = {https://link.springer.com/10.1007/s00521-021-06748-3},
	doi = {10.1007/s00521-021-06748-3},
	abstract = {Question answering, serving as one of important tasks in natural language processing, enables machines to understand questions in natural language and answer the questions concisely. From web search to expert systems, question answering systems are widely applied to various domains in assisting information seeking. Deep learning methods have boosted various tasks of question answering and have demonstrated dramatic effects in performance improvement for essential steps of question answering. Thus, leveraging deep learning methods for question answering has drawn much attention from both academia and industry in recent years. This paper provides a systematic review of the recent development of deep learning methods for question answering. The survey covers the scope including methods, datasets, and applications. The methods are discussed in terms of network structure characteristics, methodology innovations, and their effectiveness. The survey is expected to be a contribution to the summarization of recent research progress and future directions of deep learning methods for question answering.},
	language = {en},
	number = {4},
	urldate = {2023-09-19},
	journal = {Neural Computing and Applications},
	author = {Hao, Tianyong and Li, Xinxin and He, Yulan and Wang, Fu Lee and Qu, Yingying},
	month = feb,
	year = {2022},
	keywords = {Background, basics, overview, QA},
	pages = {2765--2783},
	annote = {[TLDR] This paper provides a systematic review of the recent development of deep learning methods for question answering in terms of network structure characteristics, methodology innovations, and their effectiveness.},
	file = {Hao et al. - 2022 - Recent progress in leveraging deep learning method.pdf:/Users/lenert/Zotero/storage/247GDDR2/Hao et al. - 2022 - Recent progress in leveraging deep learning method.pdf:application/pdf},
}

@article{etezadi_state_2023,
	title = {The state of the art in open domain complex question answering: a survey},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	shorttitle = {The state of the art in open domain complex question answering},
	url = {https://link.springer.com/10.1007/s10489-022-03732-9},
	doi = {10.1007/s10489-022-03732-9},
	abstract = {Research on question answering (QA) systems has a long tradition. QA systems, as widely used systems in various applications, seek to find the answers to the given questions through the available resources. These systems are expected to be capable of answering various types of questions, including simple questions whose answers can be found in a single passage or sentence and complex questions which need more complicated reasoning to find the answer or their answer should be found by traversing several relations. Nowadays, answering complex questions from texts or structured data is a challenge in QA systems. In this paper, we have a comparative study on QA approaches and systems for answering complex questions. For this purpose, firstly, this paper discusses what a complex question is and surveys different types of constraints that may appear in complex questions. Furthermore, it addresses the challenges of these types of questions, the methods proposed to deal with them, and benchmark datasets used to evaluate their strengths and weaknesses.},
	language = {en},
	number = {4},
	urldate = {2023-09-19},
	journal = {Applied Intelligence},
	author = {Etezadi, Romina and Shamsfard, Mehrnoush},
	month = feb,
	year = {2023},
	keywords = {Background, basics, overview, QA},
	pages = {4124--4144},
	annote = {[TLDR] What a complex question is, the challenges of these types of questions, the methods proposed to deal with them, and benchmark datasets used to evaluate their strengths and weaknesses are addressed.},
	file = {Etezadi und Shamsfard - 2023 - The state of the art in open domain complex questi.pdf:/Users/lenert/Zotero/storage/TTQ32SLA/Etezadi und Shamsfard - 2023 - The state of the art in open domain complex questi.pdf:application/pdf},
}

@book{jurafsky_speech_2023,
	address = {Palo Alto},
	edition = {3},
	title = {Speech and {Language} {Processing}},
	abstract = {An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
	author = {Jurafsky, Dan and Martin, James H.},
	month = jan,
	year = {2023},
	file = {Speech and Language Processing.pdf:/Users/lenert/Zotero/storage/3K7TMW7N/14.pdf:application/pdf},
}

@unpublished{farea_evaluation_2022,
  title={Evaluation of Question Answering Systems: Complexity of judging a natural language},
  author={Amer Ali Sallam Farea and Zhen Yang and Kien Duong and Nadeesha Perera and Frank Emmert-Streib},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.12617},
  note={arXiv. \url{http://arxiv.org/abs/2209.12617}},
  url={https://api.semanticscholar.org/CorpusID:252531182}
}

@inproceedings{green_baseball_1961,
	address = {New York, NY, USA},
	series = {{IRE}-{AIEE}-{ACM} '61 ({Western})},
	title = {Baseball: an automatic question-answerer},
	isbn = {978-1-4503-7872-7},
	shorttitle = {Baseball},
	url = {https://dl.acm.org/doi/10.1145/1460690.1460714},
	doi = {10.1145/1460690.1460714},
	abstract = {{\textless}u{\textgreater}Baseball{\textless}/u{\textgreater} is a computer program that answers questions phrased in ordinary English about stored data. The program reads the question from punched cards. After the words and idioms are looked up in a dictionary, the phrase structure and other syntactic facts are determined for a content analysis, which lists attribute-value pairs specifying the information given and the information requested. The requested information is then extracted from the data matching the specifications, and any necessary processing is done. Finally, the answer is printed. The program's present context is baseball games; it answers such questions as "Where did each team play on July 7?"},
	urldate = {2023-09-19},
	booktitle = {Papers presented at the {May} 9-11, 1961, western joint {IRE}-{AIEE}-{ACM} computer conference},
	publisher = {Association for Computing Machinery},
	author = {Green, Bert F. and Wolf, Alice K. and Chomsky, Carol and Laughery, Kenneth},
	month = may,
	year = {1961},
	keywords = {Background, QA, groundwork},
	pages = {219--224},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/A9B5I6B8/Green et al. - 1961 - Baseball an automatic question-answerer.pdf:application/pdf},
}

@article{ferrucci_introduction_2012,
	title = {Introduction to “{This} is {Watson}”},
	volume = {56},
	issn = {0018-8646},
	doi = {10.1147/JRD.2012.2184356},
	abstract = {In 2007, IBM Research took on the grand challenge of building a computer system that could compete with champions at the game of Jeopardy!™. In 2011, the open-domain question-answering (QA) system, dubbed Watson, beat the two highest ranked players in a nationally televised two-game Jeopardy! match. This paper provides a brief history of the events and ideas that positioned our team to take on the Jeopardy! challenge, build Watson, IBM Watson™, and ultimately triumph. It describes both the nature of the QA challenge represented by Jeopardy! and our overarching technical approach. The main body of this paper provides a narrative of the DeepQA processing pipeline to introduce the articles in this special issue and put them in context of the overall system. Finally, this paper summarizes our main results, describing how the system, as a holistic combination of many diverse algorithmic techniques, performed at champion levels, and it briefly discusses the team's future research plans.},
	number = {3.4},
	journal = {IBM Journal of Research and Development},
	author = {Ferrucci, D. A.},
	month = may,
	year = {2012},
	note = {Conference Name: IBM Journal of Research and Development},
	keywords = {Background, QA, groundwork, Algorithm design and analysis, Computer architecture, Computers, Games, History, Semantics},
	pages = {1:1--1:15},
	file = {IEEE Xplore Abstract Record:/Users/lenert/Zotero/storage/9LBMHVTC/6177724.html:text/html},
}

@inproceedings{rajpurkar_squad_2016,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@article{mishra_survey_2016,
	title = {A survey on question answering systems with classification},
	volume = {28},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157815000890},
	doi = {10.1016/j.jksuci.2014.10.007},
	abstract = {Question answering systems (QASs) generate answers of questions asked in natural languages. Early QASs were developed for restricted domains and have limited capabilities. Current QASs focus on types of questions generally asked by users, characteristics of data sources consulted, and forms of correct answers generated. Research in the area of QASs began in 1960s and since then, a large number of QASs have been developed. To identify the future scope of research in this area, the need of a comprehensive survey on QASs arises naturally. This paper surveys QASs and classifies them based on different criteria. We identify the current status of the research in the each category of QASs, and suggest future scope of the research.},
	number = {3},
	urldate = {2023-09-21},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Mishra, Amit and Jain, Sanjay Kumar},
	month = jul,
	year = {2016},
	keywords = {Background, QA, overview, basics, Information retrieval, Natural language processing, Natural language understanding, Question answering system, Search engine},
	pages = {345--361},
	file = {ScienceDirect Snapshot:/Users/lenert/Zotero/storage/ZGAGW9F2/S1319157815000890.html:text/html},
}

@article{dimitrakis_survey_2020,
	title = {A survey on question answering systems over linked data and documents},
	volume = {55},
	issn = {1573-7675},
	url = {https://doi.org/10.1007/s10844-019-00584-7},
	doi = {10.1007/s10844-019-00584-7},
	abstract = {Question Answering (QA) systems aim at supplying precise answers to questions, posed by users in a natural language form. They are used in a wide range of application areas, from bio-medicine to tourism. Their underlying knowledge source can be structured data (e.g. RDF graphs and SQL databases), unstructured data in the form of plain text (e.g. textual excerpts from Wikipedia), or combinations of the above. In this paper we survey the recent work that has been done in the area of stateless QA systems with emphasis on methods that have been applied in RDF and Linked Data, documents, and mixtures of these. We identify the main challenges, we categorize the existing approaches according to various aspects, we review 21 recent systems, and 23 evaluation and training datasets that are most commonly used in the literature categorized according to the type of the domain, the underlying knowledge source, the provided tasks, and the associated evaluation metrics.},
	language = {en},
	number = {2},
	urldate = {2023-09-21},
	journal = {Journal of Intelligent Information Systems},
	author = {Dimitrakis, Eleftherios and Sgontzos, Konstantinos and Tzitzikas, Yannis},
	month = oct,
	year = {2020},
	keywords = {Background, QA, overview, Question answering, dialogue systems, evaluation collections, RDF and linked data, Example, KBQA},
	pages = {233--259},
}

@inproceedings{roberts_how_2020,
	address = {Online},
	title = {How {Much} {Knowledge} {Can} {You} {Pack} {Into} the {Parameters} of a {Language} {Model}?},
	url = {https://aclanthology.org/2020.emnlp-main.437},
	doi = {10.18653/v1/2020.emnlp-main.437},
	abstract = {It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.},
	urldate = {2023-09-21},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Roberts, Adam and Raffel, Colin and Shazeer, Noam},
	month = nov,
	year = {2020},
	keywords = {Background, Example Architectures, generative, QA},
	pages = {5418--5426},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/B98FDWN7/Roberts et al. - 2020 - How Much Knowledge Can You Pack Into the Parameter.pdf:application/pdf},
}

@article{harabagiu_open-domain_2003,
	title = {Open-domain textual question answering techniques},
	volume = {9},
	issn = {1469-8110, 1351-3249},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/opendomain-textual-question-answering-techniques/31811E06D60998460030D304F6869D7E},
	doi = {10.1017/S1351324903003176},
	abstract = {Textual question answering is a technique of extracting a sentence or text snippet from a document or document collection that responds directly to a query. Open-domain textual question answering presupposes that questions are natural and unrestricted with respect to topic. The question answering (Q/A) techniques, as embodied in today's systems, can be roughly divided into two types: (1) techniques for Information Seeking (IS), which localize the answer in vast document collections; and (2) techniques for Reading Comprehension (RC) that answer a series of questions related to a given document. Although these two types of techniques and systems are different, it is desirable to combine them for enabling more advanced forms of Q/A. This paper discusses an approach that successfully enhanced an existing IS system with RC capabilities. This enhancement is important because advanced Q/A, as exemplified by the ARDA AQUAINT program, is moving towards Q/A systems that incorporate semantic and pragmatic knowledge enabling dialogue-based Q/A. Because today's RC systems involve a short series of questions in context, they represent a rudimentary form of interactive Q/A which constitutes a possible foundation for more advanced forms of dialogue-based Q/A.},
	language = {en},
	number = {3},
	urldate = {2023-09-21},
	journal = {Natural Language Engineering},
	author = {Harabagiu, Sanda M. and Maiorano, Steven J. and Paşca, Marius A.},
	month = sep,
	year = {2003},
	note = {Publisher: Cambridge University Press},
	keywords = {Background, Example Architectures, QA},
	pages = {231--267},
}

@article{nassiri_transformer_2023,
	title = {Transformer models used for text-based question answering systems},
	volume = {53},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-022-04052-8},
	doi = {10.1007/s10489-022-04052-8},
	abstract = {The question answering system is frequently applied in the area of natural language processing (NLP) because of the wide variety of applications. It consists of answering questions using natural language. The problem is, in general, solved by employing a dataset that consists of an input text, a query, and the text segment or span from the input text that provides the question’s answer. The ability to make human-level predictions from data has improved significantly thanks to deep learning models, particularly the Transformer architecture, which has been state-of-the-art in text-based models in recent years. This paper reviews studies related to the use of transformer models in the implementation of question-answering (QA) systems. The paper’s first focus is on the attention and transformer models. A brief description of the architectures is presented by classifying them into models based on encoders, decoders, and on both Encoder-Decoder. Following that, we examine the most recent research trends in textual QA datasets by highlighting the architecture of QA systems and categorizing them according to various criteria. We survey also a significant set of evaluation metrics that have been developed in order to evaluate the models’ performance. Finally, we highlight solutions built to simplify the implementation of Transformer models.},
	language = {en},
	number = {9},
	urldate = {2023-09-21},
	journal = {Applied Intelligence},
	author = {Nassiri, Khalid and Akhloufi, Moulay},
	month = may,
	year = {2023},
	keywords = {Background, basics, Example Architectures, overview, QA},
	pages = {10602--10635},
	annote = {[TLDR] This paper reviews studies related to the use of transformer models in the implementation of question-answering (QA) systems and examines the most recent research trends in textual QA datasets by highlighting the architecture of QA systems and categorizing them according to various criteria.},
	file = {Nassiri und Akhloufi - 2023 - Transformer models used for text-based question an.pdf:/Users/lenert/Zotero/storage/CBW5CYTP/Nassiri und Akhloufi - 2023 - Transformer models used for text-based question an.pdf:application/pdf},
}

@inproceedings{lewis_retrieval-augmented_2021,
author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
title = {Retrieval-augmented generation for knowledge-intensive NLP tasks},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {793},
numpages = {16},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@inproceedings{guu_realm_2020,
author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
title = {REALM: retrieval-augmented language model pre-training},
year = {2020},
publisher = {JMLR.org},
abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring everlarger networks to cover more facts.To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.We demonstrate the effectiveness of Retrieval-Augmented Language Model pretraining (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {368},
numpages = {10},
series = {ICML'20}
}

@inproceedings{nishida_retrieve-and-read_2018,
	title = {Retrieve-and-{Read}: {Multi}-task {Learning} of {Information} {Retrieval} and {Reading} {Comprehension}},
	shorttitle = {Retrieve-and-{Read}},
	url = {http://arxiv.org/abs/1808.10628},
	doi = {10.1145/3269206.3271702},
	abstract = {This study considers the task of machine reading at scale (MRS) wherein, given a question, a system first performs the information retrieval (IR) task of finding relevant passages in a knowledge source and then carries out the reading comprehension (RC) task of extracting an answer span from the passages. Previous MRS studies, in which the IR component was trained without considering answer spans, struggled to accurately find a small number of relevant passages from a large set of passages. In this paper, we propose a simple and effective approach that incorporates the IR and RC tasks by using supervised multi-task learning in order that the IR component can be trained by considering answer spans. Experimental results on the standard benchmark, answering SQuAD questions using the full Wikipedia as the knowledge source, showed that our model achieved state-of-the-art performance. Moreover, we thoroughly evaluated the individual contributions of our model components with our new Japanese dataset and SQuAD. The results showed significant improvements in the IR task and provided a new perspective on IR for RC: it is effective to teach which part of the passage answers the question rather than to give only a relevance score to the whole passage.},
	urldate = {2023-09-21},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	author = {Nishida, Kyosuke and Saito, Itsumi and Otsuka, Atsushi and Asano, Hisako and Tomita, Junji},
	month = oct,
	year = {2018},
	note = {arXiv:1808.10628 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, Example Architectures, QA},
	pages = {647--656},
	annote = {Comment: 10 pages, 6 figure. Accepted as a full paper at CIKM 2018},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/YZ4J87GJ/Nishida et al. - 2018 - Retrieve-and-Read Multi-task Learning of Informat.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/TRPAWYK9/1808.html:text/html},
}

@article{wang_modern_2022,
	title = {Modern {Question} {Answering} {Datasets} and {Benchmarks}: {A} {Survey}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Modern {Question} {Answering} {Datasets} and {Benchmarks}},
	url = {https://arxiv.org/abs/2206.15030},
	doi = {10.48550/ARXIV.2206.15030},
	abstract = {Question Answering (QA) is one of the most important natural language processing (NLP) tasks. It aims using NLP technologies to generate a corresponding answer to a given question based on the massive unstructured corpus. With the development of deep learning, more and more challenging QA datasets are being proposed, and lots of new methods for solving them are also emerging. In this paper, we investigate influential QA datasets that have been released in the era of deep learning. Specifically, we begin with introducing two of the most common QA tasks - textual question answer and visual question answering - separately, covering the most representative datasets, and then give some current challenges of QA research.},
	urldate = {2023-09-22},
	author = {Wang, Zhen},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Background, Computation and Language (cs.CL), Dataset, FOS: Computer and information sciences, overview, QA},
	annote = {[TLDR] Two of the most common QA tasks - textual question answer and visual question answering - are introduced separately, covering the most representative datasets, and some current challenges of QA research are given.},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/8SLQGJSR/Wang - 2022 - Modern Question Answering Datasets and Benchmarks.pdf:application/pdf},
}

@incollection{tito_document_2021,
	title = {Document {Collection} {Visual} {Question} {Answering}},
	volume = {12822},
	url = {http://arxiv.org/abs/2104.14336},
	abstract = {Current tasks and methods in Document Understanding aims to process documents as single elements. However, documents are usually organized in collections (historical records, purchase invoices), that provide context useful for their interpretation. To address this problem, we introduce Document Collection Visual Question Answering (DocCVQA) a new dataset and related task, where questions are posed over a whole collection of document images and the goal is not only to provide the answer to the given question, but also to retrieve the set of documents that contain the information needed to infer the answer. Along with the dataset we propose a new evaluation metric and baselines which provide further insights to the new dataset and task.},
	urldate = {2023-09-22},
	author = {Tito, Rubèn and Karatzas, Dimosthenis and Valveny, Ernest},
	year = {2021},
	doi = {10.1007/978-3-030-86331-9_50},
	note = {arXiv:2104.14336 [cs]},
	keywords = {Background, Computer Science - Information Retrieval, Indexing, QA, VQA},
	pages = {778--792},
	file = {2104.14336.pdf:/Users/lenert/Zotero/storage/UKBJDGUY/2104.14336.pdf:application/pdf;arXiv Fulltext PDF:/Users/lenert/Zotero/storage/AC44N9FM/Tito et al. - 2021 - Document Collection Visual Question Answering.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/2HDBBTH3/2104.html:text/html},
}

@inproceedings{wang_multi-passage_2019,
    title = "Multi-passage {BERT}: A Globally Normalized {BERT} Model for Open-domain Question Answering",
    author = "Wang, Zhiguo  and
      Ng, Patrick  and
      Ma, Xiaofei  and
      Nallapati, Ramesh  and
      Xiang, Bing",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1599",
    doi = "10.18653/v1/D19-1599",
    pages = "5878--5882",
    abstract = "BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4{\%}. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2{\%}. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4{\%} EM and 21.5{\%} F1 over all non-BERT models, and 5.8{\%} EM and 6.5{\%} F1 over BERT-based models.",
}

@misc{noauthor_langchain-ailangchain_nodate,
    author = {{Langchain}},
    title = {Building applications with LLMs through composability. \url{https://github.com/langchain-ai/langchain}},
    year = {2023},
    note = {Accessed: 2023-09-22},
}

@misc{noauthor_chatgpt_2023,
    title = {ChatGPT Retrieval Plugin. \url{https://github.com/openai/chatgpt-retrieval-plugin}},
    author = {{OpenAI}},
    year = {2023},
    note = {Accessed: 2023-09-22},
    keywords = {Background, chatgpt, chatgpt-plugins, Indexing, LLM, Open-Source, QA},
}

@unpublished{mathew_document_2021,
	title = {Document {Visual} {Question} {Answering} {Challenge} 2020},
	url = {http://arxiv.org/abs/2008.08899},
	doi = {10.48550/arXiv.2008.08899},
	abstract = {This paper presents results of Document Visual Question Answering Challenge organized as part of "Text and Documents in the Deep Learning Era" workshop, in CVPR 2020. The challenge introduces a new problem - Visual Question Answering on document images. The challenge comprised two tasks. The first task concerns with asking questions on a single document image. On the other hand, the second task is set as a retrieval task where the question is posed over a collection of images. For the task 1 a new dataset is introduced comprising 50,000 questions-answer(s) pairs defined over 12,767 document images. For task 2 another dataset has been created comprising 20 questions over 14,362 document images which share the same document template.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Mathew, Minesh and Tito, Ruben and Karatzas, Dimosthenis and Manmatha, R. and Jawahar, C. V.},
	month = jul,
	year = {2021},
	note = {arXiv. \url{http://arxiv.org/abs/2008.08899}},
	keywords = {Background, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Indexing, QA, VQA},
	annote = {Comment: to be published as a short paper in DAS 2020},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/6A5UEC82/Mathew et al. - 2021 - Document Visual Question Answering Challenge 2020.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/MYGTHFBZ/2008.html:text/html},
}

@inproceedings{li_dit_2022,
author = {Li, Junlong and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
title = {DiT: Self-supervised Pre-training for Document Image Transformer},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547911},
doi = {10.1145/3503161.3547911},
abstract = {Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose DiT, a self-supervised pre-trained Document Image Transformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human-labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 - 92.69), document layout analysis (91.0 - 94.9), table detection (94.23 - 96.55) and text detection for OCR (93.07 - 94.29). The code and pre-trained models are publicly available at https://aka.ms/msdit.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3530–3539},
numpages = {10},
keywords = {OCR, document image classification, document image transformer, document layout analysis, self-supervised pre-training, table detection, text detection},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@incollection{meuschke_benchmark_2023,
	title = {A {Benchmark} of {PDF} {Information} {Extraction} {Tools} using a {Multi}-{Task} and {Multi}-{Domain} {Evaluation} {Framework} for {Academic} {Documents}},
	volume = {13972},
	url = {http://arxiv.org/abs/2303.09957},
	abstract = {Extracting information from academic PDF documents is crucial for numerous indexing, retrieval, and analysis use cases. Choosing the best tool to extract specific content elements is difficult because many, technically diverse tools are available, but recent performance benchmarks are rare. Moreover, such benchmarks typically cover only a few content elements like header metadata or bibliographic references and use smaller datasets from specific academic disciplines. We provide a large and diverse evaluation framework that supports more extraction tasks than most related datasets. Our framework builds upon DocBank, a multi-domain dataset of 1.5M annotated content elements extracted from 500K pages of research papers on arXiv. Using the new framework, we benchmark ten freely available tools in extracting document metadata, bibliographic references, tables, and other content elements from academic PDF documents. GROBID achieves the best metadata and reference extraction results, followed by CERMINE and Science Parse. For table extraction, Adobe Extract outperforms other tools, even though the performance is much lower than for other content elements. All tools struggle to extract lists, footers, and equations. We conclude that more research on improving and combining tools is necessary to achieve satisfactory extraction quality for most content elements. Evaluation datasets and frameworks like the one we present support this line of research. We make our data and code publicly available to contribute toward this goal.},
	urldate = {2023-09-22},
	author = {Meuschke, Norman and Jagdale, Apurva and Spinde, Timo and Mitrović, Jelena and Gipp, Bela},
	year = {2023},
	doi = {10.1007/978-3-031-28032-0_31},
	note = {arXiv:2303.09957 [cs]},
	keywords = {Background, Computer Science - Information Retrieval, Indexing, QA},
	pages = {383--405},
	annote = {Comment: iConference 2023},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/P8TZJEYY/Meuschke et al. - 2023 - A Benchmark of PDF Information Extraction Tools us.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/TZI8SLKP/2303.html:text/html},
}

@article{robertson_probabilistic_2009,
	title = {The {Probabilistic} {Relevance} {Framework}: {BM25} and {Beyond}},
	volume = {3},
	shorttitle = {The {Probabilistic} {Relevance} {Framework}},
	doi = {10.1561/1500000019},
	abstract = {The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
	journal = {Foundations and Trends in Information Retrieval},
	author = {Robertson, Stephen and Zaragoza, Hugo},
	month = jan,
	year = {2009},
	keywords = {Background, historical, QA, Retrieval},
	pages = {333--389},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/T6AIFA9G/Robertson und Zaragoza - 2009 - The Probabilistic Relevance Framework BM25 and Be.pdf:application/pdf},
}

@inproceedings{thakur_beir_2021,
    title = "{BEIR}:  Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models",
    author = {Thakur, Nandan and
      Reimers, Nils and
      R{\"u}ckl{\'e}, Andreas and
      Srivastava, Abhishek and
      Gurevych, Iryna},
  booktitle = {Proceedings of the 2021 Neural Information Processing Systems (NeurIPS-2021): Track on Datasets and Benchmarks},
  year = {2021},
  url = "https://arxiv.org/abs/2104.08663"
}

@inproceedings{devlin_bert_2019,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{khattab_colbert_2020,
author = {Khattab, Omar and Zaharia, Matei},
title = {ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401075},
doi = {10.1145/3397271.3401075},
abstract = {Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {39–48},
numpages = {10},
keywords = {bert, deep language models, efficiency, neural ir},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{yang_hotpotqa_2018,
    title = "{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering",
    author = "Yang, Zhilin  and
      Qi, Peng  and
      Zhang, Saizheng  and
      Bengio, Yoshua  and
      Cohen, William  and
      Salakhutdinov, Ruslan  and
      Manning, Christopher D.",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1259",
    doi = "10.18653/v1/D18-1259",
    pages = "2369--2380",
    abstract = "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems{'} ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.",
}

@unpublished{zhang_beam_2023,
	title = {Beam {Retrieval}: {General} {End}-to-{End} {Retrieval} for {Multi}-{Hop} {Question} {Answering}},
	shorttitle = {Beam {Retrieval}},
	url = {http://arxiv.org/abs/2308.08973},
	doi = {10.48550/arXiv.2308.08973},
	abstract = {Multi-hop QA involves finding multiple relevant passages and step-by-step reasoning to answer complex questions. While previous approaches have developed retrieval modules for selecting relevant passages, they face challenges in scenarios beyond two hops, owing to the limited performance of one-step methods and the failure of two-step methods when selecting irrelevant passages in earlier stages. In this work, we introduce Beam Retrieval, a general end-to-end retrieval framework for multi-hop QA. This approach maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. Moreover, Beam Retrieval jointly optimizes an encoder and two classification heads by minimizing the combined loss across all hops. To establish a complete QA system, we incorporate a supervised reader or a zero-shot GPT-3.5. Experimental results demonstrate that Beam Retrieval achieves a nearly 50\% improvement compared with baselines on challenging MuSiQue-Ans, and it also surpasses all previous retrievers on HotpotQA and 2WikiMultiHopQA. Providing high-quality context, Beam Retrieval helps our supervised reader achieve new state-of-the-art performance and substantially improves (up to 28.8 points) the QA performance of zero-shot GPT-3.5.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Zhang, Jiahao and Zhang, Haiyang and Zhang, Dongmei and Liu, Yong and Huang, Shen},
	month = aug,
	year = {2023},
	note = {arXiv. \url{http://arxiv.org/abs/2308.08973}},
	keywords = {Background, Computer Science - Computation and Language, LLM, QA, Retrieval},
	annote = {Comment: Code is available at https://github.com/canghongjian/beam\_retriever},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/CRKYSGUB/Zhang et al. - 2023 - Beam Retrieval General End-to-End Retrieval for M.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/HKGI5N7M/2308.html:text/html},
}

@inproceedings{he_deberta_2020,
	title = {{DEBERTA}: {DECODING}-{ENHANCED} {BERT} {WITH} {DISENTANGLED} {ATTENTION}},
	shorttitle = {{DEBERTA}},
	url = {https://openreview.net/forum?id=XPZIaotutsD},
	abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models’ generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understand(NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9\% (90.2\% vs. 91.1\%), on SQuAD v2.0 by +2.3\% (88.4\% vs. 90.7\%) and RACE by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, outperforming the human baseline by a decent margin (90.3 versus 89.8). The pre-trained DeBERTa models and the source code were released at: https://github.com/microsoft/DeBERTa.},
	language = {en},
	urldate = {2023-09-23},
	author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
	month = oct,
	year = {2020},
	keywords = {Background, LLM, QA, Retrieval},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/ICFYT6KS/He et al. - 2020 - DEBERTA DECODING-ENHANCED BERT WITH DISENTANGLED .pdf:application/pdf},
}

@inproceedings{luo_choose_2022,
    author = {Luo, Man and Hashimoto, Kazuma and Yavuz, Semih and Liu, Zhiwei and Baral, Chitta and Zhou, Yingbo},
    title = {Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering},
    booktitle = {Proceedings of the 1st Workshop on Semiparametric Methods in NLP: Decoupling Logic from Knowledge},
    year = {2022},
    month = {01},
    pages = {7-22},
    doi = {10.18653/v1/2022.spanlp-1.2},
}


@unpublished{khashabi_unifiedqa-v2_2022,
  title={UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training},
  author={Daniel Khashabi and Yeganeh Kordi and Hannaneh Hajishirzi},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.12359},
  note = {arXiv. \url{http://arxiv.org/abs/2202.12359}},
  url={https://api.semanticscholar.org/CorpusID:247154787}
}

@inproceedings{serban_generating_2016,
	address = {Berlin, Germany},
	title = {Generating {Factoid} {Questions} {With} {Recurrent} {Neural} {Networks}: {The} {30M} {Factoid} {Question}-{Answer} {Corpus}},
	shorttitle = {Generating {Factoid} {Questions} {With} {Recurrent} {Neural} {Networks}},
	url = {https://aclanthology.org/P16-1056},
	doi = {10.18653/v1/P16-1056},
	urldate = {2023-09-23},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Serban, Iulian Vlad and García-Durán, Alberto and Gulcehre, Caglar and Ahn, Sungjin and Chandar, Sarath and Courville, Aaron and Bengio, Yoshua},
	month = aug,
	year = {2016},
	keywords = {Background, Limits, QA},
	pages = {588--598},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/F3HMB6DH/Serban et al. - 2016 - Generating Factoid Questions With Recurrent Neural.pdf:application/pdf},
}

@article{kwiatkowski_natural_2019,
	title = {Natural {Questions}: {A} {Benchmark} for {Question} {Answering} {Research}},
	volume = {7},
	shorttitle = {Natural {Questions}},
	url = {https://aclanthology.org/Q19-1026},
	doi = {10.1162/tacl_a_00276},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	urldate = {2023-09-23},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	year = {2019},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	keywords = {Background, Dataset, QA},
	pages = {452--466},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/4DA5FW2C/Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answer.pdf:application/pdf},
}

@unpublished{neelakantan_text_2022,
  title={Text and Code Embeddings by Contrastive Pre-Training},
  author={Arvind Neelakantan and Tao Xu and Raul Puri and Alec Radford and Jesse Michael Han and Jerry Tworek and Qiming Yuan and Nikolas A. Tezak and Jong Wook Kim and Chris Hallacy and Johannes Heidecke and Pranav Shyam and Boris Power and Tyna Eloundou Nekoul and Girish Sastry and Gretchen Krueger and David P. Schnurr and Felipe Petroski Such and Kenny Sai-Kin Hsu and Madeleine Thompson and Tabarak Khan and Toki Sherbakov and Joanne Jang and Peter Welinder and Lilian Weng},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.10005},
  note = {arXiv. \url{https://arxiv.org/abs/2201.10005}},
  url={https://api.semanticscholar.org/CorpusID:246275593}
}

@article{sachan_questions_2023,
    title = "Questions Are All You Need to Train a Dense Passage Retriever",
    author = "Sachan, Devendra Singh  and
      Lewis, Mike  and
      Yogatama, Dani  and
      Zettlemoyer, Luke  and
      Pineau, Joelle  and
      Zaheer, Manzil",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.35",
    doi = "10.1162/tacl_a_00564",
    pages = "600--616",
    abstract = "We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g., questions and potential answer passages). It uses a new passage-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence passages, and (2) the passages are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both passage and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtains state-of-the-art results on multiple QA retrieval benchmarks with only generic initialization from a pre-trained language model, removing the need for labeled data and task-specific losses.1 Our code and model checkpoints are available at: \url{https://github.com/DevSinghSachan/art}.",
}

@inproceedings{ni_large_2021,
    title = "Large Dual Encoders Are Generalizable Retrievers",
    author = "Ni, Jianmo  and
      Qu, Chen  and
      Lu, Jing  and
      Dai, Zhuyun  and
      Hernandez Abrego, Gustavo  and
      Ma, Ji  and
      Zhao, Vincent  and
      Luan, Yi  and
      Hall, Keith  and
      Chang, Ming-Wei  and
      Yang, Yinfei",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.669",
    doi = "10.18653/v1/2022.emnlp-main.669",
    pages = "9844--9855",
    abstract = "It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited compared to models with fine-grained interactions between the query and the passage. In this paper, we challenge this belief by scaling up the size of the dual encoder model \textit{while keeping the bottleneck layer as a single dot-product with a fixed size.} With multi-stage training, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. We further analyze the impact of the bottleneck layer and demonstrate diminishing improvement when scaling up the embedding size. Experimental results show that our dual encoders, \textbf{G}eneralizable \textbf{T}5-based dense \textbf{R}etrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10{\%} of MS Marco supervised data to match the out-of-domain performance of using all supervised data.",
}

@unpublished{dai_promptagator_2022,
	title = {Promptagator: {Few}-shot {Dense} {Retrieval} {From} 8 {Examples}},
	shorttitle = {Promptagator},
	url = {http://arxiv.org/abs/2209.11755},
	doi = {10.48550/arXiv.2209.11755},
	abstract = {Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples \{without\} using Natural Questions or MS MARCO to train \%question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Dai, Zhuyun and Zhao, Vincent Y. and Ma, Ji and Luan, Yi and Ni, Jianmo and Lu, Jing and Bakalov, Anton and Guu, Kelvin and Hall, Keith B. and Chang, Ming-Wei},
	month = sep,
	year = {2022},
	note = {arXiv. \url{http://arxiv.org/abs/2209.11755}},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, OOD, QA, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/6DV4A7KK/Dai et al. - 2022 - Promptagator Few-shot Dense Retrieval From 8 Exam.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/ACSLQCES/2209.html:text/html},
}

@inproceedings{lyu_extending_2022,
    title = "Extending the Scope of Out-of-Domain: Examining {QA} models in multiple subdomains",
    author = "Lyu, Chenyang  and
      Foster, Jennifer  and
      Graham, Yvette",
    editor = "Tafreshi, Shabnam  and
      Sedoc, Jo{\~a}o  and
      Rogers, Anna  and
      Drozd, Aleksandr  and
      Rumshisky, Anna  and
      Akula, Arjun",
    booktitle = "Proceedings of the Third Workshop on Insights from Negative Results in NLP",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.insights-1.4",
    doi = "10.18653/v1/2022.insights-1.4",
    pages = "24--37",
    abstract = "Past work that investigates out-of-domain performance of QA systems has mainly focused on general domains (e.g. news domain, wikipedia domain), underestimating the importance of subdomains defined by the internal characteristics of QA datasets. In this paper, we extend the scope of {``}out-of-domain{''} by splitting QA examples into different subdomains according to their internal characteristics including question type, text length, answer position. We then examine the performance of QA systems trained on the data from different subdomains. Experimental results show that the performance of QA systems can be significantly reduced when the train data and test data come from different subdomains. These results question the generalizability of current QA systems in multiple subdomains, suggesting the need to combat the bias introduced by the internal characteristics of QA datasets.",
}

@unpublished{gholami_zero-shot_2021,
  title={Zero-Shot Open-Book Question Answering},
  author={Sia Gholami and Mehdi Noori},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.11520},
  note = {arXiv. \url{https://arxiv.org/abs/2111.11520}},
  url={https://api.semanticscholar.org/CorpusID:244488674}
}

@inproceedings{ding_v-doc_2022,
author = {Y. Ding and Z. Huang and R. Wang and Y. Zhang and X. Chen and Y. Ma and H. Chung and S. Han},
booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {V-Doc : Visual questions answers with Documents},
year = {2022},
volume = {},
issn = {},
pages = {21460-21466},
abstract = {We propose V-Doc, a question-answering tool using document images and PDF, mainly for researchers and general non-deep learning experts looking to generate, process, and understand the document visual question answering tasks. The V-Doc supports generating and using both extractive and abstractive question-answer pairs using documents images. The extractive QA selects a subset of tokens or phrases from the document contents to predict the answers, while the abstractive QA recognises the language in the content and generates the answer based on the trained model. Both aspects are crucial to understanding the documents, especially in an image format. We include a detailed scenario of question generation for the abstractive QA task. V-Doc supports a wide range of datasets and models, and is highly extensible through a declarative, framework-agnostic platform.11Data and demo video: https://github.com/usydnlp/vdoc},
keywords = {deep learning;visualization;computer vision;computational modeling;predictive models;portable document format;question answering (information retrieval)},
doi = {10.1109/CVPR52688.2022.02083},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52688.2022.02083},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@misc{noauthor_question_nodate,
    author = {Langchain},
    title = {Question Answering | Langchain. \url{https://python.langchain.com/docs/use_cases/question_answering/}},
    year = {2023},
    note = {Accessed: 2023-09-24},
    keywords = {Background, LLM, QA, related work},
}

@inproceedings{izacard_leveraging_2021,
	address = {Online},
	title = {Leveraging {Passage} {Retrieval} with {Generative} {Models} for {Open} {Domain} {Question} {Answering}},
	url = {https://aclanthology.org/2021.eacl-main.74},
	doi = {10.18653/v1/2021.eacl-main.74},
	abstract = {Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.},
	urldate = {2023-09-24},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {Izacard, Gautier and Grave, Edouard},
	month = apr,
	year = {2021},
	keywords = {Background, LLM, QA, related work},
	pages = {874--880},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/8RPVFNAT/Izacard und Grave - 2021 - Leveraging Passage Retrieval with Generative Model.pdf:application/pdf},
}

@inproceedings{chen_improving_2021,
	title = {Improving {Out}-of-{Domain} {Question} {Answering} with {Mixture} of {Experts}},
	url = {https://www.semanticscholar.org/paper/Improving-Out-of-Domain-Question-Answering-with-of-Chen/646be1b52a97c415f299eb333f2b97d51421be46#related-papers},
	abstract = {Question answering (QA) is an important problem with numerous application in real life. Sometimes, the resource of certain QA task is limited. This work aims to build a robust QA system that can generalize to novel QA tasks with few examples and gradient steps. We propose a Mixture-of-Experts(MoE) style training framework, and use meta-learning methods for domain adaptation. We also explored data augmentation techniques, and successfully improve out-of-domain QA performance of baseline models on F-1 score from 50.81 to 53.84 and exact match (EM) score from 34.82 to 39.27. Our approach achieves a F-1 score of 60.8 and EM score of 42.2 on the out-of-domain QA testing leaderboard.},
	urldate = {2023-09-24},
	author = {Chen, Haofeng},
	year = {2021},
	keywords = {Background, OOD, QA, related work},
	annote = {[TLDR] This work aims to build a robust QA system that can generalize to novel QA tasks with few examples and gradient steps, and proposes a Mixture-of-Experts (MoE) style training framework, and uses meta-learning methods for domain adaptation.},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/89BQMZAT/Chen - 2021 - Improving Out-of-Domain Question Answering with Mi.pdf:application/pdf},
}

@article{raffel_exploring_2023,
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	year = {2020},
	issue_date = {January 2020},
	publisher = {JMLR.org},
	volume = {21},
	number = {1},
	issn = {1532-4435},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	journal = {J. Mach. Learn. Res.},
	month = {jan},
	articleno = {140},
	numpages = {67},
	keywords = {deep learning, attention based models, multi-task learning, natural language processing, transfer learning}
}

@inproceedings{reddy_synthetic_2022,
	author = {Gangi Reddy, Revanth and Iyer, Bhavani and Sultan, Md Arafat and Zhang, Rong and Sil, Avirup and Castelli, Vittorio and Florian, Radu and Roukos, Salim},
	title = {Synthetic Target Domain Supervision for Open Retrieval QA},
	year = {2021},
	isbn = {9781450380379},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3404835.3463085},
	doi = {10.1145/3404835.3463085},
	abstract = {Neural passage retrieval is a new and promising approach in open retrieval question answering. In this work, we stress-test the Dense Passage Retriever (DPR)---a state-of-the-art (SOTA) open domain neural retrieval model---on closed and specialized target domains such as COVID-19, and find that it lags behind standard BM25 in this important real-world setting. To make DPR more robust under domain shift, we explore its fine-tuning with synthetic training examples, which we generate from unlabeled target domain text using a text-to-text generator. In our experiments, this noisy but fully automated target domain supervision gives DPR a sizable advantage over BM25 in out-of-domain settings, making it a more viable model in practice. Finally, an ensemble of BM25 and our improved DPR model yields the best results, further pushing the SOTA for open retrieval QA on multiple out-of-domain test sets.},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {1793–1797},
	numpages = {5},
	keywords = {weak supervision, out-of-domain neural IR, open retrieval question answering, neural passage retrieval},
	location = {<conf-loc>, <city>Virtual Event</city>, <country>Canada</country>, </conf-loc>},
	series = {SIGIR '21}
}

@article{johnson_billion-scale_2017,
  author={Johnson, Jeff and Douze, Matthijs and Jégou, Hervé},
  journal={IEEE Transactions on Big Data}, 
  title={Billion-Scale Similarity Search with GPUs}, 
  year={2021},
  volume={7},
  number={3},
  pages={535-547},
  keywords={Graphics processing units;Quantization (signal);Big Data;Indexing;Task analysis;Random access memory;Similarity search;multimedia databases;indexing methods;graphical processing units},
  doi={10.1109/TBDATA.2019.2921572}
}

@inproceedings{muller_bioasq_2015,
	address = {Cham},
	title = {{BioASQ}: {A} {Challenge} on {Large}-{Scale} {Biomedical} {Semantic} {Indexing} and {Question} {Answering}},
	volume = {9059},
	isbn = {978-3-319-24470-9 978-3-319-24471-6},
	shorttitle = {{BioASQ}},
	url = {http://link.springer.com/10.1007/978-3-319-24471-6_3},
	doi = {10.1007/978-3-319-24471-6_3},
	abstract = {BioASQ is a series of challenges that aims to assess the performance of information systems in supporting two tasks that are central to the biomedical question answering process: a the indexing of large volumes of unlabelled data, primarily scientific articles, with biomedical concepts, b the processing of biomedical questions and the generation of answers and supporting material. In this paper, the main results of the first two BioASQ challenges are presented.},
	language = {en},
	urldate = {2023-09-24},
	publisher = {Springer International Publishing},
	author = {Balikas, Georgios and Krithara, Anastasia and Partalas, Ioannis and Paliouras, George},
	editor = {Müller, Henning and Jimenez Del Toro, Oscar Alfonso and Hanbury, Allan and Langs, Georg and Foncubierta Rodriguez, Antonio},
	year = {2015},
	doi = {10.1007/978-3-319-24471-6_3},
	note = {Book Title: Multimodal Retrieval in the Medical Domain
Series Title: Lecture Notes in Computer Science},
	keywords = {Background, Dataset, QA, related work},
	pages = {26--39},
	annote = {[TLDR] The main results of the first two BioASQ challenges are presented and the performance of information systems in supporting two tasks that are central to the biomedical question answering process are assessed.},
}

@article{gururangan_dont_2020,
	title = {Don’t {Stop} {Pretraining}: {Adapt} {Language} {Models} to {Domains} and {Tasks}},
	shorttitle = {Don’t {Stop} {Pretraining}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.740},
	doi = {10.18653/v1/2020.acl-main.740},
	abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
	language = {en},
	urldate = {2023-09-24},
	journal = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	author = {Gururangan, Suchin and Marasović, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
	year = {2020},
	note = {Conference Name: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
Place: Online
Publisher: Association for Computational Linguistics},
	keywords = {Background, OOD, QA, related work},
	pages = {8342--8360},
	annote = {[TLDR] It is consistently found that multi-phase adaptive pretraining offers large gains in task performance, and it is shown that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable.},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/6KXDI3ZI/Gururangan et al. - 2020 - Don’t Stop Pretraining Adapt Language Models to D.pdf:application/pdf},
}

@misc{noauthor_welcome_nodate,
    author = {PyPDF2},
    title = {Welcome to PyPDF2 — PyPDF2 documentation. \url{https://pypdf2.readthedocs.io/en/3.0.0/index.html}},
    year = {2023},
    note = {Accessed: 2023-09-25},
    keywords = {Background, Extract, Main, Tools},
}
@inproceedings{santhanam_colbertv2_2022,
    title = "{C}ol{BERT}v2: Effective and Efficient Retrieval via Lightweight Late Interaction",
    author = "Santhanam, Keshav  and
      Khattab, Omar  and
      Saad-Falcon, Jon  and
      Potts, Christopher  and
      Zaharia, Matei",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.272",
    doi = "10.18653/v1/2022.naacl-main.272",
    pages = "3715--3734",
    abstract = "Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6{--}10x.",
}

@inproceedings{bajaj_ms_2018,
  author       = {Tri Nguyen and
                  Mir Rosenberg and
                  Xia Song and
                  Jianfeng Gao and
                  Saurabh Tiwary and
                  Rangan Majumder and
                  Li Deng},
  editor       = {Tarek Richard Besold and
                  Antoine Bordes and
                  Artur S. d'Avila Garcez and
                  Greg Wayne},
  title        = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
  booktitle    = {Proceedings of the Workshop on Cognitive Computation: Integrating
                  neural and symbolic approaches 2016 co-located with the 30th Annual
                  Conference on Neural Information Processing Systems {(NIPS} 2016),
                  Barcelona, Spain, December 9, 2016},
  series       = {{CEUR} Workshop Proceedings},
  volume       = {1773},
  publisher    = {CEUR-WS.org},
  year         = {2016},
  url          = {https://ceur-ws.org/Vol-1773/CoCoNIPS\_2016\_paper9.pdf},
  timestamp    = {Fri, 10 Mar 2023 16:22:16 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/NguyenRSGTMD16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang_minilm_2020,
	author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
	title = {MINILM: deep self-attention distillation for task-agnostic compression of pre-trained transformers},
	year = {2020},
	isbn = {9781713829546},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Pre-trained language models (e.g., BERT [12] and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer [42] based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant [26] also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99\% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50\% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.},
	booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	articleno = {485},
	numpages = {13},
	location = {Vancouver, BC, Canada},
	series = {NIPS'20}
}

@article{liu_roberta_2019,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {CoRR},
  volume       = {abs/1907.11692},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.11692},
  eprinttype    = {arXiv},
  eprint       = {1907.11692},
  timestamp    = {Thu, 14 Dec 2023 18:03:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lewis_bart_2019,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@inproceedings{pereira_visconde_2022,
  author    = {Jayr Alencar Pereira and
               Robson do Nascimento Fidalgo and
               Roberto de Alencar Lotufo and
               Rodrigo Frassetto Nogueira},
  editor    = {Jaap Kamps and
               Lorraine Goeuriot and
               Fabio Crestani and
               Maria Maistro and
               Hideo Joho and
               Brian Davis and
               Cathal Gurrin and
               Udo Kruschwitz and
               Annalina Caputo},
  title     = {Visconde: Multi-document {QA} with {GPT-3} and Neural Reranking},
  booktitle = {Advances in Information Retrieval - 45th European Conference on Information
               Retrieval, {ECIR} 2023, Dublin, Ireland, April 2-6, 2023, Proceedings,
               Part {II}},
  series    = {Lecture Notes in Computer Science},
  volume    = {13981},
  pages     = {534--543},
  publisher = {Springer},
  year      = {2023},
  url       = {https://doi.org/10.1007/978-3-031-28238-6\_44},
  doi       = {10.1007/978-3-031-28238-6\_44},
  timestamp = {Tue, 21 Mar 2023 16:23:57 +0100},
  biburl    = {https://dblp.org/rec/conf/ecir/PereiraFLN23.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{fajcik_r2-d2_2021,
	address = {Punta Cana, Dominican Republic},
	title = {R2-{D2}: {A} {Modular} {Baseline} for {Open}-{Domain} {Question} {Answering}},
	shorttitle = {R2-{D2}},
	url = {https://aclanthology.org/2021.findings-emnlp.73},
	doi = {10.18653/v1/2021.findings-emnlp.73},
	abstract = {This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank twice, reaD twice). The pipeline is composed of a retriever, passage reranker, extractive reader, generative reader and a mechanism that aggregates the final prediction from all system's components. We demonstrate its strength across three open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA, surpassing state-of-the-art on the first two. Our analysis demonstrates that: (i) combining extractive and generative reader yields absolute improvements up to 5 exact match and it is at least twice as effective as the posterior averaging ensemble of the same models with different parameters, (ii) the extractive reader with fewer parameters can match the performance of the generative reader on extractive QA datasets.},
	urldate = {2023-09-26},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Fajcik, Martin and Docekal, Martin and Ondrej, Karel and Smrz, Pavel},
	month = nov,
	year = {2021},
	keywords = {Main, Reader, Retrieval},
	pages = {854--870},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/BFJ55W2M/Fajcik et al. - 2021 - R2-D2 A Modular Baseline for Open-Domain Question.pdf:application/pdf},
}


@inproceedings{xu_laprador_2022,
	title = {{LaPraDoR}: {Unsupervised} {Pretrained} {Dense} {Retriever} for {Zero}-{Shot} {Text} {Retrieval}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{LaPraDoR}},
	url = {https://arxiv.org/abs/2203.06169},
	doi = {10.48550/ARXIV.2203.06169},
	abstract = {In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training. Specifically, we first present Iterative Contrastive Learning (ICoL) that iteratively trains the query and document encoders with a cache mechanism. ICoL not only enlarges the number of negative instances but also keeps representations of cached examples in the same hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a simple yet effective way to enhance dense retrieval with lexical matching. We evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18 datasets of 9 zero-shot text retrieval tasks. Experimental results show that LaPraDoR achieves state-of-the-art performance compared with supervised dense retrieval models, and further analysis reveals the effectiveness of our training strategy and objectives. Compared to re-ranking, our lexicon-enhanced approach can be run in milliseconds (22.5x faster) while achieving superior performance.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
	year = {2022},
	note = {Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Background, Computation and Language (cs.CL), FOS: Computer and information sciences, Information Retrieval (cs.IR), Machine Learning (cs.LG), Main, OOD, QA, Retrieval},
	annote = {Other
ACL 2022 (Findings)},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/4FIY9ESH/Xu et al. - 2022 - LaPraDoR Unsupervised Pretrained Dense Retriever .pdf:application/pdf},
}

@article{zaib_conversational_2021,
       author = {{Zaib}, Munazza and {Zhang}, Wei Emma and {Sheng}, Quan Z. and {Mahmood}, Adnan and {Zhang}, Yang},
        title = "{Conversational Question Answering: A Survey}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
         year = 2021,
        month = jun,
          eid = {arXiv:2106.00874},
        pages = {arXiv:2106.00874},
          doi = {10.48550/arXiv.2106.00874},
archivePrefix = {arXiv},
       eprint = {2106.00874},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210600874Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{reddy_coqa_2018,
    title = "{C}o{QA}: A Conversational Question Answering Challenge",
    author = "Reddy, Siva  and
      Chen, Danqi  and
      Manning, Christopher D.",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1016",
    doi = "10.1162/tacl_a_00266",
    pages = "249--266",
    abstract = "Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4{\%}, which is 23.4 points behind human performance (88.8{\%}), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at \url{https://stanfordnlp.github.io/coqa}.",
}

@inproceedings{gupta_conversational_2020,
	address = {Barcelona, Spain (Online)},
	title = {Conversational {Machine} {Comprehension}: a {Literature} {Review}},
	shorttitle = {Conversational {Machine} {Comprehension}},
	url = {https://aclanthology.org/2020.coling-main.247},
	doi = {10.18653/v1/2020.coling-main.247},
	abstract = {Conversational Machine Comprehension (CMC), a research track in conversational AI, expects the machine to understand an open-domain natural language text and thereafter engage in a multi-turn conversation to answer questions related to the text. While most of the research in Machine Reading Comprehension (MRC) revolves around single-turn question answering (QA), multi-turn CMC has recently gained prominence, thanks to the advancement in natural language understanding via neural language models such as BERT and the introduction of large-scale conversational datasets such as CoQA and QuAC. The rise in interest has, however, led to a flurry of concurrent publications, each with a different yet structurally similar modeling approach and an inconsistent view of the surrounding literature. With the volume of model submissions to conversational datasets increasing every year, there exists a need to consolidate the scattered knowledge in this domain to streamline future research. This literature review attempts at providing a holistic overview of CMC with an emphasis on the common trends across recently published models, specifically in their approach to tackling conversational history. The review synthesizes a generic framework for CMC models while highlighting the differences in recent approaches and intends to serve as a compendium of CMC for future researchers.},
	urldate = {2023-09-28},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Gupta, Somil and Rawat, Bhanu Pratap Singh and Yu, Hong},
	month = dec,
	year = {2020},
	keywords = {Background, basics, CQA},
	pages = {2739--2753},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/BEBVP43P/Gupta et al. - 2020 - Conversational Machine Comprehension a Literature.pdf:application/pdf},
}


@article{owoicho_trec_2022,
	title = {{TREC} {CAsT} 2022: {Going} {Beyond} {User} {Ask} and {System} {Retrieve} with {Initiative} and {Response} {Generation}},
	language = {en},
	author = {Owoicho, Paul and Dalton, Jeffrey and Aliannejadi, Mohammad and Azzopardi, Leif and Trippas, Johanne R and Vakulenko, Svitlana},
	year = {2022},
	keywords = {Background, CQA, groundwork},
	file = {Owoicho et al. - 2022 - TREC CAsT 2022 Going Beyond User Ask and System R.pdf:/Users/lenert/Zotero/storage/AEKWQX27/Owoicho et al. - 2022 - TREC CAsT 2022 Going Beyond User Ask and System R.pdf:application/pdf},
}

@unpublished{rastogi_schema-guided_2020,
  title={Schema-Guided Dialogue State Tracking Task at DSTC8},
  author={Abhinav Rastogi and Xiaoxue Zang and Srinivas Sunkara and Raghav Gupta and Pranav Khaitan},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.01359},
  note = {arXiv. \url{https://arxiv.org/abs/2002.01359}},
  url={https://api.semanticscholar.org/CorpusID:211020805}
}

@inproceedings{voskarides_query_2020,
	title = {Query {Resolution} for {Conversational} {Search} with {Limited} {Supervision}},
	url = {http://arxiv.org/abs/2005.11723},
	doi = {10.1145/3397271.3401130},
	abstract = {In this work we focus on multi-turn passage retrieval as a crucial component of conversational search. One of the key challenges in multi-turn passage retrieval comes from the fact that the current turn query is often underspecified due to zero anaphora, topic change, or topic return. Context from the conversational history can be used to arrive at a better expression of the current turn query, defined as the task of query resolution. In this paper, we model the query resolution task as a binary term classification problem: for each term appearing in the previous turns of the conversation decide whether to add it to the current turn query or not. We propose QuReTeC (Query Resolution by Term Classification), a neural query resolution model based on bidirectional transformers. We propose a distant supervision method to automatically generate training data by using query-passage relevance labels. Such labels are often readily available in a collection either as human annotations or inferred from user interactions. We show that QuReTeC outperforms state-of-the-art models, and furthermore, that our distant supervision method can be used to substantially reduce the amount of human-curated data required to train QuReTeC. We incorporate QuReTeC in a multi-turn, multi-stage passage retrieval architecture and demonstrate its effectiveness on the TREC CAsT dataset.},
	urldate = {2023-09-29},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Voskarides, Nikos and Li, Dan and Ren, Pengjie and Kanoulas, Evangelos and de Rijke, Maarten},
	month = jul,
	year = {2020},
	note = {arXiv:2005.11723 [cs]},
	keywords = {Background, Computer Science - Computation and Language, Computer Science - Information Retrieval, context, CQA},
	pages = {921--930},
	annote = {Comment: SIGIR 2020 full conference paper},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/UXFU4WJK/Voskarides et al. - 2020 - Query Resolution for Conversational Search with Li.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/9ZNVM853/2005.html:text/html},
}


@article{yang_query_2019,
	title = {Query and {Answer} {Expansion} from {Conversation} {History}},
	abstract = {In this paper, we present our methods, experimental analysis, and final submissions for the Conversational Assistance Track (CAsT) at TREC 2019. In addition to language understanding, extracting knowledge from historical dialogues (e.g., previous queries, searching results) is a key to the conversational IR task. However, limited annotated data in the CAsT task makes machine learning or other data-driven approaches infeasible. Along this line, we propose two ad hoc and intuitive approaches: Historical Query Expansion and Historical Answer Expansion, to improve the performance of the conversational IR system with limited training data. Our empirical result on the CAsT training set shows that the proposed methods significantly improve the quality of conversational search in terms of retrieval (recall@1000: 0.774 → 0.844) and ranking (mAP: 0.187 → 0.197) compared to our strong baseline. As a result, our submitted entries outperform the median performance of all the 21 teams.},
	language = {en},
	author = {Yang, Jheng-Hong and Lin, Sheng-Chieh and Lin, Jimmy and Tsai, Ming-Feng and Wang, Chuan-Ju},
	year = {2019},
	keywords = {Background, context, CQA},
	file = {Yang et al. - 2019 - Query and Answer Expansion from Conversation Histo.pdf:/Users/lenert/Zotero/storage/R6QI87IJ/Yang et al. - 2019 - Query and Answer Expansion from Conversation Histo.pdf:application/pdf},
}

@article{dalton_trec_2020,
  author       = {Jeffrey Dalton and
                  Chenyan Xiong and
                  Jamie Callan},
  title        = {{TREC} CAsT 2019: The Conversational Assistance Track Overview},
  journal      = {CoRR},
  volume       = {abs/2003.13624},
  year         = {2020},
  url          = {https://arxiv.org/abs/2003.13624},
  eprinttype    = {arXiv},
  eprint       = {2003.13624},
  timestamp    = {Mon, 06 Apr 2020 14:36:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2003-13624.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{mao_large_2023,
	title = {Large {Language} {Models} {Know} {Your} {Contextual} {Search} {Intent}: {A} {Prompting} {Framework} for {Conversational} {Search}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Large {Language} {Models} {Know} {Your} {Contextual} {Search} {Intent}},
	url = {https://arxiv.org/abs/2303.06573},
	doi = {10.48550/ARXIV.2303.06573},
	abstract = {In this paper, we present a prompting framework called LLMCS that leverages large language models, such as code-davinci-002 of GPT-3, to perform few-shot conversational query rewriting for conversational search. We explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose aggregating them into an integrated representation that can robustly represent the user's real contextual search intent. Experimental results on two conversational search datasets, including CAst-19 and CAsT-20, show that our approach achieves significant improvements in search effectiveness over existing baselines and manual rewrites. Notably, LLMCS can significantly outperform the state-of-the-art baselines by up to +5.9{\textbackslash}\% and +32.9{\textbackslash}\% w.r.t. NDCG@3 on CAsT-19 and CAsT-20, highlighting the vast potential of large language models for conversational search. Our code will be released at https://github.com/kyriemao/LLMCS.},
	urldate = {2023-09-29},
	author = {Mao, Kelong and Dou, Zhicheng and Chen, Haonan and Mo, Fengran and Qian, Hongjin},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Background, FOS: Computer and information sciences, Information Retrieval (cs.IR), CQA, context},
	annote = {Other
Work in progress},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/MRFP5LSC/Mao et al. - 2023 - Large Language Models Know Your Contextual Search .pdf:application/pdf},
}


@article{dai_dialog_2022,
	title = {Dialog {Inpainting}: {Turning} {Documents} into {Dialogs}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Dialog {Inpainting}},
	url = {https://arxiv.org/abs/2205.09073},
	doi = {10.48550/ARXIV.2205.09073},
	abstract = {Many important questions (e.g. "How to eat healthier?") require conversation to establish context and explore in depth. However, conversational question answering (ConvQA) systems have long been stymied by scarce training data that is expensive to collect. To address this problem, we propose a new technique for synthetically generating diverse and high-quality dialog data: dialog inpainting. Our approach takes the text of any document and transforms it into a two-person dialog between the writer and an imagined reader: we treat sentences from the article as utterances spoken by the writer, and then use a dialog inpainter to predict what the imagined reader asked or said in between each of the writer's utterances. By applying this approach to passages from Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets totalling 19 million diverse information-seeking dialogs -- 1,000x larger than the largest existing ConvQA dataset. Furthermore, human raters judge the answer adequacy and conversationality of WikiDialog to be as good or better than existing manually-collected datasets. Using our inpainted data to pre-train ConvQA retrieval systems, we significantly advance state-of-the-art across three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40\% relative gains on standard evaluation metrics.},
	urldate = {2023-09-28},
	author = {Dai, Zhuyun and Chaganty, Arun Tejasvi and Zhao, Vincent and Amini, Aida and Rashid, Qazi Mamunur and Green, Mike and Guu, Kelvin},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Background, basics, Computation and Language (cs.CL), CQA, FOS: Computer and information sciences, OOD},
	annote = {[TLDR] Using inpainted data to pre-train ConvQA retrieval systems, this work significantly advance state-of-the-art across three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40\% relative gains on standard evaluation metrics.},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/MZCXY4DC/Dai et al. - 2022 - Dialog Inpainting Turning Documents into Dialogs.pdf:application/pdf},
}


@article{liusie_university_nodate,
	title = {{UNIVERSITY} {OF} {CAMBRIDGE} {AT} {TREC} {CAST} 2022},
	abstract = {Team heatwave (of the University of Cambridge) submitted 3 automatic runs to the TREC 2022 Conversational Assistance Track. This paper discusses our approach to the challenge of conversational informational retrieval. We first describe our four stage approach of query reformulation, BM25 retrieval, passage reranking, and response extraction. Our experiments then show that our multi-query approach, which uses the raw concatenated conversational context for BM25 and the rewritten query for reranking, shows considerable performance improvement over a single-query approach, where our best performing system achieves a NDCG@3 of 0.440 in the 2022 CAsT challenge.},
	language = {en},
	author = {Liusie, Adian and Qian, Mengjie and Li, Xiang and Gales, Mark},
	keywords = {Background, context, CQA},
	file = {Liusie et al. - UNIVERSITY OF CAMBRIDGE AT TREC CAST 2022.pdf:/Users/lenert/Zotero/storage/BIN984IL/Liusie et al. - UNIVERSITY OF CAMBRIDGE AT TREC CAST 2022.pdf:application/pdf},
	year = {2022}
}


@inproceedings{elgohary_can_2019,
	address = {Hong Kong, China},
	title = {Can {You} {Unpack} {That}? {Learning} to {Rewrite} {Questions}-in-{Context}},
	shorttitle = {Can {You} {Unpack} {That}?},
	url = {https://aclanthology.org/D19-1605},
	doi = {10.18653/v1/D19-1605},
	abstract = {Question answering is an AI-complete problem, but existing datasets lack key elements of language understanding such as coreference and ellipsis resolution. We consider sequential question answering: multiple questions are asked one-by-one in a conversation between a questioner and an answerer. Answering these questions is only possible through understanding the conversation history. We introduce the task of question-in-context rewriting: given the context of a conversation's history, rewrite a context-dependent into a self-contained question with the same answer. We construct, CANARD, a dataset of 40,527 questions based on QuAC (Choi et al., 2018) and train Seq2Seq models for incorporating context into standalone questions.},
	urldate = {2023-09-29},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Elgohary, Ahmed and Peskov, Denis and Boyd-Graber, Jordan},
	month = nov,
	year = {2019},
	keywords = {Background, context, CQA},
	pages = {5918--5924},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/MTJJ779F/Elgohary et al. - 2019 - Can You Unpack That Learning to Rewrite Questions.pdf:application/pdf},
}

@article{touvron_llama_2023,
       author = {{Touvron}, Hugo and {Martin}, Louis and {Stone}, Kevin and {Albert}, Peter and {Almahairi}, Amjad and {Babaei}, Yasmine and {Bashlykov}, Nikolay and {Batra}, Soumya and {Bhargava}, Prajjwal and {Bhosale}, Shruti and {Bikel}, Dan and {Blecher}, Lukas and {Canton Ferrer}, Cristian and {Chen}, Moya and {Cucurull}, Guillem and {Esiobu}, David and {Fernandes}, Jude and {Fu}, Jeremy and {Fu}, Wenyin and {Fuller}, Brian and {Gao}, Cynthia and {Goswami}, Vedanuj and {Goyal}, Naman and {Hartshorn}, Anthony and {Hosseini}, Saghar and {Hou}, Rui and {Inan}, Hakan and {Kardas}, Marcin and {Kerkez}, Viktor and {Khabsa}, Madian and {Kloumann}, Isabel and {Korenev}, Artem and {Singh Koura}, Punit and {Lachaux}, Marie-Anne and {Lavril}, Thibaut and {Lee}, Jenya and {Liskovich}, Diana and {Lu}, Yinghai and {Mao}, Yuning and {Martinet}, Xavier and {Mihaylov}, Todor and {Mishra}, Pushkar and {Molybog}, Igor and {Nie}, Yixin and {Poulton}, Andrew and {Reizenstein}, Jeremy and {Rungta}, Rashi and {Saladi}, Kalyan and {Schelten}, Alan and {Silva}, Ruan and {Smith}, Eric Michael and {Subramanian}, Ranjan and {Tan}, Xiaoqing Ellen and {Tang}, Binh and {Taylor}, Ross and {Williams}, Adina and {Kuan}, Jian Xiang and {Xu}, Puxin and {Yan}, Zheng and {Zarov}, Iliyan and {Zhang}, Yuchen and {Fan}, Angela and {Kambadur}, Melanie and {Narang}, Sharan and {Rodriguez}, Aurelien and {Stojnic}, Robert and {Edunov}, Sergey and {Scialom}, Thomas},
        title = "{Llama 2: Open Foundation and Fine-Tuned Chat Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2023,
        month = jul,
          eid = {arXiv:2307.09288},
          doi = {10.48550/arXiv.2307.09288},
archivePrefix = {arXiv},
       eprint = {2307.09288},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230709288T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@unpublished{anil_palm_2023,
  title={PaLM 2 Technical Report},
  author={Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and Alexandre Tachard Passos and Siamak Shakeri and Emanuel Taropa and Paige Bailey and Z. Chen and Eric Chu and J. Clark and Laurent El Shafey and Yanping Huang and Kathleen S. Meier-Hellstern and Gaurav Mishra and Erica Moreira and Mark Omernick and Kevin Robinson and Sebastian Ruder and Yi Tay and Kefan Xiao and Yuanzhong Xu and Yujing Zhang and Gustavo Hern{\'a}ndez Abrego and Junwhan Ahn and Jacob Austin and Paul Barham and Jan A. Botha and James Bradbury and Siddhartha Brahma and Kevin Michael Brooks and Michele Catasta and Yongzhou Cheng and Colin Cherry and Christopher A. Choquette-Choo and Aakanksha Chowdhery and C Cr{\'e}py and Shachi Dave and Mostafa Dehghani and Sunipa Dev and Jacob Devlin and M. C. D'iaz and Nan Du and Ethan Dyer and Vladimir Feinberg and Fan Feng and Vlad Fienber and Markus Freitag and Xavier Garc{\'i}a and Sebastian Gehrmann and Lucas Gonz{\'a}lez and Guy Gur-Ari and Steven Hand and Hadi Hashemi and Le Hou and Joshua Howland and An Ren Hu and Jeffrey Hui and Jeremy Hurwitz and Michael Isard and Abe Ittycheriah and Matthew Jagielski and Wen Hao Jia and Kathleen Kenealy and Maxim Krikun and Sneha Kudugunta and Chang Lan and Katherine Lee and Benjamin Lee and Eric Li and Mu-Li Li and Wei Li and Yaguang Li and Jun Yu Li and Hyeontaek Lim and Han Lin and Zhong-Zhong Liu and Frederick Liu and Marcello Maggioni and Aroma Mahendru and Joshua Maynez and Vedant Misra and Maysam Moussalem and Zachary Nado and John Nham and Eric Ni and Andrew Nystrom and Alicia Parrish and Marie Pellat and Martin Polacek and Oleksandr Polozov and Reiner Pope and Siyuan Qiao and Emily Reif and Bryan Richter and Parker Riley and Alexandra Ros and Aurko Roy and Brennan Saeta and Rajkumar Samuel and Renee Marie Shelby and Ambrose Slone and Daniel Smilkov and David R. So and Daniela Sohn and Simon Tokumine and Dasha Valter and Vijay Vasudevan and Kiran Vodrahalli and Xuezhi Wang and Pidong Wang and Zirui Wang and Tao Wang and John Wieting and Yuhuai Wu and Ke Xu and Yunhan Xu and Lin Wu Xue and Pengcheng Yin and Jiahui Yu and Qiaoling Zhang and Steven Zheng and Ce Zheng and Wei Zhou and Denny Zhou and Slav Petrov and Yonghui Wu},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.10403},
  note={arXiv. \url{https://arxiv.org/abs/2305.10403}},
  url={https://api.semanticscholar.org/CorpusID:258740735}
}

@article{treviso_efficient_2023,
	title = {Efficient {Methods} for {Natural} {Language} {Processing}: {A} {Survey}},
	volume = {11},
	issn = {2307-387X},
	shorttitle = {Efficient {Methods} for {Natural} {Language} {Processing}},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00577/116725/Efficient-Methods-for-Natural-Language-Processing},
	doi = {10.1162/tacl_a_00577},
	abstract = {Abstract
            Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.},
	language = {en},
	urldate = {2023-10-01},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty Van and Cao, Qingqing and Ciosici, Manuel R. and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and Martins, Pedro H. and Martins, André F. T. and Forde, Jessica Zosa and Milder, Peter and Simpson, Edwin and Slonim, Noam and Dodge, Jesse and Strubell, Emma and Balasubramanian, Niranjan and Derczynski, Leon and Gurevych, Iryna and Schwartz, Roy},
	month = jul,
	year = {2023},
	keywords = {Background, LLM, overview},
	pages = {826--860},
	annote = {[TLDR] This survey synthesizes and relates current methods and findings in efficient NLP to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.},
	file = {Volltext:/Users/lenert/Zotero/storage/TESAP6NV/Treviso et al. - 2023 - Efficient Methods for Natural Language Processing.pdf:application/pdf},
}


@unpublished{ling_domain_2023,
	title = {Domain {Specialization} as the {Key} to {Make} {Large} {Language} {Models} {Disruptive}: {A} {Comprehensive} {Survey}},
	shorttitle = {Domain {Specialization} as the {Key} to {Make} {Large} {Language} {Models} {Disruptive}},
	url = {http://arxiv.org/abs/2305.18703},
	doi = {10.48550/arXiv.2305.18703},
	abstract = {Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Ling, Chen and Zhao, Xujiang and Lu, Jiaying and Deng, Chengyuan and Zheng, Can and Wang, Junxiang and Chowdhury, Tanmoy and Li, Yun and Cui, Hejie and Zhang, Xuchao and Zhao, Tianjiao and Panalkar, Amit and Cheng, Wei and Wang, Haoyu and Liu, Yanchi and Chen, Zhengzhang and Chen, Haifeng and White, Chris and Gu, Quanquan and Pei, Jian and Zhao, Liang},
	month = aug,
	year = {2023},
	note = {arXiv. \url{http://arxiv.org/abs/2305.18703}},
	keywords = {Background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, LLM, overview},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/WC7XEBER/Ling et al. - 2023 - Domain Specialization as the Key to Make Large Lan.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/5SZ74W5X/2305.html:text/html},
}


@unpublished{zhao_survey_2023,
  title={A Survey of Large Language Models},
  author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Z. Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jianyun Nie and Ji-rong Wen},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.18223},
  note={arXiv. \url{https://arxiv.org/abs/2303.18223}},
  url={https://api.semanticscholar.org/CorpusID:257900969}
}

@article{hu_lora_nodate,
	title = {{LORA}: {LOW}-{RANK} {ADAPTATION} {OF} {LARGE} {LAN}- {GUAGE} {MODELS}},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	author = {Hu, Edward and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	keywords = {Background, LLM, PEFT},
	file = {Hu et al. - LORA LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODE.pdf:/Users/lenert/Zotero/storage/8WF4K7RM/Hu et al. - LORA LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODE.pdf:application/pdf},
	year = {2021},
}


@article{chung_scaling_2022,
  author       = {Sheng Shen and
                  Le Hou and
                  Yanqi Zhou and
                  Nan Du and
                  Shayne Longpre and
                  Jason Wei and
                  Hyung Won Chung and
                  Barret Zoph and
                  William Fedus and
                  Xinyun Chen and
                  Tu Vu and
                  Yuexin Wu and
                  Wuyang Chen and
                  Albert Webson and
                  Yunxuan Li and
                  Vincent Zhao and
                  Hongkun Yu and
                  Kurt Keutzer and
                  Trevor Darrell and
                  Denny Zhou},
  title        = {Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse
                  Mixture of Experts},
  journal      = {CoRR},
  volume       = {abs/2305.14705},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.14705},
  doi          = {10.48550/ARXIV.2305.14705},
  eprinttype    = {arXiv},
  eprint       = {2305.14705},
  timestamp    = {Mon, 04 Dec 2023 13:56:31 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-14705.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{noauthor_peft_nodate,
    author = {Huggingface},
    title = {PEFT. \url{https://huggingface.co/docs/peft/index}},
    year = {2023},
    note = {Accessed: 2023-10-01},
    keywords = {Background, LLM, PEFT},
}


@inproceedings{houlsby_parameter-efficient_2019,
  author       = {Neil Houlsby and
                  Andrei Giurgiu and
                  Stanislaw Jastrzebski and
                  Bruna Morrone and
                  Quentin de Laroussilhe and
                  Andrea Gesmundo and
                  Mona Attariyan and
                  Sylvain Gelly},
  editor       = {Kamalika Chaudhuri and
                  Ruslan Salakhutdinov},
  title        = {Parameter-Efficient Transfer Learning for {NLP}},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning,
                  {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {97},
  pages        = {2790--2799},
  publisher    = {{PMLR}},
  year         = {2019},
  url          = {http://proceedings.mlr.press/v97/houlsby19a.html},
  timestamp    = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/HoulsbyGJMLGAG19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li_prefix-tuning_2021,
  author       = {Xiang Lisa Li and
                  Percy Liang},
  editor       = {Chengqing Zong and
                  Fei Xia and
                  Wenjie Li and
                  Roberto Navigli},
  title        = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational
                  Linguistics and the 11th International Joint Conference on Natural
                  Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
                  Event, August 1-6, 2021},
  pages        = {4582--4597},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.acl-long.353},
  doi          = {10.18653/V1/2021.ACL-LONG.353},
  timestamp    = {Wed, 16 Mar 2022 23:55:03 +0100},
  biburl       = {https://dblp.org/rec/conf/acl/LiL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{lester_power_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
	url = {https://aclanthology.org/2021.emnlp-main.243},
	doi = {10.18653/v1/2021.emnlp-main.243},
	abstract = {In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.},
	urldate = {2023-10-01},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	month = nov,
	year = {2021},
	keywords = {Background, LLM, PEFT},
	pages = {3045--3059},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/E7UW5QLV/Lester et al. - 2021 - The Power of Scale for Parameter-Efficient Prompt .pdf:application/pdf},
}


@article{liu_few-shot_2022,
	title = {Few-{Shot} {Parameter}-{Efficient} {Fine}-{Tuning} is {Better} and {Cheaper} than {In}-{Context} {Learning}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2205.05638},
	doi = {10.48550/ARXIV.2205.05638},
	abstract = {Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)\${\textasciicircum}3\$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6\% absolute. All of the code used in our experiments is publicly available.},
	urldate = {2023-10-01},
	author = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Background, overview, Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, LLM, Machine Learning (cs.LG), PEFT},
	annote = {[TLDR] This paper rigorously compares few-shot ICL and PEFT and demonstrates that the latter offers better accuracy as well as dramatically lower computational costs, and introduces a new PEFT method called (IA)\${\textasciicircum}3\$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters.},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/AN4GV9KA/Liu et al. - 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better.pdf:application/pdf},
}

@inproceedings{brown_language_2020,
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
title = {Language models are few-shot learners},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {159},
numpages = {25},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@article{white_prompt_2023,
  author       = {Jules White and
                  Quchen Fu and
                  Sam Hays and
                  Michael Sandborn and
                  Carlos Olea and
                  Henry Gilbert and
                  Ashraf Elnashar and
                  Jesse Spencer{-}Smith and
                  Douglas C. Schmidt},
  title        = {A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT},
  journal      = {CoRR},
  volume       = {abs/2302.11382},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.11382},
  doi          = {10.48550/ARXIV.2302.11382},
  eprinttype    = {arXiv},
  eprint       = {2302.11382},
  timestamp    = {Fri, 24 Feb 2023 11:55:23 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-11382.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{liu_gpt_2021,
	title = {GPT understands, too},
	journal = {AI Open},
	year = {2023},
	issn = {2666-6510},
	doi = {https://doi.org/10.1016/j.aiopen.2023.08.012},
	url = {https://www.sciencedirect.com/science/article/pii/S2666651023000141},
	author = {Xiao Liu and Yanan Zheng and Zhengxiao Du and Ming Ding and Yujie Qian and Zhilin Yang and Jie Tang},
	keywords = {Pre-trained language models, Prompt tuning, Natural language understanding},
	abstract = {Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance—e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.}
}

@inproceedings{wei_chain--thought_2023,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Brian Ichter and
                  Fei Xia and
                  Ed H. Chi and
                  Quoc V. Le and
                  Denny Zhou},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:37 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Wei0SBIXCLZ22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@unpublished{zhu_survey_2023,
	title = {A {Survey} on {Model} {Compression} for {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2308.07633v3},
	abstract = {Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of compressed LLMs. By providing insights into the latest developments and practical implications, this survey serves as an invaluable resource for both researchers and practitioners. As LLMs continue to evolve, this survey aims to facilitate enhanced efficiency and real-world applicability, establishing a foundation for future advancements in the field.},
	language = {en},
	urldate = {2023-10-02},
	journal = {arXiv.org},
	author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
	month = aug,
	year = {2023},
	keywords = {Background, compression, LLM},
	note = {arXiv. \url{https://arxiv.org/abs/2308.07633v3}},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/BRAAL7EM/Zhu et al. - 2023 - A Survey on Model Compression for Large Language M.pdf:application/pdf},
}

@inproceedings{fang_depgraph_2023,
	author = {G. Fang and X. Ma and M. Song and M. Bi Mi and X. Wang},
	booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	title = {DepGraph: Towards Any Structural Pruning},
	year = {2023},
	volume = {},
	issn = {},
	pages = {16091-16101},
	abstract = {Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant, thereby avoiding structural issues and significant performance degradation after pruning. To address this problem, we propose a general and fully automatic method, Dependency Graph (DepGraph), to explicitly model the dependency between layers and comprehensively group coupled parameters for pruning. In this work, we extensively evaluate our method on several architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and Vision transformer for images, GAT for graph, DGCNN for 3D point cloud, alongside LSTM for language, and demonstrate that, even with a simple norm-based criterion, the proposed method consistently yields gratifying performances.},
	keywords = {point cloud compression;degradation;couplings;three-dimensional displays;neural networks;computer architecture;manuals},
	doi = {10.1109/CVPR52729.2023.01544},
	url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.01544},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jun}
}


@inproceedings{frantar_sparsegpt_2023,
author = {Frantar, Elias and Alistarh, Dan},
title = {SparseGPT: massive language models can be accurately pruned in one-shot},
year = {2023},
publisher = {JMLR.org},
abstract = {We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50\% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60\% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/ sparsegpt.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {414},
numpages = {15},
location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
series = {ICML'23}
}


@misc{noauthor_deepsparse_2023,
    author = {Neural Magic},
    title = {DeepSparse. \url{https://github.com/neuralmagic/deepsparse}},
    year = {2023},
    note = {Accessed: 2023-10-02},
    keywords = {auto-ml, Background, compression, computer-vision, cpu-inference-api, cpus, deepsparse-engine, inference, LLM, machinelearning, ml, nlp, object-detection, onnx, pretrained-models, pruning, pytorch, quantization, sparsification},
}



@unpublished{ma_llm-pruner_2023,
	title = {{LLM}-{Pruner}: {On} the {Structural} {Pruning} of {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Pruner}},
	url = {http://arxiv.org/abs/2305.11627},
	doi = {10.48550/arXiv.2305.11627},
	abstract = {Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
	month = sep,
	year = {2023},
	note = {arXiv. \url{http://arxiv.org/abs/2305.11627}},
	keywords = {Background, compression, Computer Science - Computation and Language, LLM},
	annote = {Comment: Accepted at NeurIPS 2023},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/HX6KEILZ/Ma et al. - 2023 - LLM-Pruner On the Structural Pruning of Large Lan.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/GX3SH3NB/2305.html:text/html},
}


@unpublished{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	doi = {10.48550/arXiv.1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv. \url{http://arxiv.org/abs/1503.02531}},
	keywords = {Background, compression, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, LLM, Statistics - Machine Learning},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/S38Z4JPW/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/JDKYW9IV/1503.html:text/html},
}


@article{zhu_teach_nodate,
	title = {Teach {Less}, {Learn} {More}: {On} the {Undistillable} {Classes} in {Knowledge} {Distillation}},
	abstract = {Knowledge distillation (KD) can effectively compress neural networks by training a smaller network (student) to simulate the behavior of a larger one (teacher). A counter-intuitive observation is that a more expansive teacher does not make a better student, but the reasons for this phenomenon remain unclear. In this paper, we demonstrate that this is directly attributed to the presence of undistillable classes: when trained with distillation, the teacher’s knowledge of some classes is incomprehensible to the student model. We observe that while KD improves the overall accuracy, it is at the cost of the model becoming inaccurate in these undistillable classes. After establishing their widespread existence in state-ofthe-art distillation methods, we illustrate their correlation with the capacity gap between teacher and student models. Finally, we present a simple “Teach Less Learn More” (TLLM) framework to identify and discard the undistillable classes during training. We validate the effectiveness of our approach on multiple datasets with varying network architectures. In all settings, our proposed method is able to exceed the performance of competitive state-of-the-art techniques.},
	language = {en},
	author = {Zhu, Yichen and Liu, Ning and Xu, Zhiyuan and Liu, Xin and Meng, Weibing and Wang, Yi},
	keywords = {Background, compression, LLM},
	file = {Zhu et al. - Teach Less, Learn More On the Undistillable Class.pdf:/Users/lenert/Zotero/storage/Y5D8CS6Y/Zhu et al. - Teach Less, Learn More On the Undistillable Class.pdf:application/pdf},
	year = {2022}
}



@unpublished{gu_knowledge_2023,
	title = {Knowledge {Distillation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2306.08543},
	doi = {10.48550/arXiv.2306.08543},
	abstract = {Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge from white-box generative LLMs is still under-explored, which becomes more and more important with the prosperity of LLMs. In this work, we propose MiniLLM that distills smaller language models from generative larger language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. Extensive experiments in the instruction-following setting show that the MiniLLM models generate more precise responses with the higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance. Our method is also scalable for different model families with 120M to 13B parameters. We will release our code and model checkpoints at https://aka.ms/MiniLLM.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
	month = jun,
	year = {2023},
	note = {arXiv. \url{http://arxiv.org/abs/2306.08543}},
	keywords = {Background, compression, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, LLM},
	annote = {Comment: 20 pages, 12 figures},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/32BTGCA9/Gu et al. - 2023 - Knowledge Distillation of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/AUR3UWX6/2306.html:text/html},
}


@unpublished{huang_-context_2022,
	title = {In-context {Learning} {Distillation}: {Transferring} {Few}-shot {Learning} {Ability} of {Pre}-trained {Language} {Models}},
	shorttitle = {In-context {Learning} {Distillation}},
	url = {http://arxiv.org/abs/2212.10670},
	doi = {10.48550/arXiv.2212.10670},
	abstract = {Given the success with in-context learning of large pre-trained language models, we introduce in-context learning distillation to transfer in-context few-shot learning ability from large models to smaller models. We propose to combine in-context learning objectives with language modeling objectives to distill both the ability to read in-context examples and task knowledge to the smaller models. We perform in-context learning distillation under two different few-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and Multitask In-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitask few-shot learning but also requires more computation than Meta-ICT. Our method shows consistent improvements for both Meta-ICT and Multitask-ICT on two benchmarks: LAMA and CrossFit. Our extensive experiments and analysis reveal that in-context learning objectives and language modeling objectives are complementary under the Multitask-ICT paradigm. In-context learning objectives achieve the best performance when combined with language modeling objectives.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Huang, Yukun and Chen, Yanda and Yu, Zhou and McKeown, Kathleen},
	month = dec,
	year = {2022},
	note = {arXiv. \url{http://arxiv.org/abs/2212.10670}},
	keywords = {Background, compression, Computer Science - Computation and Language, Computer Science - Machine Learning, LLM},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/7QJDGN4J/Huang et al. - 2022 - In-context Learning Distillation Transferring Few.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/H9ZJYS6N/2212.html:text/html},
}


@unpublished{li_explanations_2022,
	title = {Explanations from {Large} {Language} {Models} {Make} {Small} {Reasoners} {Better}},
	url = {http://arxiv.org/abs/2210.06726},
	doi = {10.48550/arXiv.2210.06726},
	abstract = {Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5\% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Li, Shiyang and Chen, Jianshu and Shen, Yelong and Chen, Zhiyu and Zhang, Xinlu and Li, Zekun and Wang, Hong and Qian, Jing and Peng, Baolin and Mao, Yi and Chen, Wenhu and Yan, Xifeng},
	month = oct,
	year = {2022},
	note = {arXiv. \url{http://arxiv.org/abs/2210.06726}},
	keywords = {Background, compression, Computer Science - Computation and Language, LLM},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/9ELILQFW/Li et al. - 2022 - Explanations from Large Language Models Make Small.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/5I8W66V2/2210.html:text/html},
}

@inproceedings{jiang_lion_2023,
  title={Lion: Adversarial Distillation of Proprietary Large Language Models},
  author={Yuxin Jiang and Chunkit Chan and Mingyang Chen and Wei Wang},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258833333}
}

@unpublished{frantar_gptq_2023,
	title = {{GPTQ}: {Accurate} {Post}-{Training} {Quantization} for {Generative} {Pre}-trained {Transformers}},
	shorttitle = {{GPTQ}},
	url = {http://arxiv.org/abs/2210.17323},
	doi = {10.48550/arXiv.2210.17323},
	abstract = {Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
	month = mar,
	year = {2023},
	note = {arXiv. \url{http://arxiv.org/abs/2210.17323}},
	keywords = {Background, compression, Computer Science - Machine Learning, LLM},
	annote = {Comment: ICLR 2023},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/B8AEBAXP/Frantar et al. - 2023 - GPTQ Accurate Post-Training Quantization for Gene.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/6V53VLXP/2210.html:text/html},
}

@inbook{gholami_survey_2021,
	author = {Gholami, Asghar and Kim, Sehoon and Zhen, Dong and Yao, Zhewei and Mahoney, Michael and Keutzer, Kurt},
	year = {2022},
	month = {01},
	pages = {291-326},
	title = {A Survey of Quantization Methods for Efficient Neural Network Inference},
	isbn = {9781003162810},
	doi = {10.1201/9781003162810-13}
}

@misc{noauthor_quantize_nodate,
    author = {Huggingface},
    title = {Quantize Transformers models. \url{https://huggingface.co/docs/transformers/main_classes/quantization}},
    year = {2023},
    note = {Accessed: 2023-10-02},
    keywords = {Background, compression, LLM},
}


@unpublished{liu_llm-qat_2023,
	title = {{LLM}-{QAT}: {Data}-{Free} {Quantization} {Aware} {Training} for {Large} {Language} {Models}},
	shorttitle = {{LLM}-{QAT}},
	url = {http://arxiv.org/abs/2305.17888},
	doi = {10.48550/arXiv.2305.17888},
	abstract = {Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
	month = may,
	year = {2023},
	note = {arXiv. \url{http://arxiv.org/abs/2305.17888}},
	keywords = {Background, compression, Computer Science - Computation and Language, LLM},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/EDYY565R/Liu et al. - 2023 - LLM-QAT Data-Free Quantization Aware Training for.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/J6A3DWHH/2305.html:text/html},
}

@inproceedings{frantar_optimal_2023,
  author       = {Elias Frantar and
                  Dan Alistarh},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Optimal Brain Compression: {A} Framework for Accurate Post-Training
                  Quantization and Pruning},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/1caf09c9f4e6b0150b06a07e77f2710c-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:30 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/FrantarA22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{william_autogptq_2023,
    author = {AutoGPTQ},
    title = {AutoGPTQ. \url{https://github.com/PanQiWei/AutoGPTQ}},
    year = {2023},
    note = {Original date: 2023-04-13. Accessed: 2023-10-02},
    keywords = {Background, compression, deep-learning, inference, large-language-models, LLM, llms, nlp, pytorch, quantization, transformer, transformers},
}

@misc{noauthor_generative_nodate,
    author = {GCS},
    title = {Generative AI applications with Vertex AI PaLM 2 Models and LangChain. \url{https://cloud.google.com/blog/products/ai-machine-learning/generative-ai-applications-with-vertex-ai-palm-2-models-and-langchain}},
    year = {2023},
    note = {Accessed: 2023-10-03},
    keywords = {Main},
}

@misc{noauthor_quickly_2023,
    author = {AWS},
    title = {Quickly build high-accuracy Generative AI applications on enterprise data using Amazon Kendra, LangChain, and large language models | AWS Machine Learning Blog. \url{https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/}},
    year = {2023},
    note = {Section: Amazon Kendra. Accessed: 2023-10-03},
    keywords = {Main},
}


@article{izacard_atlas_2022,
	title = {Atlas: {Few}-shot {Learning} with {Retrieval} {Augmented} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Atlas},
	url = {https://arxiv.org/abs/2208.03299},
	doi = {10.48550/ARXIV.2208.03299},
	abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42\% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3\% despite having 50x fewer parameters.},
	urldate = {2023-10-03},
	author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Background, Computation and Language (cs.CL), FOS: Computer and information sciences, LLM, RAG, Reader, Retriever},
	annote = {[TLDR] This work presents Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples, and studies the impact of the content of the document index, showing that it can easily be updated.},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/NIUYI8CR/Izacard et al. - 2022 - Atlas Few-shot Learning with Retrieval Augmented .pdf:application/pdf},
}

@inproceedings{papineni_bleu_2002,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2023-10-06},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	month = jul,
	year = {2002},
	keywords = {Evaluation, metric},
	pages = {311--318},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/U8I5KG6Z/Papineni et al. - 2002 - Bleu a Method for Automatic Evaluation of Machine.pdf:application/pdf},
}

@inproceedings{lin_rouge_2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013},
	urldate = {2023-10-06},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	keywords = {Evaluation, metric},
	pages = {74--81},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/697S6A96/Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summa.pdf:application/pdf},
}

@inproceedings{kamalloo_evaluating_2023,
  author       = {Ehsan Kamalloo and
                  Nouha Dziri and
                  Charles L. A. Clarke and
                  Davood Rafiei},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {Evaluating Open-Domain Question Answering in the Era of Large Language
                  Models},
  booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada,
                  July 9-14, 2023},
  pages        = {5591--5606},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.acl-long.307},
  doi          = {10.18653/V1/2023.ACL-LONG.307},
  timestamp    = {Thu, 10 Aug 2023 12:36:03 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/KamallooDCR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li_ditch_2022,
    title = "Ditch the Gold Standard: Re-evaluating Conversational Question Answering",
    author = "Li, Huihan  and
      Gao, Tianyu  and
      Goenka, Manan  and
      Chen, Danqi",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.555",
    doi = "10.18653/v1/2022.acl-long.555",
    pages = "8074--8085",
    abstract = "Conversational question answering aims to provide natural-language answers to users in information-seeking conversations. Existing conversational QA benchmarks compare models with pre-collected human-human conversations, using ground-truth answers provided in conversational history. It remains unclear whether we can rely on this static evaluation for model development and whether current systems can well generalize to real-world human-machine conversations. In this work, we conduct the first large-scale human evaluation of state-of-the-art conversational QA systems, where human evaluators converse with models and judge the correctness of their answers. We find that the distribution of human machine conversations differs drastically from that of human-human conversations, and there is a disagreement between human and gold-history evaluation in terms of model ranking. We further investigate how to improve automatic evaluations, and propose a question rewriting mechanism based on predicted history, which better correlates with human judgments. Finally, we analyze the impact of various modeling strategies and discuss future directions towards building better conversational question answering systems.",
}

@inproceedings{zhang_bertscore_2020,
  author       = {Tianyi Zhang and
                  Varsha Kishore and
                  Felix Wu and
                  Kilian Q. Weinberger and
                  Yoav Artzi},
  title        = {BERTScore: Evaluating Text Generation with {BERT}},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=SkeHuCVFDr},
  timestamp    = {Wed, 03 Jun 2020 10:08:32 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ZhangKWWA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{cohen_coefficient_1960,
	title = {A {Coefficient} of {Agreement} for {Nominal} {Scales}},
	volume = {20},
	issn = {0013-1644, 1552-3888},
	url = {http://journals.sagepub.com/doi/10.1177/001316446002000104},
	doi = {10.1177/001316446002000104},
	abstract = {CONSIDER Table 1. It represents in its formal characteristics a situation which arises in the clinical-social-personality areas of psychology, where it frequently occurs that the only useful level of measurement obtainable is nominal scaling (Stevens, 1951, pp. 2526), i.e. placement in a set of k unordered categories. Because the categorizing of the units is a consequence of some complex judgment process performed by a \&dquo;two-legged meter\&dquo; (Stevens, 1958), it becomes important to determine the extent to which these judgments are reproducible, i.e., reliable. The procedure which suggests itself is that of having two (or more) judges independently categorize a sample of units and determine the degree, significance, and},
	language = {en},
	number = {1},
	urldate = {2023-10-07},
	journal = {Educational and Psychological Measurement},
	author = {Cohen, Jacob},
	month = apr,
	year = {1960},
	keywords = {Evaluation, metric},
	pages = {37--46},
}
	
@article{bajaj2016ms,
title={Ms marco: A human generated machine reading comprehension dataset},
author={Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
journal={arXiv preprint arXiv:1611.09268},
year={2016}
}

@misc{pluster_leolm_2023,
    author = {Plüster, Björn},
    title = {LeoLM: Ein Impuls für Deutschsprachige LLM-Forschung | LAION. \url{https://laion.ai/blog-de/leo-lm/}},
    year = {2023},
    note = {Accessed: 2023-10-17},
    keywords = {Evaluation, german, LLM, Reader, Retrieval},
}


@unpublished{muennighoff_sgpt_2022,
	title = {{SGPT}: {GPT} {Sentence} {Embeddings} for {Semantic} {Search}},
	shorttitle = {{SGPT}},
	url = {http://arxiv.org/abs/2202.08904},
	doi = {10.48550/arXiv.2202.08904},
	abstract = {Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7\% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.},
	urldate = {2023-09-27},
	publisher = {arXiv},
	author = {Muennighoff, Niklas},
	month = aug,
	year = {2022},
	note = {arXiv. \url{http://arxiv.org/abs/2202.08904}},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Retrieval, LLM, OOD, Main},
	annote = {Comment: 19 pages, 3 figures, 12 tables. v2 corrects a misreported nDCG@10 number for the SGPT-BE-5.8B model. v3 updates SGPT-BE-5.8B scores based on retrained models with larger batch sizes v4 removes a superfluous table. v5 adds OpenAI scores on USEB and makes the paper easier to read},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/RTAZ3T3I/Muennighoff - 2022 - SGPT GPT Sentence Embeddings for Semantic Search.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/WYMQ6JBQ/2202.html:text/html},
}

@inproceedings{ouyang_training_2022,
  author       = {Long Ouyang and
                  Jeffrey Wu and
                  Xu Jiang and
                  Diogo Almeida and
                  Carroll L. Wainwright and
                  Pamela Mishkin and
                  Chong Zhang and
                  Sandhini Agarwal and
                  Katarina Slama and
                  Alex Ray and
                  John Schulman and
                  Jacob Hilton and
                  Fraser Kelton and
                  Luke Miller and
                  Maddie Simens and
                  Amanda Askell and
                  Peter Welinder and
                  Paul F. Christiano and
                  Jan Leike and
                  Ryan Lowe},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Training language models to follow instructions with human feedback},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:36 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Ouyang0JAWMZASR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{pluster_leolm_nodate,
	title = {{LeoLM}: {Ein} {Impuls} für {Deutschsprachige} {LLM}-{Forschung} {\textbar} {LAION}},
	shorttitle = {{LeoLM}},
	url = {https://laion.ai/blog-de/leo-lm},
	abstract = {{\textless}p{\textgreater}Lernen Sie LeoLM kennen, das erste offen und kommerziell verfügbare deutsche Foundation Language Model, das auf Llama-2 basiert.
Unsere Modelle erweitern ...},
	language = {en},
	urldate = {2023-12-19},
	year = {2023},
	author = {Plüster, Björn},
	keywords = {Evaluation, German, LLM},
	file = {Snapshot:/Users/lenert/Zotero/storage/R4TYHLK6/leo-lm.html:text/html},
}


@article{guo_abg-coqa_nodate,
	title = {Abg-{CoQA}: {Clarifying} {Ambiguity} in {Conversational} {Question} {Answering}},
	abstract = {Eﬀective communication requires the ability to identify ambiguities and request clariﬁcation of utterances. For machines to engage in a conversation, they need to learn to generate diﬀerent forms of clariﬁcation questions. This paper aims at studying the extent to which the state of the art neural generation models can generate eﬀective clariﬁcation questions in conversational question answering. We introduce Abg-CoQA, a novel crowdsourced dataset for clarifying ambiguities in conversational question answering systems. Our dataset contains 8,615 questions with answers where 994 questions are ambiguous. The conversational questions are about 3,968 text passages from ﬁve diverse domains which are pre-selected from the CoQA dataset. For ambiguous turns, we have collected the clariﬁcation questions and their answers. We evaluate strong language generation models and conversational question answering models on Abg-CoQA. The best-performing system achieves a F1-score of 23.6\% on ambiguity detection; an accuracy of 56.0\% on generating clariﬁcation question in human evaluation; and a F1 score of 40.1\% on question answering after clariﬁcation, which is 35.1 points behind human performance (75.2\%), indicating there is ample room for improvement.},
	language = {en},
	author = {Guo, Meiqi and Zhang, Mingda and Reddy, Siva and Alikhani, Malihe},
	file = {Guo et al. - Abg-CoQA Clarifying Ambiguity in Conversational Q.pdf:/Users/lenert/Zotero/storage/T8DG4W36/Guo et al. - Abg-CoQA Clarifying Ambiguity in Conversational Q.pdf:application/pdf},
	year= {2021}
}

@unpublished{kuhn_clam_2023,
	title = {{CLAM}: {Selective} {Clarification} for {Ambiguous} {Questions} with {Generative} {Language} {Models}},
	shorttitle = {{CLAM}},
	url = {http://arxiv.org/abs/2212.07769},
	abstract = {Users often ask dialogue systems ambiguous questions that require clarification. We show that current language models rarely ask users to clarify ambiguous questions and instead provide incorrect answers. To address this, we introduce CLAM: a framework for getting language models to selectively ask for clarification about ambiguous user questions. In particular, we show that we can prompt language models to detect whether a given question is ambiguous, generate an appropriate clarifying question to ask the user, and give a final answer after receiving clarification. We also show that we can simulate users by providing language models with privileged information. This lets us automatically evaluate multi-turn clarification dialogues. Finally, CLAM significantly improves language models' accuracy on mixed ambiguous and unambiguous questions relative to SotA.},
	urldate = {2023-12-28},
	publisher = {arXiv},
	author = {Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
	month = feb,
	year = {2023},
	note = {arXiv. \url{http://arxiv.org/abs/2212.07769}},
	keywords = {basics, clarification, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/lenert/Zotero/storage/39K9S6FY/2212.html:text/html;Full Text PDF:/Users/lenert/Zotero/storage/TL7F7GA9/Kuhn et al. - 2023 - CLAM Selective Clarification for Ambiguous Questi.pdf:application/pdf},
}

@inproceedings{larsson_issue-based_2002,
	title = {Issue-based {Dialogue} {Management}},
	url = {https://www.semanticscholar.org/paper/Issue-based-Dialogue-Management-Larsson/975e3dfccc0cf203565ea7fb370428743c1347a6},
	abstract = {Semantic Scholar extracted view of "Issue-based Dialogue Management" by Staffan Larsson},
	urldate = {2023-12-28},
	author = {Larsson, Staffan},
	year = {2002},
	keywords = {basics, clarification},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/TT8TR3ME/Larsson - 2002 - Issue-based Dialogue Management.pdf:application/pdf},
}

@inproceedings{zamani_analyzing_2020,
	author = {Zamani, Hamed and Mitra, Bhaskar and Chen, Everest and Lueck, Gord and Diaz, Fernando and Bennett, Paul N. and Craswell, Nick and Dumais, Susan T.},
	title = {Analyzing and Learning from User Interactions for Search Clarification},
	year = {2020},
	isbn = {9781450380164},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3397271.3401160},
	doi = {10.1145/3397271.3401160},
	abstract = {Asking clarifying questions in response to search queries has been recognized as a useful technique for revealing the underlying intent of the query. Clarification has applications in retrieval systems with different interfaces, from the traditional web search interfaces to the limited bandwidth interfaces as in speech-only and small screen devices. Generation and evaluation of clarifying questions have been recently studied in the literature. However, user interaction with clarifying questions is relatively unexplored. In this paper, we conduct a comprehensive study by analyzing large-scale user interactions with clarifying questions in a major web search engine. In more detail, we analyze the user engagements received by clarifying questions based on different properties of search queries, clarifying questions, and their candidate answers. We further study click bias in the data, and show that even though reading clarifying questions and candidate answers does not take significant efforts, there still exist some position and presentation biases in the data. We also propose a model for learning representation for clarifying questions based on the user interaction data as implicit feedback. The model is used for re-ranking a number of automatically generated clarifying questions for a given query. Evaluation on both click data and human labeled data demonstrates the high quality of the proposed method.},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {1181–1190},
	numpages = {10},
	keywords = {clarifying questions, conversational search, mixed-initiative interaction, query intent disambiguation, web search},
	location = {Virtual Event, China},
	series = {SIGIR '20}
}
@article{tavakoli_analyzing_2021,
	title = {Analyzing clarification in asynchronous information‐seeking conversations},
	volume = {73},
	doi = {10.1002/asi.24562},
	abstract = {This research analyzes human‐generated clarification questions to provide insights into how they are used to disambiguate and provide a better understanding of information needs. A set of clarification questions is extracted from posts on the Stack Exchange platform. Novel taxonomy is defined for the annotation of the questions and their responses. We investigate the clarification questions in terms of whether they add any information to the post (the initial question posted by the asker) and the accepted answer, which is the answer chosen by the asker. After identifying, which clarification questions are more useful, we investigated the characteristics of these questions in terms of their types and patterns. Non‐useful clarification questions are identified, and their patterns are compared with useful clarifications. Our analysis indicates that the most useful clarification questions have similar patterns, regardless of topic. This research contributes to an understanding of clarification in conversations and can provide insight for clarification dialogues in conversational search scenarios and for the possible system generation of clarification requests in information‐seeking conversations.},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Tavakoli, Leila and Zamani, Hamed and Scholer, Falk and Croft, William and Sanderson, Mark},
	month = aug,
	year = {2021},
	keywords = {basics, clarification},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/8S8YMAIA/Tavakoli et al. - 2021 - Analyzing clarification in asynchronous informatio.pdf:application/pdf},
}


@unpublished{gao_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	abstract = {Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces the metrics and benchmarks for assessing RAG models, along with the most up-to-date evaluation framework. In conclusion, the paper delineates prospective avenues for research, including the identification of challenges, the expansion of multi-modalities, and the progression of the RAG infrastructure and its ecosystem.},
	urldate = {2024-01-11},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Guo, Qianyu and Wang, Meng and Wang, Haofen},
	month = jan,
	year = {2024},
	note = {arXiv. \url{http://arxiv.org/abs/2312.10997}},
	keywords = {basicallymeinemasterthesis, Basics, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Evaluation, Fazit, Main, RAG},
	annote = {Comment: Ongoing Work},
	file = {arXiv.org Snapshot:/Users/lenert/Zotero/storage/JVPIZM9F/2312.html:text/html;Full Text PDF:/Users/lenert/Zotero/storage/G8GV4FMB/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language .pdf:application/pdf},
}

@misc{noauthor_truelens,
    author = {TruLens},
    title = {RAG Triad - TruLens. \url{https://www.trulens.org/trulens_eval/core_concepts_rag_triad/}},
    year = {2024},
    note = {Accessed: 2024-01-11},
    keywords = {Evaluation, RAG},
}

@misc{noauthor_ragas,
    author = {Ragas},
    title = {Core Concepts | Ragas. \url{https://docs.ragas.io/en/stable/concepts/index.html}},
    year = {2024},
    note = {Accessed: 2024-01-11},
    keywords = {Evaluation, RAG},
}

@article{liu_lost_2023,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
  journal={Transactions of the Association for Computational Linguistics},
  year={2023},
  volume={12},
  pages={157-173},
  url={https://api.semanticscholar.org/CorpusID:259360665}
}


@unpublished{thoppilan_lamda_2022,
	title = {{LaMDA}: {Language} {Models} for {Dialog} {Applications}},
	shorttitle = {{LaMDA}},
	url = {http://arxiv.org/abs/2201.08239},
	doi = {10.48550/arXiv.2201.08239},
	abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
	urldate = {2024-02-07},
	publisher = {arXiv},
	author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
	month = feb,
	year = {2022},
	note = {arXiv. \url{http://arxiv.org/abs/2201.08239}},
	keywords = {background, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, LLM, related work},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/EYPUIG5Q/Thoppilan et al. - 2022 - LaMDA Language Models for Dialog Applications.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/BBQWL4F5/2201.html:text/html},
}


@unpublished{naveed_comprehensive_2023,
	title = {A {Comprehensive} {Overview} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.06435},
	doi = {10.48550/arXiv.2307.06435},
	abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.},
	urldate = {2024-02-07},
	publisher = {arXiv},
	author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
	month = dec,
	year = {2023},
	note = {arXiv. \url{http://arxiv.org/abs/2307.06435}},
	keywords = {Background, Computer Science - Computation and Language, LLM, related work},
	annote = {Comment: Work in-progress},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/HPX6CVJF/Naveed et al. - 2023 - A Comprehensive Overview of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/6XIEJ89Z/2307.html:text/html},
}

@unpublished{nakano_webgpt_2022,
	title = {{WebGPT}: {Browser}-assisted question-answering with human feedback},
	shorttitle = {{WebGPT}},
	url = {http://arxiv.org/abs/2112.09332},
	doi = {10.48550/arXiv.2112.09332},
	abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
	urldate = {2024-02-07},
	publisher = {arXiv},
	author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
	month = jun,
	year = {2022},
	note = {arXiv. \url{http://arxiv.org/abs/2112.09332}},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, related work},
	annote = {Comment: 32 pages},
	file = {arXiv Fulltext PDF:/Users/lenert/Zotero/storage/G8UCUU2X/Nakano et al. - 2022 - WebGPT Browser-assisted question-answering with h.pdf:application/pdf;arXiv.org Snapshot:/Users/lenert/Zotero/storage/DDDWX75K/2112.html:text/html},
}

@inproceedings{feng_dialdoc_2021,
	address = {Online},
	title = {{DialDoc} 2021 {Shared} {Task}: {Goal}-{Oriented} {Document}-grounded {Dialogue} {Modeling}},
	shorttitle = {{DialDoc} 2021 {Shared} {Task}},
	url = {https://aclanthology.org/2021.dialdoc-1.1},
	doi = {10.18653/v1/2021.dialdoc-1.1},
	abstract = {We present the results of Shared Task at Workshop DialDoc 2021 that is focused on document-grounded dialogue and conversational question answering. The primary goal of this Shared Task is to build goal-oriented information-seeking conversation systems that can identify the most relevant knowledge in the associated document for generating agent responses in natural language. It includes two subtasks on predicting agent responses: the first subtask is to predict the grounding text span in the given document for next agent response; the second subtask is to generate agent response in natural language given the context. Many submissions outperform baseline significantly. For the first task, the best-performing system achieved 67.1 Exact Match and 76.3 F1. For the second subtask, the best system achieved 41.1 SacreBLEU and highest rank by human evaluation.},
	urldate = {2023-09-04},
	booktitle = {Proceedings of the 1st {Workshop} on {Document}-grounded {Dialogue} and {Conversational} {Question} {Answering} ({DialDoc} 2021)},
	publisher = {Association for Computational Linguistics},
	author = {Feng, Song},
	month = aug,
	year = {2021},
	keywords = {Background, pdfs, related work, CQA},
	pages = {1--7},
	file = {Full Text PDF:/Users/lenert/Zotero/storage/XHK7GAYM/Feng - 2021 - DialDoc 2021 Shared Task Goal-Oriented Document-g.pdf:application/pdf},
}

@misc{demandsage2022chatgpt,
    author = {Rohit Shewale},
    title = {ChatGPT Statistics. \url{https://www.demandsage.com/chatgpt-statistics/}},
    year = {2024},
    note = {Accessed: 2024-02-20},
}

@article{marcus_next_2020,
       author = {{Marcus}, Gary},
        title = "{The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2, I.2.6},
         year = 2020,
        month = feb,
          eid = {arXiv:2002.06177},
        pages = {arXiv:2002.06177},
          doi = {10.48550/arXiv.2002.06177},
archivePrefix = {arXiv},
       eprint = {2002.06177},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200206177M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


