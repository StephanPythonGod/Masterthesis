This chapter provides essential background information and reviews relevant prior research. It commences with an introduction to the sub-task of \gls{qa}, as presented in Section \ref*{sec:qa}. As mentioned in Chapter \ref{chap:intro}, this chapter maintains a clear distinction between \gls{qa} and \gls{convqa}. Consequently, Section \ref{sec:cqa} extends upon the foundational knowledge of \gls*{qa} and introduces the requisite concepts for the transformation of a QA-System into a \gls{convqa}-System. Section \ref{sec:efficient_llm} elaborates on different approaches towards the application of \gls{llm}s in resource-constrained settings. Section \ref{sec:related_work} delves into the related work, providing a comprehensive overview of the current state-of-the-art in the field of \gls{qa} and \gls{convqa} over textual knowledge sources. The goal of this chapter is to provide a holistic foundation of the research field of \gls{qa} and \gls{convqa}. Not all mentioned concepts and methods will further be utilized in Chapters \ref{chap:main} and \ref{chap:eval} but are a necessary part of the research to understand the advantages and limitations of modern approaches and system design decisions. The introductions to certain sections will elaborate on the relevance of the respective topics for the research of this thesis.

\section{Question Answering}
\label{sec:qa}

In Section \ref{subsec:qa_basics} we will lay the groundwork by introducing the fundamental aspects of \gls{qa}-Systems and the techniques used to differentiate and categorize them. Following that, subsequent sections will delve deeper into the examination of specific system components.

The concepts and taxonomy of \gls{qa} systems mentioned in Section \ref{subsec:qa_basics} are essential background knowledge. While Section \ref{subsec:qa_architectures} outlines different conceptual approaches towards the holistic architecture of \gls{qa}-systems, this thesis will later focus solely on \gls{rag}s. The following sections, \ref{subsec:qa_indexing}, \ref{subsec:qa_retrieval}, and \ref{subsec:qa_reader}, will give a holistic image of possible approaches towards these components, which is necessary to understand on a higher level what possible system designs entail. However, the thesis will mainly focus on zero-shot retrievers, their optimization, and generative readers. The other mentioned methods can be viewed as possible alternatives and historical developments that should not be forgotten in the research process. The decision to include this information was motivated by the dynamic nature of the research field of \gls{qa}. While certain methods may have gained popularity in recent months, it's essential to recognize that the landscape of QA is continually evolving. Just because a particular method has gained traction does not render other approaches obsolete. By acknowledging the ongoing development and diversity within the field, we aim to provide a comprehensive understanding and context in this thesis.


\subsection{Basics}
\label{subsec:qa_basics}

Jurafsky and Martin define a \gls{qa}-System as a system \enquote{designed to satisfy human information needs} \cite{jurafsky_speech_2023}. Hence, it primarily functions as an Information Retrieval System, with its primary objective being to provide users with the desired and accurate information in response to natural language requests.

The research community has yet to establish a universally accepted classification framework for \gls{qa} systems. For instance, Hao et al. and Farea et al. \cite{hao_recent_2022, farea_evaluation_2022} take a comprehensive approach to classify QA systems but differ in certain aspects, such as their treatment of question types and knowledge sources. On the other hand, other researchers \cite{zhu_retrieving_2021, jurafsky_speech_2023, etezadi_state_2023, zhang_survey_2023} employ a similar classification methodology but often focus solely on retrieval-based approaches, thereby lacking a holistic perspective.

The classification proposed by Farea et al. \cite{farea_evaluation_2022} goes a step further by distinguishing between the \textbf{QA-Framework} and \textbf{QA-Paradigms}, enhancing its versatility for comparing classical and modern QA systems. An adaptation of this classification will be utilized in this thesis. The originally proposed QA algorithms have been extended to include the Retrieval-based approach, and the Question Types have been revised based on the typology introduced by Mishra et al. in their 2016 survey \cite{mishra_survey_2016}, which was further elaborated upon by Etezadi et al. \cite{etezadi_state_2023}. Also the answer types were adjusted to align with the classifications used in \cite{mcdonald_detect_2022,dasigi_dataset_2021}. In this context, a crucial distinction is made between a \textbf{QA} and \textbf{ConvQA} system, guided by the criteria outlined in \cite{zamani_conversational_2023}: a \textbf{QA} system exclusively handles standalone questions, while any inquiry exceeding a single question and involving conversational context falls within the domain of a \textbf{ConvQA} system.

The \textbf{\gls{qa}-Framework} encompasses external factors such as Question and Answer Types, while also considering system-related factors like the \gls{qa} Algorithm and Knowledge Source \cite{farea_evaluation_2022, hao_recent_2022}. Conversely, the \textbf{\gls{qa}-Paradigm} defines the fundamental underlying concept of a system and can be seen as a subset of possible combinations within the \textbf{\gls{qa} Framework}. Currently, three dominant paradigms prevail:

\begin{enumerate}
    \item \textbf{Information Retrieval (IR)-Based QA}: This paradigm involves searching through extensive multi-modal data based on a user's question and using the retrieved passages to generate an answer.
    
    \item \textbf{Knowledge Base (KB) QA}: In this approach, a semantic representation of the question is constructed, and a knowledge base is queried using this representation. The returned results are then used to generate an answer.
    
    \item \textbf{Generative Question Answering}: Here, knowledge is fully implicit, and a \gls{nn} generates answers based on its trained parameters.

\end{enumerate}

For visual clarity, a diagram illustrating the adjusted \gls{qa} Framework Classification by Farea et al. is provided in Figure \ref{fig:qa_classification}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Grafiken/QA_Framework.png}
    \caption{Adjusted QA Framework Classification by Farea et al. \cite{farea_evaluation_2022}}
    \label{fig:qa_classification}
\end{figure}


Figure \ref{fig:qa_classification} illustrates the aforementioned classification. The primary distinguishing factor is the employed \textbf{\gls{qa} Algorithm}. Rule-based approaches involve the manual crafting of feature extractions from user questions, which are then compared to the knowledge base. Rule-based approaches are typically employed in closed-domain \gls{qa} systems exclusively \cite{etezadi_state_2023}.

Retrieval-based approaches are the classic Information Retrieval (IR)-based \gls{qa} systems, comprising two key components: an intent classifier and a retriever. The intent classifier's objective is to discern the question's intent and identify important entities. Subsequently, the retriever searches the knowledge source and identifies the most relevant passages \cite{farea_evaluation_2022, zhu_retrieving_2021}.

The Neural-based approach, often referred to as the generative approach, utilizes a Sequence-to-Sequence (S2S) model to generate accurate answers to given questions. In this paradigm, the information is stored directly in the neural network's parameters, otherwise, the neural network is part of a Retrieval-based approach. Most datasets in these contexts consist of triples of question, context, and answer pairs \cite{jurafsky_speech_2023}. Notably, widely used datasets such as SQuAD and QASPER originally emerged from the field of machine reading comprehension, representing a foundational step in the evolution of \gls{qa} systems \cite{rajpurkar_squad_2016, dasigi_dataset_2021, zhu_retrieving_2021}.

In addition to the \textbf{\gls{qa} Algorithms}, the \textbf{Knowledge Source} plays a pivotal role in distinguishing various aspects of Question Answering (QA) systems. The nature of the knowledge source can range from structured to unstructured or semi-structured, and it may encompass diverse data modalities, including text, audio, and video. A common point of comparison in the QA landscape is between closed and open-domain systems.

In the broad sense, a \textbf{closed-domain} QA system operates within the confines of a specific knowledge domain, which means it has limited access to information. In contrast, \textbf{open-domain} QA systems grapple with an extensive array of knowledge sources, necessitating a more versatile approach \cite{farea_evaluation_2022}.

Furthermore, a closed-domain setup often entails limitations on the types of questions it can handle, primarily focusing on factoid questions or predefined templates. Additionally, it frequently relies on structured knowledge bases like graphs or logically organized repositories \cite{hao_recent_2022}.

Conversely, open-domain QA systems are designed to tackle a wide spectrum of user queries, ranging from factoids to more complex inquiries. They typically deal with unstructured knowledge sources, which can be substantial and diverse in content \cite{zhu_retrieving_2021, farea_evaluation_2022, jurafsky_speech_2023}.

An alternative perspective for distinguishing \gls{qa}-Systems lies in the \textbf{Question Types} that users can input into the system. Questions can fall into various categories, such as \textit{factoid, list, casual, confirmation, hypothetical} \cite{mishra_survey_2016}, or \textit{complex}~\cite{etezadi_state_2023}.\nopagebreak

\begin{itemize}
   \item \textit{Factoid questions}, the most common type, is typically signaled by question words (what, when, which, who, how) and yields a concise factual answer.
   
   \item \textit{List questions} represent a specialized subset of factoid questions, where the answer comprises a list of facts.
   
   \item \textit{Casual questions} encompass inquiries that deviate from the factoid format, often involving words like \textit{how} or \textit{why} and requiring more advanced reasoning.
   
   \item \textit{Confirmation questions} seek simple yes or no responses, frequently employed in personal assistant applications.
   
   \item \textit{Hypothetical questions} delve into hypothetical scenarios (e.g., "what would happen if"), aiming for plausible rather than definitive answers.
   
   \item \textit{Complex questions} can be further categorized into \textit{answer-retrieval-complex} and \textit{question-understanding-complex}. In the case of question-understanding-complex questions, the complexity arises from nuances like multiple constraints, making the question itself intricate to comprehend. In contrast, answer-retrieval-complex questions involve complexities in finding the correct answer, often requiring the combination of information from multiple documents or similar sources. This is commonly referred to as long-form \gls{qa}.
\end{itemize}

Lastly, a \gls{qa}-System can be characterized by the \textbf{Answer Types} it offers, a concept closely intertwined with Question Types. Farea et al. \cite{farea_evaluation_2022} delineate four categories of answers: \textit{extractive, abstractive, boolean} and \textit{reactive}. 

\begin{itemize}
   \item \textit{Extractive answers} represent the most common type, where the answer is a specific factual excerpt presented as a span of tokens.

   \item \textit{Abstractive answers} typically correspond to complex questions that necessitate the system to consider multiple documents and information sources to formulate a response. In such cases, no predefined or annotated answer exists.

   \item \textit{Boolean answers} are typically the result of confirmation questions, where the answer is either \textit{yes} or \textit{no}.

   \item \textit{Reactive answers} often arise in response to confirmation questions and can be a system-generated reaction based on the user's provided answer.
\end{itemize}

% Pushed 1st

\subsection{Information Retrieval Architectures}
\label{subsec:qa_architectures}

As stated in the previous section, there are three major paradigms in \gls{qa}: \gls{ir}-based \gls{qa}, \gls{kb}-based QA, and Generative \gls{qa}. This section will primarily concentrate on the first paradigm, \gls{ir}-based QA, as it holds the most promise for addressing the objectives of this thesis topic.

This thesis will not focus on \gls{kb} QA, as this approach requires the mapping of the query to a structured data representation. As the task of this thesis is to develop a general system, that is adaptable to different data inputs, \gls{kb} QA will be excluded \cite{dimitrakis_survey_2020} (See Section \ref{sec:overview}).

Generative \gls{qa} is often denoted as \textit{Retriever-free} or \textit{Neural-based} approaches. The central characteristic of this paradigm is that knowledge resides within the parameters of a neural network. Consequently, the knowledge is implicit, and the \gls{qa} system will not furnish a specific document, passage, or other source from which it extracted the information. Instead, it offers a textual excerpt. While these systems can achieve competitive performance compared to \gls{ir}-based \gls{qa} systems, they are not under consideration for this thesis due to their lack of reference, which is a crucial requirement for the system (See Section \ref{sec:overview}) to be developed \cite{roberts_how_2020}.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Grafiken/Retriever_Reader.png}
    \caption{Reader-Retriever-System Architecture for QA by Zhu et al. \cite{zhu_retrieving_2021}.}
    \label{fig:rr_architecture}
\end{figure}

Figure \ref{fig:rr_architecture} depicts the general architecture of a \textbf{Retriever-Reader-System}, as defined by Zhu et al. \cite{zhu_retrieving_2021}. This architecture serves as the foundational framework for \gls{ir}-Based \gls{qa} systems and was initially introduced by Harabagiu et al. \cite{harabagiu_open-domain_2003}. In this framework, all modules operate independently, can be trained separately, and are subject to independent evaluation.

The \textbf{Retriever} module's primary role is to retrieve relevant documents, passages, or other pertinent information from a knowledge source and rank them based on their relevance to answering the user's query. Subsequently, the \textbf{Reader} module extracts the answer from the retrieved documents and presents it to the user. This task bears a close resemblance to \gls{mrc}, with the key distinction that in \gls{ir}-Based \gls{qa}, the system must handle multiple documents and comprehend them to formulate a response, unlike classical \gls{mrc} tasks, which typically involve only one context document.

The \textbf{Document Post-Processor} module's role is to curate and refine the set of documents that will be forwarded as \textit{Relevant Documents} to the subsequent stage, the Reader. Concurrently, the \textbf{Answer Post-Processor} assists the Reader in addressing complex questions for which the answer may not be found in a single document alone \cite{zhu_retrieving_2021,jurafsky_speech_2023}.

It's worth noting that some researchers include a \textbf{Question Analysis} module preceding the Retriever, which aims to preprocess the received question for more efficient query execution in the Retriever \cite{nassiri_transformer_2023}. However, for the purposes of this thesis, we adhere to Zhu et al.'s definition \cite{zhu_retrieving_2021}, where this functionality is considered part of the Retriever.

Conceptually, there are three distinct approaches to the Retriever itself: \textit{Sparse Retrieval, Dense Retrieval,} and \textit{Iterative Retrieval.} The specifics of these approaches will be thoroughly explored in Section \ref{subsec:qa_retrieval}.

Document Post-Processors can be categorized into \textit{Supervised Learning, Reinforcement Learning,} and \textit{Transfer Learning}-based approaches. A detailed discussion of these approaches is also provided in Section \ref{subsec:qa_retrieval}.

In Section \ref{subsec:qa_reader}, we will delve into the finer details of Reader approaches and Answer Post-processing. Broadly speaking, there are two primary types of Readers: \textit{Extractive} and \textit{Generative} Readers. As for Answer Post-processing, it involves two key categories: \textit{Rule-based} and \textit{Learning-based} approaches.

There are also \textbf{End-to-End} approaches that employ a single module to execute the entire \gls{qa} task. Excluding generative approaches, two common categories of such approaches are \textbf{Retriever-Reader} and \textbf{Retriever-only} models.

An End-to-End Retriever-Reader aims to train both the Retriever and Reader in a single backpropagation step, and in some cases, it introduces additional knowledge sources beyond the traditional \gls{ir} framework. An illustrative example is \gls{rag} \cite{lewis_retrieval-augmented_2021}. \gls{rag} consists of a pre-trained Generator with implicit knowledge encoded in its parameters and a pre-trained Retriever. For each question, the Retriever identifies the most relevant documents and generates a latent vector based on them. This latent vector, along with the original question, is fed into the Generator. Section \ref{subsec:qa_reader} will delve into details regarding the \gls{rag} architecture.

Another end-to-end approach, similar to \gls{rag}, is \gls{realm} \cite{guu_realm_2020}. While these previous two approaches extended the capabilities of pre-trained \gls{s2s} models, Nishida et al. pursued a different path by training a single \gls{nn} to perform both tasks simultaneously: \gls{ir} and \gls{mrc} \cite{nishida_retrieve-and-read_2018}.

It is noteworthy that all these end-to-end approaches have demonstrated competitive performance compared to state-of-the-art methods on specific \gls{qa} datasets.

An essential yet often underestimated question is: What defines textual data, and how should one preprocess formats such as PDFs to extract this textual content? While many datasets already comprise small contextual snippets \cite{wang_modern_2022}, it's crucial not to overlook the entire process of extracting snippets from unstructured PDFs, for example. Approaches to tackle this challenge will be explored in detail in the upcoming Section \ref{subsec:qa_indexing}.

% 2nd push

\subsection{Extraction Approaches}
\label{subsec:qa_indexing}

As discussed in the previous Section \ref{subsec:qa_basics}, the knowledge source for a \gls{qa}-System can take the form of textual or multimodal data. The specific type of data may necessitate certain requirements or specific adjustments to the Retriever used for \gls{ir}.

In the context of this thesis, the primary knowledge source to be employed is PDF documents (See Section \ref{sec:overview}). In the research field, three major approaches exist for extracting textual information from unstructured data types like PDFs: \textit{visual} \cite{tito_document_2021}, \textit{direct} \cite{wang_multi-passage_2019}, and \textit{alternative} \cite{dasigi_dataset_2021} extraction methods.

It's important to note upfront that the chosen extraction method is intricately connected to the subsequent retrieval approach. The specifics, including metadata alongside pure textual data and quality requirements, may vary among different extraction and retrieval methods.

The visual approach is closely aligned with the research field of \textit{Document Question Answering}. A well-known example dataset in this field is \gls{docvqa} \cite{tito_document_2021}. The primary concept behind the visual approach to document question-answering is to capture not only the text of a PDF but also additional information such as the document's structure, various hierarchies on a page (e.g., sections, subsections), and the ability to analyze tables and figures. These hierarchical structures can be leveraged to create two-stage retrieval approaches. In these approaches, initially, a collection of relevant files is identified based on higher-level attributes like the document's title and abstract. Subsequently, a more granular retrieval process is executed over lower-level attributes such as passages within the relevant files. This \textit{Iterative Retrievers} will be further discussed in Section \ref{subsec:qa_retrieval} \cite{liu_dense_2021}.

The challenge of \textit{Visual Document Question Answering} typically involves taking images of PDF pages as inputs and mapping question-answer pairs to them. The answers are extracted from either a single paragraph or a combination of multiple paragraphs \cite{mathew_document_2021}. Nonetheless, the extraction pipeline in this case usually resembles the \textit{Retriever-Reader} architecture, where the extracted information from the visual processing is fed into such a system afterward. Researchers in this field often employ a pipeline that includes a \textit{Document Layout Analysis} model, followed by the application of an \gls{ocr} tool to the detected regions \cite{mcdonald_detect_2022}. Examples of a \textit{Document Layout Analysis} model include the Document Image Transformer by Li et al. \cite{li_dit_2022}.

The direct approach is the most prevalent method in the field of Question Answering (\gls{qa}) and Information Retrieval (\gls{ir}). The primary concept behind this approach is to extract textual information from PDFs and store it in a database. The extraction process can be accomplished using various tools such as \textit{PDFMiner} or \textit{Adobe Extract} \cite{meuschke_benchmark_2023}. However, a lingering question is how to effectively split the extracted textual data, especially considering that they are often not cleaned after extraction.

A common practice when employing a Language Model (\gls{llm}) is to optionally cleanse the text corpus and then divide it based on a predefined token size. This approach is evident in two notable open-source LLM projects: \textit{Langchain} and the \textit{Retrieval Plugin for ChatGPT} by OpenAI \cite{noauthor_langchain-ailangchain_nodate,noauthor_chatgpt_2023}. In the original Dense Retrieval paper by Karpukhin et al., a sliding window of token size 5 was utilized \cite{karpukhin_dense_2020}. Therefore, it can be assumed that for contemporary LLM applications, the precise quality of the data, ensuring that a document contains syntactically correct sentences, may not be as critical.

Apart from modern approaches involving text clipping, previous methods aimed to identify paragraphs and similar structures within the extracted texts \cite{zhu_retrieving_2021}.

An alternative approach involves the methodology employed in constructing the QASPER dataset. In this case, the authors conducted a pre-filtering of scientific papers' PDFs, selecting only those with freely accessible LaTeX files. They then utilized the S2ORC tool to extract cleaned textual data from these LaTeX files \cite{dasigi_dataset_2021}. It's important to note that this approach is highly specific to the QASPER dataset and cannot be universally applied. Nonetheless, it serves as an illustration of alternative methods for extracting textual data from PDFs.

% Commit #3

\subsection{Retrieval Approaches}
\label{subsec:qa_retrieval}

The traditional state-of-the-art in \gls{ir} relies on \textbf{Sparse Retrievers}, with one notable example being BM25. BM25 is renowned as "one of the most empirically successful retrieval models and is widely used in current search engines" \cite{zhu_retrieving_2021}. Nandan et al. even demonstrated that on modern \gls{odqa} datasets, BM25 remains a viable baseline for zero-shot \gls{ir} \cite{thakur_beir_2021}.

BM25 was originally introduced by Robertson et al. \cite{robertson_probabilistic_2009}. It operates by utilizing the TF-IDF token weights between a question $q$ containing tokens $q_1, \ldots, q_T$ and a set of passages $P$, where $p \in P$.

\begin{equation}
    \mathbf{s}_{q, p}^{\text{BM25}}=\sum_{i=1}^T \log \left(\frac{|\mathcal{P}|}{N\left(q_i, \mathcal{P}\right)}\right) \frac{n\left(q_i, p\right)\left(k_1+1\right)}{k_1\left(1-b+\frac{b|p|}{a v p l}\right)+n\left(q_i, p\right)}
    \label{eq:bm25}
\end{equation}

    
Equation \ref{eq:bm25} illustrates the BM25 score for a question $q$ and a passage $p$. In this equation, $N\left(q_i, \mathcal{P}\right)$ represents the count of passages in $\mathcal{P}$ that contain the token $q_i$, while $n\left(q_i, p\right)$ indicates the frequency of token $q_i$ within the passage $p$. The variable $|p|$ signifies the length of passage $p$, and $avpl$ stands for the average passage length in $\mathcal{P}$. The parameters $k_1$ and $b$ are free parameters, typically set to $k_1 = 0.9$ and $b = 0.4$ \cite{mcdonald_detect_2022,robertson_probabilistic_2009}.

Traditionally, this lexical Information Retrieval (\gls{ir}) approach has been capable of providing satisfactory retrieval results. However, in 2020, Karpukhin et al. demonstrated for the first time that a \textbf{Dense Retrieval} approach could outperform the Sparse Retrieval approach across multiple \gls{odqa} datasets \cite{karpukhin_dense_2020}. Consequently, the search for a general Dense Retrieval model has been ongoing, as these Dense Retrieval approaches offer advantages such as semantic matching and the ability to handle lengthy documents.

In general, there are three types of Dense Retrieval approaches \cite{zhu_retrieving_2021}: the \textbf{Representation-based Retriever}, often referred to as the \textit{dual-encoder} \cite{karpukhin_dense_2020}; the \textbf{Interaction-based Retriever}, often referred to as the \textit{cross-encoder}; and the \textbf{Representation-interaction Retriever}, often referred to as the \textit{multi-stop retriever}. Figure \ref{fig:types_of_retriever} illustrates the general architecture of these three types of Dense Retrievers.

The \textbf{Dense Passage Retriever (DPR)} by Karpukhin et al. serves as a notable example to explain the \textbf{Representation-Based Retriever}. Given a collection $M$ of text passages $p$ and a question $q$, the objective of DPR is to identify the $k$ most similar passages to the question. To achieve this, DPR employs two distinct \textbf{BERT} \cite{devlin_bert_2019} Encoders. One Encoder, denoted as $E_Q(\cdot)$, encodes the question $q$ into a $d$-dimensional vector, where $d = 768$. The other Encoder, labeled as $E_P(\cdot)$, encodes the passage $p$ into a $d$-dimensional vector at the \verb|[CLS]| token. The similarity between these two vectors is computed using the inner product:


\begin{equation}
    \mathbf{s}_{q, p}^{D P R}=\mathbf{E}_{Q}(q)^{\top} \mathbf{E}_{P}(p)
    \label{eq:dpr}
\end{equation}

The choice of the inner product as the similarity function is motivated by its computational efficiency and the demonstrated, comparable performance \cite{karpukhin_dense_2020}. The dot-product must yield a small value for pairs of questions and passages that are genuinely related. The training dataset $D$ comprises $m$ instances, where $q_i$ represents the question, $p_i^+$ denotes the positive passage, and $p_{i,n^-}$ represents the negative passage:

\begin{equation}
    \mathbf{D}=\left\{\left(q_{i}, p_{i}^{+}, p_{i, 1}^{-}, \ldots, p_{i, n}^{-}\right)\right\}_{i=1}^{m}
\end{equation}

The loss function is optimized using the negative log-likelihood of $p_i^+$:

\begin{equation}
    \mathcal{L}_{D P R}=-\log \frac{\exp \left(\mathbf{s}_{q_i,p_i^{+}}^{D P R}\right)}{\exp \left(\mathbf{s}_{q_i,p_i^{+}}^{D P R}\right) + \sum_{j=1}^{n} \exp \left(\mathbf{s}_{q_i,p_{i,j}^{-}}^{D P R}\right)}
\end{equation}

It's important to note that in \cite{karpukhin_dense_2020}, the selection of negative passages was not arbitrary. Instead, two additional approaches were employed: BM25 top passages that do not contain the answer and positive passages paired with other questions.

One significant advantage of the Representation-Based Retriever is that passages can be pre-indexed locally rather than at runtime. This reduction in latency between the question and the response may, however, come with trade-offs in the quality of the retrieved passages.

The \textbf{Interaction-Based Retriever} incorporates both the question $q$ and the passage $p$ within a single model, separated by a \verb|[SEP]| indicator. These models offer various approaches for modeling the relationship between $q$ and $p$. For instance, one common method is to utilize the \verb|[CLS]| classifier as an indicator of whether the passage is relevant to the question. This approach was first introduced with \gls{bert} \cite{devlin_bert_2019}. While these models perform competitively with previous Representation-Based Retrievers, it's important to note that they are 100-1000 times more computationally expensive \cite{khattab_colbert_2020}.

To address this latency issue, models like ColBERT introduced the concept of \textbf{co}ntextualized \textbf{l}ate interaction \cite{khattab_colbert_2020}. In this thesis and subsequently in research, it is referred to as the \textbf{Representation-Interaction Retriever} \cite{zhu_retrieving_2021}.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Grafiken/Types_of_Retriever.png}
    \caption{Types of Dense Retriever by Zhu et al. \cite{zhu_retrieving_2021}.}
    \label{fig:types_of_retriever}
\end{figure}

ColBERT, like \gls{dpr}, employs two \gls{bert} Encoders, denoted as $E_Q\left(\cdot\right)$ and $E_P\left(\cdot\right)$. However, it introduces a late interaction mechanism. When provided with a query $q$, it is initially tokenized into BERT-based Wordpiece tokens, resulting in $q_1, \ldots, q_T$. Following the \verb|[CLS]| token, a \verb|[Q]| token is appended to signify the question. If the length of the tokenized question is less than $N_q$, a predetermined token length, the remaining portion of the question is padded with BERT's \verb|[mask]| token. Otherwise, it is truncated. This process, known as *query augmentation*, allows BERT to re-weight existing terms or expand the query, and it is pivotal to ColBERT's performance. The generated embeddings are then passed through a linear layer to reduce the output dimensions to a fixed size $m$, which is smaller than the original dimensions of BERT. The output is subsequently normalized to ensure that the L2 norm of each result equals one.

For each passage $p$, $E_P\left(\cdot\right)$ is employed for encoding. Similar to the question encoding process, $p$ is segmented into its $p_1, \ldots, p_{T_d}$ Wordpiece tokens. The special token \verb|[D]| indicates a passage. Short passages are not padded with a \verb|[mask]| token. After the classical BERT output, a similar post-processing step is applied to the encoded passages, and all embeddings corresponding to punctuation are filtered out.

\begin{equation}
    \mathbf{E}_{q}:= Normalize(CNN(BERT("[Q] q_0 q_1 \dots q_T [mask] \dots [mask]")))
\end{equation}

\begin{equation}
    \mathbf{E}_{p}:= Filter(Normalize(CNN(BERT("[D]p_0 p_1 \dots p_T"))))
\end{equation}

The late interaction mechanism applied to the encodings involves computing the maximum similarity, which utilizes cosine similarity through dot products. This is made possible by the earlier normalization applied to the embeddings:

\begin{equation}
    \mathbf{s}_{q, p}^{C o l B E R T}=\sum_{I\in\left[\left|\mathbf{E}_q\right|\right]}\max _{j\in\left[\left|\mathbf{E}_d\right|\right]} \mathbf{E}_{q, i} \cdot \mathbf{E}_{p, j}^{\top}
\end{equation}

The interaction mechanism has no trainable parameters. ColBERT is differentiable end-to-end. During training, for example, with a triple $(q, p^+, p^-)$, ColBERT independently produces a score for each passage and is subsequently optimized pairwise using softmax cross-entropy loss over the scores of $p^+$ and $p^-$ \cite{khattab_colbert_2020}.

Another type of Retriever is the \textbf{Iterative Retriever}. Iterative Retrievers are necessary when dealing with questions that are more complex than simple factoid questions, which can be answered by identifying the right passage in the knowledge source. An example is the HotpotQA dataset \cite{yang_hotpotqa_2018}, designed specifically for multi-hop questions. The fundamental concept here is that such questions cannot be answered with just one precise piece of evidence. They require multiple passages from different documents at the very least. Iterative Retrievers encompass three stages in the pipeline: (1) document retrieval, (2) query reformulation, and (3) retrieval stopping.

An example is BEAM, currently holding the title of the highest-performing\footnote{Status as of September 23, 2023, according to https://paperswithcode.com and the authors of \cite{zhang_beam_2023}}, \gls{qa}-System across multi-hop \gls{qa} datasets such as HotpotQA \cite{zhang_beam_2023}. The document retrieval component can take the form of any retrieval model, including options like ColBERT, BM25, or \gls{dpr}. In the case of BEAM, it leverages an Interaction-Based Retriever using DeBERTa. For each candidate passage $p_c$, BEAM calculates a relevance score concerning this passage within the context of all previously identified relevant passages $p_r$ and the question $q$, using the embeddings of the \verb|[CLS]| tokens \cite{he_deberta_2020}. The second step, query reformulation, can be executed explicitly or implicitly, meaning it can either be expressed in natural language or as a dense embedding. The advantage of using natural language lies in its interpretability; while employing dense embeddings operates within a semantic space and does not lack vocabulary interpretability \cite{zhu_retrieving_2021}. BEAM adopts a natural language-based approach. Specifically, after each hop, it appends the newly identified passage to the previously identified ones and feeds this information into DeBERTa:

\begin{equation}
    \text{s}_{q, p}^{BEAM} = \text{Classifier}(\text{DeBERTa}("[CLS] q \: p_{r_1} \: \ldots \: p_{r_i} \: ")) \quad | \quad p_{c} \in P
\end{equation}

The nature of query reformulation depends on the type of retriever in use. Lastly retrieval stopping poses its own set of challenges. A common approach involves setting either a fixed number of hops or a maximum limit on the retrieved documents. Alternatively, some methods introduce a new token, such as \verb|[EOE]| (End-of-Evidence), to signal the end of retrieval \cite{zhu_retrieving_2021}. BEAM, for example, employs a fixed number of hops, specifically 2, as determined through empirical evaluation.

The task of \textbf{Document Post-Processing} is to reduce the number of passages forwarded to the Reader, aiming to eliminate irrelevant ones. Traditional Retrievers, like Sparse Retrievers, often require a Document Post-Processor. However, Dense Retrievers often incorporate ranking and retrieval simultaneously, rendering this module unnecessary \cite{zhu_retrieving_2021}. Nevertheless, it remains possible to construct multi-stage Retrievers. This can be achieved by using a simpler Dense Retriever for pre-filtering passages and subsequently applying a more accurate one \cite{liu_dense_2021}.

% Commit #4

\subsection{Reader Approaches}
\label{subsec:qa_reader}

Readers originally emerged from the field of \gls{mrc}, where the objective is to extract an answer from a given context. A well-known example is the SQuAD \cite{rajpurkar_squad_2016} dataset, which was mentioned in Section \ref{subsec:qa_basics}. However, unlike the original \gls{mrc} task, a Reader in a Retrieval-Reader-System must process multiple passages to determine the relevant information needed to answer a given question \cite{zhu_retrieving_2021}. 

Modern readers rely on \gls{prlm}s since they establish new baselines on well-known datasets \cite{luo_choose_2022}. In general, there are two types of Readers that use \gls{prlm}s: \textbf{Extractive Readers} and \textbf{Generative Readers} \cite{jurafsky_speech_2023,zhu_retrieving_2021,luo_choose_2022}.

In general, an \textbf{Extractive Reader} employs an encoder to identify the token sequence span that is relevant for answering a question. These encoders can be any autoencoder models, such as BERT \cite{devlin_bert_2019}, DeBERTa \cite{he_deberta_2020}, or RoBERTa \cite{liu_roberta_2019}. Luo et al. \cite{luo_choose_2022} even utilized the encoder components of established encoder-decoder models like T5 \cite{raffel_exploring_2023} and BART \cite{lewis_bart_2019}. They demonstrated that, after fine-tuning, these models can outperform encoder-only models on certain tasks.

Figure \ref{fig:extractive_reader} illustrates the span labeling process performed by the extractive reader. The question tokens $q_1, \ldots, q_n$ and the passage tokens $p_1, \ldots, p_m$ are input into the encoder, separated by a \verb|[SEP]| token. The encoder learns two new embeddings, $S$ and $E$, which represent span-start and -end tokens, respectively. To obtain the span start probability for an output token $p_i^{\prime}$, the dot product between the output token and $S$ is computed and then normalized by a softmax function over all output tokens. The process is similar for the span-end token. The score of a span from position $i$ to $j$ is calculated as $S * p_{i}^{\prime} + E * p_{j}^{\prime}$. The span with the highest score, where $j \geq i$, is selected as the answer span. If the total length of tokens in $q$ and $p$ exceeds the maximum input length of the encoder, the passage is split into multiple segments, and the process is repeated for each segment \cite{jurafsky_speech_2023,luo_choose_2022}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Grafiken/Extractive_Reader.png}
    \caption{Adjusted Graphic of the Extractive Reader by Jurafsky et al. \cite{jurafsky_speech_2023}}
    \label{fig:extractive_reader}
\end{figure}

The \textbf{Generative Reader} operates straightforwardly when familiar with a \gls{s2s} encoder-decoder model. Given a dataset containing $(q,p,a)$ tuples, the encoder takes $q$ and $p$ as input and outputs the contextual representation $h$. Then, it is the decoder's task to generate a token sequence based on $h$ and attention. The training objective can be described as minimizing the following loss function:

\begin{equation}
    \mathcal{L}_{\mathrm{Gen}}=\sum_{i=1}^K \log P\left(a_i \mid h, a_{: i}\right)
\end{equation}

Here, $K$ represents the length of tokens in $a$, $a_i$ is the $i^{th}$ token in $a$, and $a_0$ is a special beginning of sequence token. In cases where the answer is not contained within the passages, the \verb|[CLS]| token indicates this situation \cite{luo_choose_2022,zhu_retrieving_2021}.

Latest research projects like Visconde \cite{pereira_visconde_2022} even employ \gls{llm} as Generative Readers. The performance and usability of these models remain active topics of research.

Luo et al. conducted the first survey comparing state-of-the-art Extractive and Generative Readers \cite{luo_choose_2022}. They discovered that \enquote{on average, extractive readers perform better than generative ones} \cite{luo_choose_2022}, except in cases involving long context passages, where generative approaches outperform the extractive ones.

\textbf{RAG} can be seen as a generative reader, but with a much more capable \gls{nn} as the reader, specifically the idea is that the reader itself is a \gls{llm} with implicit knowledge encoded in its parameters, which it uses to generate an answer, the retrieved passages intentionally function as support in order to guide the reader and reduce risk of hallucination. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Grafiken/RAG-Figure1.png}
    \caption{Overview of RAG by Lewis et al. \cite{lewis_retrieval-augmented_2021}}
    \label{fig:rag}
\end{figure}

Figure \ref{fig:rag} is taken from the original paper by Lewis et al. \cite{lewis_retrieval-augmented_2021} and displays the general approach of \gls{rag}. The original idea of \gls{rag} is to have an end-to-end backpropagation in order to train the retriever and reader (generator) at once and on the same data, not separate as in most Retriever-Reader-Systems. The used retriever in the original \gls{rag} is a \gls{dpr}. Other retrievers can be used, as this is just a decision to make as the generator does not directly depend on the type of retriever. More interestingly is the kind of implementation of the generator. \gls{rag} implements a \textit{sequence-based} generator, while future work, such as \gls{fid} \cite{izacard_leveraging_2021} use an \textit{attention-based} generator. The sequence-based generator works the following way: Given an arbitrary encoder-decoder $p_{\theta}(y_i | q, p, y_{1:i-1})$, the query $q$, the $k$-relevant passages $p$, and the previously generated tokens $y_{1:i-1}$, the generator computes the probability distribution over the next token $y_i$. $q$ and $p$ where simply concatenated.

Further \gls{rag} generators are attention-based like \gls{fid} \cite{izacard_leveraging_2021}. Here the encoder and decoder of the generator are slightly decoupled as to the classic \gls{rag}. Given a question $q$, the retriever retrieves the top-$k$ passages $p$. The encoder encodes every single passage in a question, title, passage triple $(q, t, p)$. The encodings of multiple passages are afterward concatenated and passed into the encoder-decoder attention of the decoder. An illustration can be found in Figure \ref{fig:fid} This allows for the combination of multiple passages, so there is no input token limitation as for the classical \gls{rag}. Also, experiments by Izacard et al. showed, that the performance improves over multiple tasks, as multi-passage relations can easily be resolved by the decoder. The latest work of Izacard et al. is ATLAS, which set new state-of-the-art benchmarks on multiple evaluation tasks \cite{izacard_atlas_2022}. ATLAS extends on the idea of \gls{fid}.   

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Grafiken/fid.png}
    \caption{Overview of FiD by Izacard et al. \cite{izacard_leveraging_2021}}
    \label{fig:fid}
\end{figure}

Still overall \gls{rag} approaches, the main idea is to have a fully end-to-end backpropagation during training or fine-tuning of the systems and a \gls{llm} whose generation is supported by passages.

The \textbf{Answer Post-Processor} is similar to the Document Post-Processor, serving as an optional component. Its primary task is to provide support for multi-hop complex questions, helping determine the final answer from a set of answers extracted by the reader component \cite{zhu_retrieving_2021}. Depending on the implementation of the Reader, this component may become obsolete.

% Commit #5

\subsection{Limitations}
\label{subsec:qa_limitations}

The evaluation metrics for \gls{ir} systems will be discussed in detail in Chapter \ref{chap:eval}. In general, selecting the components and models for an \gls{ir} system always involves a trade-off between accuracy, memory consumption, and inference speed \cite{zhang_survey_2023}.

Accuracy is primarily determined by the chosen Retriever-Reader-System. Sparse retrievers often lack a certain degree of semantic understanding, resulting in less accurate retrieved passages. In contrast, Dense Retrievers can achieve higher levels of accuracy but require thorough evaluation and training for the desired use case. Thakur et al. demonstrated that high-accuracy Dense Retrievers like \gls{dpr} can underperform in zero-shot scenarios compared to BM25 by -47.7\% \cite{thakur_beir_2021}. This highlights another crucial limitation of all \gls{nn}-based retrievers and readers: training. BM25 is, by nature, an unsupervised model for \gls{ir}, while common approaches for Dense Retrieval usually belong to the group of supervised models. These models heavily depend on their training data, whereas a Sparse Retriever like BM25 can be used without any training. According to experiments conducted by Thakur et al. \cite{thakur_beir_2021}, the best-performing out-of-distribution standalone Retrievers are Representation-Interaction Retrievers like ColBERT. Nevertheless, there exist enhanced approaches like \textit{Mixture-of-Experts}/\textit{Hybrid-Search}  or the multi-stage retrievers like \textit{BM25+CE} (will be discussed in Section \ref{subsec:retriever}), which are an implementation of the mentioned \textit{Document Post-Processing}. These approaches can outperform standalone Retrievers.

Constructing a training dataset for a \gls{qa} task can be a tedious process, as these datasets must consist of tuples in the form of $(\text{question, context, answer})$, which is not always feasible. One established research direction to address this issue is \gls{qg} \cite{serban_generating_2016}. In \gls{qg}, a \gls{s2s} model is employed to generate questions and answers based on a given passage.

Zhang et al. provide an example of \gls{dpr} applied to the Natural Questions dataset in their survey on efficient \gls{odqa} \cite{zhang_survey_2023}. The total processing time for a query is 0.91 seconds\footnote{It's important to mention that \gls{dpr} is a Representation-based Retriever, which allows offline storage of passage embeddings. The result was obtained using an Nvidia GeForce Rtx 2080 Ti GPU, averaged over 1000 examples}. This time is divided into 74.79\% for evidence search and 23.95\% for reading. The total memory cost is 79.32GB, with the index occupying 81.95\%, the raw corpus 16.39\%, and the model 1.66\%. Approaches to optimize this may include:

\begin{enumerate}
    \item Reducing Processing Time: (1) Accelerating Evidence Search, (2) Accelerating Reading
    \item Reducing Memory Cost: (1) Reducing Index Size, (2) Reducing Corpus Size, (3) Reducing Model Size
    \item One-stage Frameworks: (1) Directly Generating Answers, (2) Directly Retrieving Answers
\end{enumerate}

Techniques used in this context may include:

\begin{enumerate}
    \item Data-based: (1) Passage Filtering, (2) Dimension Reduction, (3) Product Quantization
    \item Model-based: (1) Model Pruning, (2) Knowledge Distillation, (3) Knowledge Source 
\end{enumerate}

A common technique, which is used in nearly every experimental setup for \gls{qa}-Systems, is FAISS \cite{johnson_billion-scale_2017}, a GPU-optimized implementation of the exact $k$-means clustering algorithm.

For a detailed overview of approaches towards more efficient \gls{odqa} systems, please refer to the comprehensive survey by Zhang et al. \cite{zhang_survey_2023}.

% Commit #6

\section{Conversational Question Answering}
\label{sec:cqa}

The differentiation of \gls{convqa} towards \gls{qa} will be discussed in Section \ref{subsec:cqa_basics}. This section also introduces the fundamental concepts of \gls{convqa} which are necessary to understand challenges and necessary components compared to a regular \gls{qa}-System. Section \ref{subsec:cqa_contextual_query_understanding} will cover approaches toward the concept of query expansion. Section \ref{subsec:cqa_initiative} will clampse on the concept of initiative and further approaches towards a Conversation Manager.

 Most crucial for the research of this thesis are the concepts introduced in Section \ref{subsec:cqa_basics}. Especially how a \gls{convqa} distinguishes from a regular \gls{qa}-System and the core concepts of a conversation. The approaches introduced in Section \ref{subsec:cqa_contextual_query_understanding} are good to know, but the thesis will later focus mostly on query rewriting. The other approaches are necessary for alternative system designs which will not be discussed. Section \ref{subsec:cqa_initiative} only scratches the surface of the research field of \textit{Initiative}. This is a highly complex field, with ongoing research and won't be covered in this thesis.

\subsection{Basics}
\label{subsec:cqa_basics}

Core concepts in the field of \gls{odcqa} towards a conversation in terms of \gls{convqa} are: \textit{Turns}, \textit{Hisotry}, \textit{Memory}, \textit{Session} and \textit{Dialog Features} and \textit{Dialog State} \cite{zamani_conversational_2023}. It's important to mention, that in other subdomains/-tasks of \gls{cis} more concepts are introduced, such as \textit{State}, those are not necessary or applicable for \gls{odcqa} \cite{zaib_conversational_2021}.

Figure \ref{fig:conversation_explain} shows the core concepts based on a chat. Firstly, a \textbf{Turn} is a question-response pair. Whereas a conversation usually consists of multiple turns (multi-turn). \gls{coqa} is a dataset published in 2019 by researchers at Stanford in order to extend the known \gls{qa} dataset SQuAD towards a conversational dataset, whereas on average one conversation session consists of 15 turns \cite{reddy_coqa_2018}. Multi-turns are the main distinguisher between the in Section \ref{sec:qa} introduced task, to a \gls{convqa} task. In a multi-turn scenario natural language phenomena like \textit{coreference} (multiple expressions referring to the same thing) or \textit{ellipsis} (omitting words or topics implied by the context) can occur. While in regular \gls{qa} the System will only be challenged with single-turn scenarios, so only one question, which needs an answer, in \gls{convqa} the systems have to face multi-turn scenarios, where a user might also ask followup question or in general multiple questions after each other. A \textbf{Hisotry} is consequently a set of turns that belong to one conversation session. A \textbf{Session} is a in it completed conversation. Lastly, the \textbf{Memory} is an abstract entity in which the \gls{convqa}-System stores knowledge related to a history, session or even user in general \cite{zamani_conversational_2023,gao_neural_2022}. The extent of this depends on the implementation of memory in the \gls{convqa} pipeline, which will be discussed in Section \ref{subsec:cqa_contextual_query_understanding}.

\textbf{Dialog Features} need to be assessed extra to the other mentioned concepts. While the other concepts tackle the frame of the conversation, the dialog feature evaluates the user questions themself. Possible dialog features may include \textit{drilling-down} questions, \textit{topic-shift}, \textit{clarification} or \textit{definition}. Different dialog features call for different responses by the system \cite{gupta_conversational_2020}. The \textbf{Dialog State} has to be assessed similarly. The dialog state represents the relation between turns. In cases of pre-defined domains methods like state slots are used, e.g. \verb |Date _, Location _, Artist _| have to be filled during the conversation in order to retrieve the correct information from the \gls{kb} \cite{rastogi_schema-guided_2020}. Open-Domain \gls{convqa} usually don't track the state \textit{explicitly}, but rather track it \textit{implicitly} via the type of implementation of the \textit{Contextual Query Understanding} unit.


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Grafiken/Conversation_Explain.png}
    \caption{Concepts of a Conversation in regards to a CIS}
    \label{fig:conversation_explain}
\end{figure}

Regarding the System architecture of a \gls{convqa} there is no one fits them all solution at the moment, but Gao et al. \cite{gao_neural_2022} presented a modern system architecture, which represents commonly used approaches and their corresponding components in a general fashion. This general architecture can be observed in Figure \ref{fig:convqa_system_architecture}. Modern \gls{convqa} systems are closely related to \gls{qa} systems, but lag certain generalizing components in order to be full \gls{cis} systems \cite{zamani_conversational_2023}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Grafiken/System_Architecture_ConQA.png}
    \caption{General System Architecture of a Conv QA System by Gao et al. \cite{gao_neural_2022}}
    \label{fig:convqa_system_architecture}
\end{figure}

Similar to the retriever-reader architecture introduced in Section \ref{subsec:qa_architectures}, a \gls{convqa} is made up of those two components as well. The retriever has to understand the context, so the history of a conversation and retrieve based on that the most relevant documents. The reader on the other hand is closely related to the reader of a classic retriever-reader architecture \cite{zamani_conversational_2023,gao_neural_2022}. Some implementations even feed into the reader component the context in order to rank the retrieved passages better and generate a more accurate answer \cite{owoicho_trec_2022}.

\subsection{Contextual Query Understanding}
\label{subsec:cqa_contextual_query_understanding}

There are two main distinguishing approaches towards history implementation. The first is a simple heuristic of using the last-$k$ turns for \textbf{Query Expansion}, \textbf{Query Rewriting} or \textbf{Conversational Retrievers}. The second is to extract the important parts of the history in regards to a question and use them for Query Expansion or Rewriting \cite{gao_neural_2022}.

A good example to explain the second approach towards extracting important parts of the history $H = {(q_1, a_1), \dots , (q_i, a_i)}$ given a new question $q_{i+1}$ is \gls{quretec}\cite{voskarides_query_2020}. \gls{quretec} consists of two components essentially: one BERT-based model and a trainable classification layer. The $H$ is being passed through the BERT model, whereas the following structure of concatination is being used: 

\begin{equation}
    \text{BERT}([CLS],H,[SEP],q_{i+1}) 
\end{equation}

On every first sub-token of a term of the $H$ the term classification layer is applied, which is a network consisting of a dropout layer, a linear layer and a sigmoid function. The term classification layer predicts a label between $0-1$ indicating its importance for answering the new question $q_i$. This leads to a set of terms $I$ which need to be incorporated into the retrieval \cite{voskarides_query_2020}. This is generally also known as \textbf{Query Expansion}, whereas we add terms to a given query for retrieval. Next to this supervised, trained approach, there are also implementations that work unsupervised like Historical Query Expansion (HQExp) \cite{yang_query_2019}, which was one of the best-performing models in the TREC CAsT 2019 \cite{dalton_trec_2020}.

Modern neural approaches more often implement a \textbf{Query Rewriting} module which is built on top of \gls{s2s}-models to rewrite a query $q_{i+1}$ given a history $H$ in order to use the generated new query for retrieval using an established \gls{qa} retriever \cite{owoicho_trec_2022}. The main advantage of this approach is the absence of the need for large supervised datasets as for Conversational Retrievers \cite{dai_dialog_2022}. One of the top performing models in the TREC CAsT 2022 was HEATWAVE by a Team of the University of Cambridge England \cite{liusie_university_nodate}. HEATWAVE utilized a query rewriter and a classical lexical BM25 retriever in combination with a BERT-based re-ranker. The rewriter uses a T5-based Transformer model and gives as input $ctx-n-m$, where $n$ refers to the last $n$-many user utterances and $m$ to the $m$-many system responses. In general, the task can be simply broken down into the following:

\begin{equation}
    q_{rewritten} = \text{Rewriter}(ctx-n-m)
\end{equation}

For training of this model, they used among others the canard dataset \cite{elgohary_can_2019} a manually annotated version of the QuAC dataset, specifically for the task of query rewriting given a conversation history $H$.

State-of-the-art research utilizes more and more \gls{llm} for the task of Query Rewriting, as they can handle long context histories $H$ and are in general strong zero- or few-shot models \cite{mao_large_2023}. This is also the main approach frameworks like \textit{Langchain} \cite{noauthor_question_nodate} or \textit{ChatGPT by OpenAI} \cite{noauthor_chatgpt_2023} use. 


Lastly the approach of \textbf{Conversational Retrievers} exists. Those use compared to classical \gls{qa} retrievers not a pair of $(q,p)$ in order to calculate a similarity $sim(q,p)$ between the question $q$ and the passage $p$, but use conversational interactions like $(q_1, a_1, \dots, q_i, a_i, q_{i+1}, p)$, in short $(H, q_{i+1}, p)$, so combining a conversation history $H$ with a new question $q_{i+1}$ and the relevant passage $p$ to answer this question given the history $H$ \cite{gao_neural_2022,dai_dialog_2022}. High-performing zero-shot or subdomain-adapted Conversational Retrievers do not exist, as it is extremely time-consuming to create a dataset for this type of Retriever. To close this gap, researchers proposed sufficient data augmentation techniques to generate those datasets, given a document. One example is the work of Dai et al. \cite{dai_dialog_2022} which introduced the technique of \enquote{Dialog Inpainting} \cite{dai_dialog_2022}.  

\subsection{Initiative}
\label{subsec:cqa_initiative}

Most modern human-computer interactions follow a one-sided initiative model, where either the user- or the system-initiative is given. In mixed-initiative scenarios of \gls{convqa} the system can take initiative without explicit commands of the user. Examples of initiative are: \textit{Topic Shifts}, \textit{Clarification Questions} or \textit{Question Recommendations} \cite{zamani_conversational_2023}. In this thesis, we will focus on \textit{Clarification Questions} only.

Asking \textit{Clarification Questions} is the task of identifying ambiguity in a user's search request and resolving it by posing a question with the intent to eliminate ambiguity. A common taxonomy to use for the types of ambiguous questions includes: 1) questions where the focus is ambiguous, and 2) questions with several distinct possible answers \cite{larsson_issue-based_2002}. Several studies have been conducted to understand user behavior in relation to clarification in search. Tavakoli et al. \cite{tavakoli_analyzing_2021} found that users are more likely to engage when a \textit{Clarification Question} aims at clarifying ambiguous information instead of seeking confirmation or similar. Zamani et al. \cite{zamani_analyzing_2020}, based on search-engine log analysis, found that users are more likely to engage with a \textit{Clarification Question} if their own question has high ambiguity and, therefore, multiple possible resolutions or when there is a dominant assumed search intent.

To resolve ambiguity in the context of \gls{convqa}, the most modern solutions follow a two-step approach, where, in the first step, the system identifies the ambiguity, and in the second step, the system generates a \textit{Clarification Question} to resolve the ambiguity \cite{kuhn_clam_2023, guo_abg-coqa_nodate}. The exact implementation may differ, whereas Guo et al. \cite{guo_abg-coqa_nodate} developed a \gls{s2s}-model to predict the ambiguity of a question given context, Kuhn et al. \cite{kuhn_clam_2023} used a chain-of-thought-like approach, where a \gls{llm} performs both steps sequentially. In general, it has to be said that, in the context of \gls{odqa}, the niche of \textit{Clarification Questions} is not well-researched yet, and there is no established benchmark dataset.

\section{Efficient Large Language Models}
\label{sec:efficient_llm}

With the increasing size of large language models (\gls{llm}), \gls{llama} 2 offers models ranging from 7 billion to 70 billion parameters \cite{touvron_llama_2023}. Even these models are considered relatively small compared to the largest models like PaLM 2 \cite{anil_palm_2023} with 340 billion parameters. The challenge arises when running such models in scenarios with limited computational resources, especially on smaller domains or tasks. This challenge is particularly relevant to the task presented in this thesis, which involves building a \gls{convqa} system for a custom set of documents.

While several surveys \cite{ling_domain_2023, zhao_survey_2023} have explored the topic of efficient \gls{llm} usage in resource-constrained systems, Treviso et al. \cite{treviso_efficient_2023} present the most comprehensive taxonomy of methods and approaches in this context. Figure \ref{fig:llm_taxonomy} provides a high-level overview of the stages at which efficiency-improving methods can be implemented in \gls{llm}s. Given the specific focus of this thesis, not all stages will be discussed in detail. For more comprehensive insights, please refer to the original survey by Treviso et al. \cite{treviso_efficient_2023}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Grafiken/Efficient_Survey_Steps.png}
    \caption{Adapted Stages of Efficiency Improvement for LLMs by Treviso et al. \cite{treviso_efficient_2023}}
    \label{fig:llm_taxonomy}
\end{figure}

Section \ref{subsec:llm_fine_tuning} will explore possibilities to enhance efficiency during the fine-tuning process, while Section \ref{subsec:llm_compression} will delve into the topic of model compression, which is applicable to the \textit{Inference} step in Figure \ref{fig:llm_taxonomy}.

 This thesis itself won't delve into fine-tuning (the content of Section \ref{subsec:llm_fine_tuning}) or model compression (the content of Section \ref{subsec:llm_compression}) itself. Still, concepts like multi-task learning are necessary to understand the capabilities and where those are coming from for modern \gls{llm}s, and the technique of \textit{Prompting} will be used in Chapter \ref{chap:eval}. Also, in Chapter \ref{chap:eval}, quantized models will be used to evaluate the performance of the \gls{convqa}-System, out of the necessity to run \gls{llm}s on the available hardware resources. Nevertheless, the following section can be seen as an excursion to the main topic of this thesis for real-world implementations facing resource-constrained systems.


% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{Grafiken/Efficient_Survey_Taxonomy.png}
%     \caption{Adjusted Taxonomy of efficiency improvement for \gls{llm} by Treviso et al. \cite{treviso_efficient_2023}}
%     \label{fig:llm_taxonomy_2}
% \end{figure}

\subsection{Fine-Tuning}
\label{subsec:llm_fine_tuning}

Hu et al. \cite{hu_lora_nodate} demonstrated the significant benefits of fine-tuning GPT-3 for few-shot applications, highlighting the remarkable improvements fine-tuning can achieve. This is further supported by the experiments conducted by Chung et al. \cite{chung_scaling_2022}.

Efficient fine-tuning of \gls{llm}s can be categorized into three distinct approaches: \textit{Parameter Efficiency}, \textit{Multi-task Learning}, and \textit{Prompting}. Figure \ref{fig:llm_fine_tuning} provides an overview of these approaches along with their corresponding methods.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Grafiken/fine_tuning_approaches.png}
    \caption{Adapted Fine-Tuning Approaches for \gls{llm} by Treviso et al. \cite{treviso_efficient_2023}}
    \label{fig:llm_fine_tuning}
\end{figure}

\textbf{Parameter Efficiency} is commonly referred to as \textbf{\gls{peft}} \cite{noauthor_peft_nodate}. A notable \gls{peft} approach is \gls{lora}, developed by Hu et al. \cite{hu_lora_nodate}. \gls{lora} falls under the category of adapters, a term coined because it revolves around the concept of freezing the parameters of the \gls{llm} and fine-tuning only a small set of task-specific parameters, which can be swapped depending on the desired downstream task. Unlike some other adapter-based methods \cite{houlsby_parameter-efficient_2019}, \gls{lora} does not introduce additional inference latency due to the merging of trainable matrices with frozen weights. Moreover, \gls{lora} can be seamlessly combined with many other \gls{peft} methods.

In practice, given a pre-trained weight matrix \(W_0 \in \mathbb{R}^{d \times k}\), which is typically full-rank between layers, the update can be constrained to be a low-rank composition: \(W_0 + \triangle W = W_0 + BA\), where \(B \in \mathbb{R}^{d \times r}\), \(A \in \mathbb{R}^{r \times k}\), and the rank \(r \ll \min(d, k)\). While \(W_0\) remains frozen during training, \(A\) and \(B\) become trainable parameters. The forward pass \(h = W_0x\) can be represented as the following sum:

\begin{equation}
    h = W_0x + \triangle Wx = W_0x + BAx
\end{equation}

Figure \ref{fig:lora} illustrates the architecture and initialization during training. The parameters of \(A\) are randomly sampled using Gaussian initialization, while \(B\) is initialized to 0.

Other \gls{peft} techniques include \textbf{prompt-tuning} \cite{lester_power_2021} and \textbf{prefix-tuning} \cite{li_prefix-tuning_2021}. Both approaches are similar in the way they leverage task-specific modifications to the input to guide the model's behavior. They involve concatenating learned vectors to activations or embedding sequences, making them activation-modifying \gls{peft} methods.

A \gls{peft} approach that can be considered a hybrid between \gls{lora} and activation-modifying techniques is \textbf{$\text{(IA)}^3$} \cite{liu_few-shot_2022}. What sets $\text{(IA)}^3$ apart is its focus on \gls{llm}s designed explicitly for multi-task learning, as all existing \gls{peft} techniques significantly underperformed in experiments conducted by Liu et al. \cite{liu_few-shot_2022}. In $\text{(IA)}^3$, the model's activations are rescaled using element-wise multiplication with learned vectors, known as adaptors. Specifically, $\text{(IA)}^3$ employs three learned vectors: \(l_k \in \mathbb{R}^{d_k}\) for keys and \(l_v \in \mathbb{R}^{d_v}\) for values in self-attention and encoder-decoder attention mechanisms, as well as \(l_{ff} \in \mathbb{R}^{d_{ff}}\) for the feed-forward network. The rescaling is incorporated into the attention mechanism as follows:

\begin{equation}
    \operatorname{softmax}\left(\frac{Q\left(l_{k} \odot K^T\right)}{\sqrt{d_k}}\right)\left(l_{v} \odot V\right)
\end{equation}

For the feed-forward network, the rescaling is implemented as follows, where \(\gamma\) represents the feed-forward activation:

\begin{equation}
    (l_{ff} \odot \gamma (W_1x))W_2
\end{equation}

In summary, \textbf{$\text{(IA)}^3$} is a \gls{peft} approach specifically designed for multi-task learning, and it appears to outperform \gls{lora} in terms of the number of parameters added and training computation costs.

\textbf{Multi-task Learning} refers to the concept of fine-tuning a \gls{prlm} on various tasks to achieve better zero-shot and fine-tuning performance. One of the most prominent \gls{prlm}s trained on multiple NLP tasks is T5 \cite{raffel_exploring_2023}. Liu et al. \cite{liu_few-shot_2022} demonstrated that a generically trained T5 model (T0) can be few-shot fine-tuned with approximately 10\% of the parameters of \gls{lora}, at a lower computational cost, while achieving higher accuracy in classification tasks\footnote{Currently, there is no experiment comparing LoRa and $\text{(IA)}^3$ on QA tasks.}. This is made possible, due to using $\text{(IA)}^3$ as \gls{peft} and the multi-task pre-training of the used model. Still, this also includes the drawback of having in general a larger model.

\textbf{Prompting} refers to the general concept of presenting a task as a textual instruction to a \gls{llm} \cite{brown_language_2020}. Recent advances have even led to the development of a new sub-task and job role called \textit{Prompt Engineering} \cite{white_prompt_2023}. It's worth noting that different prompts with the same intent can yield different results, making the selection of the right prompt a challenge in itself \cite{liu_gpt_2021}. Furthermore, concepts like \gls{cot} prompts have been developed. In \gls{cot} prompts, the given example in a few-shot prompt is redesigned to mimic step-by-step reasoning and conclusions known from the way humans think, aiming to achieve higher performance in zero- and few-shot scenarios simply by adjusting the explicit natural language prompt \cite{wei_chain--thought_2023}. \gls{cot} requires retraining of the \gls{llm}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Grafiken/Lora.png}
    \caption{General Idea of LoRa by Hu et al. \cite{hu_lora_nodate}}
    \label{fig:lora}
\end{figure}


\subsection{Compression}
\label{subsec:llm_compression}

Compression aims to reduce the size of a model, whether it's the number of parameters while maintaining the same level of accuracy on the downstream task, or the actual storage required for the model. There are three primary approaches to this task: \textit{Pruning}, \textit{Knowledge Distillation}, and \textit{Quantization} \cite{treviso_efficient_2023, zhu_survey_2023}. Figure \ref{fig:llm_compression} provides an overview of these approaches and their corresponding methods.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Grafiken/compression_approaches.png}
    \caption{Adapted Compression Approaches for \gls{llm} by Treviso et al. \cite{treviso_efficient_2023}}
    \label{fig:llm_compression}
\end{figure}

\textbf{Pruning} can be further categorized into \textit{structured} and \textit{unstructured} pruning. Structured pruning involves removing specific patterns of weights or activations from a model, with the goal of maintaining a dense matrix representation to ensure compatibility with existing implementations and hardware. One notable example of a structured pruner for \gls{llm}s is LLM-Pruner \cite{ma_llm-pruner_2023}. On the other hand, unstructured pruning entails removing individual weights or activations from a model, resulting in a sparse matrix representation. This approach may require specialized hardware or software implementations to efficiently compute and achieve speed improvements of 1.5$\times$ to 2.16$\times$, while reducing up to 60\% of the parameters \cite{frantar_sparsegpt_2023}. Examples of engines designed specifically for unstructured pruning include NVIDIA's CUTLASS library for GPUs \cite{frantar_sparsegpt_2023} and DeepSparse \cite{noauthor_deepsparse_2023} for CPUs.

\textbf{Knowledge Distillation} is an approach that involves using a generally well-performing \gls{llm} as a teacher to instruct a significantly smaller student model \cite{hinton_distilling_2015}. Zhu et al. distinguish between \textit{White-box} and \textit{Black-box} knowledge distillation. In the former, the student has full access to the teacher's parameters, while in the latter, only the teacher's predictions are accessible to the student \cite{zhu_survey_2023}. An example of white-box knowledge distillation is MiniLLM \cite{gu_knowledge_2023}, where the distribution of the final layer's outputs for both the teacher and the student, given a prompt, is compared using the Kullback-Leibler divergence. This comparison is used in a loss function for backpropagation in the student model.

Black-box approaches are more commonly used in knowledge distillation. In these cases, a \gls{llm} is employed to either directly provide its predictions based on a prompt \cite{huang_-context_2022}, offer assisting explanations \cite{li_explanations_2022}, or sort the training data by difficulty and artificially generate more data points \cite{jiang_lion_2023}, among other techniques. For a comprehensive overview, please refer to Section 2.2 \textit{Knowledge Distillation} in Zhu et al.'s survey \cite{zhu_survey_2023}. Experiments with different distillation approaches have shown that distillation has its limitations, and for specific downstream tasks, fine-tuning can outperform knowledge distillation \cite{zhu_teach_nodate}.

\textbf{Quantization} is an approach that involves reducing the datatype representation of weights or activations, which are typically floating-point numbers, to smaller representations in terms of bits, such as 8-bit integers or even smaller discrete formats \cite{gholami_survey_2021}. Generally, there is a distinction between \textit{Quantization-aware Training} and \textit{Post-Training Quantization}. The names are self-explanatory; the former involves applying and adjusting quantization during the training process (either pre-training or fine-tuning) \cite{liu_llm-qat_2023}, while the latter pertains to quantization after the training is completed \cite{frantar_gptq_2023}. In both cases, numerous approaches and methods exist, applying different paradigms and quantization techniques, including decisions regarding which parameters to quantize, structured vs. unstructured quantization, quantization strength, and many others. It's not possible and necessary to discuss all of these here, but for a comprehensive overview, please refer to Section 2.3 \textit{Quantization} in Zhu et al.'s survey \cite{zhu_survey_2023}.

The most prominent example of Post-Training Quantization is GPTQ, which is the only ready-to-use implementation available in the Huggingface Transformer Library \cite{noauthor_quantize_nodate}. GPTQ was the first method to achieve high compression for \gls{llm}s with over 175 billion parameters while maintaining high accuracy compared to prior state-of-the-art algorithms. Specifically, with a 4-bit quantization of the weights, GPTQ achieved approximately 5$\times$ compression for BLOOM-176B and OPT-175B, two openly available \gls{llm}s, while experiencing only a $\leq$ 0.25 decrease in perplexity compared to the original model. Therefore, the following section will explain the Post-Training Quantization approach of GPTQ in detail.

\textbf{GPTQ} builds upon \gls{obq}, the previous work by Frantar et al. on Post-Training Quantization \cite{frantar_optimal_2023}. With GPTQ, their objective was to reduce the runtime complexity of \gls{obq}, which is $O(d_{row} * d_{col}^{3})$, making it compatible with \gls{llm}s containing billions of parameters. The central idea behind GPTQ is a layer-wise optimization approach. The aim is to discover quantized weights $\widehat{\mathbf{W}}$ that minimize the squared error compared to the full-precision layer $\mathbf{W}$ output, using a given set of input data points $\mathbf{X}$:

\begin{equation}
    \operatorname{argmin}_{\widehat{\mathbf{W}}}\|\mathbf{W} \mathbf{X}-\widehat{\mathbf{W}} \mathbf{X}\|_2^2
\end{equation}

In \gls{obq}, we denote the next weight to be quantized as $w_q$. We define the function $\text{quant}(w)$, which rounds a weight $w$ to the nearest value on the quantization grid.

\begin{equation}
    w_q=\operatorname{argmin}_{w_q} \frac{\left(\text { quant }\left(w_q\right)-w_q\right)^2}{\left[\mathbf{H}_F^{-1}\right]_{q q}}
\end{equation}

In the context of \textbf{GPTQ}, a column of weights is always updated simultaneously. Therefore, $\text{quant}(W_{:,j})$ refers to the following:

\begin{equation}
    \text{quant}(W_{:,j}) := \operatorname \forall w_q \in W_{:,j} 
\end{equation}

The Hessian matrix $\mathbf{H_F} = 2X_FX_F^T$ is utilized for both weight updates and quantization error calculations. Once all columns within a block $B$ are quantized, the weight update is computed as follows, where $Q$ represents the set of indices corresponding to quantized weights:

\begin{equation}
    \boldsymbol{\delta}_F = -\left(\mathbf{w}_Q-\text { quant }\left(\mathbf{w}_Q\right)\right)\left(\left[\mathbf{H}_F^{-1}\right]_{Q Q}\right)^{-1}\left(\mathbf{H}_F^{-1}\right)_{:, Q} \\
    \label{eq:weight_update}
\end{equation}

Furthermore, the Hessian is updated in the following manner, avoiding the need for recomputation; instead, columns corresponding to quantized weights are simply dropped from the Hessian.

\begin{equation}
    \mathbf{H}_{-Q}^{-1} = \left(\mathbf{H}^{-1} - \mathbf{H}_{:, Q}^{-1}\left[\left(\mathbf{H}^{-1}\right)_{Q Q}\right]^{-1} \mathbf{H}_{Q,:}^{-1}\right)_{-Q}.
    \label{eq:hessian_upadte}
\end{equation}

This leads to Algorithm \ref{alg:quantize_W}.

\begin{algorithm}
    \caption{Quantize $W$ given inverse Hessian $H^{-1} = (2XX^T + \lambda I)^{-1}$ and blocksize $B$ by Frantar et al. \cite{frantar_gptq_2023}}
    \label{alg:quantize_W}
    \begin{algorithmic}[1]
    \State $Q \gets \mathbf{0}_{d_{\text{row}} \times d_{\text{col}}}$ \Comment{Quantized output}
    \State $E \gets \mathbf{0}_{d_{\text{row}} \times B}$ \Comment{Block quantization errors}
    \State $H^{-1} \gets \text{Cholesky}(H^{-1})^T$ \Comment{Hessian inverse information}
    
    \For{$i \gets 0, B, 2B, \ldots$}
        \For{$j \gets i, \ldots, i + B - 1$}
            \State $Q_{:,\,j} \gets \text{quant}(W_{:,\,j})$ \Comment{Quantize column}
            \State $E_{:,\,j-i} \gets \frac{W_{:,\,j} - Q_{:,\,j}}{[H^{-1}]_{j,j}}$ \Comment{Quantization error}
            \State $W_{:,\,j:(i+B)} \gets W_{:,\,j:(i+B)} - E_{:,\,j-i} \cdot H^{-1}_{j,j:(i+B)}$ \Comment{Update weights in block}
        \EndFor
        \State $W_{:,(i+B):} \gets W_{:,(i+B):} - E \cdot H^{-1}_{i:(i+B),(i+B):}$ \Comment{Update all remaining weights}
    \EndFor
    \end{algorithmic}
\end{algorithm}

To enable GPTQ to be applicable to \gls{llm}s with billions of parameters, the authors have introduced three key optimizations:

\begin{enumerate}
    \item \textit{Arbitrary Order:} In the case of large models, the order in which weights are quantized becomes irrelevant. Therefore, GPTQ updates all weights in the same order for all rows. This means that the set of unquantized weights, denoted as $F$, and $H_F^{-1}$, the Cholesky Form - Inverse Layer Hessian, remain constant across all rows. This is because $H_F$ depends solely on $X_F$ and is independent of the weights. This reduction in the number of times $H$ needs to be updated simplifies the process from $d_{col} \times d_{row}$ updates to just $d_{col}$ updates.
    \item \textit{Lazy Batch-Updates:} Quantization of a column depends solely on updates to that particular column. Therefore, GPTQ employs batches of columns (with a batch size of $B = 128$). Equations \ref{eq:weight_update} and \ref{eq:hessian_upadte} can be executed after the computation of a full batch $B$. The set of indices $Q$ corresponds to the indices of quantized weights in the batch.
    \item \textit{Cholesky Reformulation:} To address numerical errors that arise from repeated application of equation \ref{eq:hessian_upadte}, a Cholesky reformulation is applied to calculate all the necessary information about $H^{-1}$ in advance. As the complete Cholesky decomposition cannot be applied, a mild damping factor is applied to the diagonal.
\end{enumerate}

Additionally, an accessible quantization package called \textbf{AutoGPTQ} has been developed, which implements the GPTQ algorithm in PyTorch \cite{william_autogptq_2023}. This package has been adopted by Hugging Face and is currently the only ready-to-use quantization technique available in the Transformers library \cite{noauthor_quantize_nodate}.

\section{Related Work}
\label{sec:related_work}

\subsection{Question Answering based on PDFs}
\label{subsec:related_work_dbqa}

\textbf{PDF Question Answering} is the task of providing answers to questions related to the content of one or multiple documents \cite{mathew_document_2021}. The field of research which actively explores this the closest is Visual Document Question Answering. It works on the development of an IR-QA system that operates on images of documents. An exemplary architecture and a general pipeline for transforming PDFs into an IR-QA system is presented by McDonald et al. \cite{mcdonald_detect_2022}. They developed their zero-shot framework around the QASPER dataset but used the original PDFs instead of extracted text via LaTeX. Moreover, readily available open-source tools like V-Doc \cite{ding_v-doc_2022} simplify the deployment and testing of datasets, models, and IR-QA systems of the Visual Document Question Answering domain.

More recently, the open-source framework \textit{Langchain} has gained tremendous attention\footnote{As of September 24, 2023, Langchain has received 63k stars on GitHub}. Langchain focuses on harnessing LLMs using chains, which are essentially prompts for an LLM that can be chained together \cite{noauthor_langchain-ailangchain_nodate}. They also provide documentation on building a QA system based on PDFs \cite{noauthor_question_nodate}. Similarly, \textit{OpenAI} offers a Retrieval Plugin for \textit{ChatGPT} \cite{noauthor_chatgpt_2023}, also an open-source repository. These QA systems adhere to the paradigms established in previous works such as \cite{karpukhin_dense_2020,ni_large_2021,neelakantan_text_2022,lewis_retrieval-augmented_2021}. Specifically, this entails:

\begin{itemize}
    \item Given a text corpus, documents can be retrieved by extracting relevant passages. Data cleaning of the corpus is optional but not necessary. Therefore, these systems employ a \textit{direct extraction} approach, especially when dealing with PDFs.
    \item Utilizing large-scale, diversely trained encoders. Representation-based Retrievers, when equipped with sufficient trainable parameters and diverse training datasets, often yield comparable results to fine-tuned, more complex retrieval models \cite{ni_large_2021,neelakantan_text_2022}.
    \item Using the LLM as a generative reader for QA, as demonstrated in the work of Izacard et al. \cite{izacard_leveraging_2021}.
\end{itemize}

Non-LLM research for \gls{qa} based on PDFs is notably scarce. In the field of ODQA, discussions regarding applicable frameworks that encompass the entire pipeline from PDFs to \gls{qa} are infrequent. Instead, the focus often revolves around constructing \gls{qa} systems using predefined and well-supervised datasets. However, there is some research that explores the feasibility of deploying high-performing \gls{qa} systems in out-of-domain scenarios, bypassing the initial stage of data preprocessing (from PDFs to passages). This research strives to outline possibilities for using a \gls{qa} system in real-world passage collections.

\noindent \textbf{Applying Dense Retrievers Out-of-Domain}: As emphasized by Thakur et al. in their \enquote{Heterogeneous Benchmark for zero-shot Evaluation of Information Retrieval Models} (BEIR) \cite{thakur_beir_2021}, dense retrievers exhibit weak out-of-domain performance. Lyu et al. \cite{farea_evaluation_2022} also demonstrate the limited generality of dense retrievers when trained in one subdomain and subsequently applied in a different one. This underscores the conclusion that there are two approaches to employing retrievers in out-of-domain scenarios: (1) fine-tuning or (2) zero-shot, but with large encoders that have been trained on diverse datasets \cite{ni_large_2021}.

The challenge with fine-tuning lies in the unavailability of labeled data, which is typically required for supervised models in the form of tuples such as $(question,\allowbreak answer,\allowbreak context)$. Several diverse approaches have been developed to address this issue. One approach employs \gls{qg} techniques, as exemplified by PROMPTAGATOR \cite{dai_promptagator_2022}, which utilizes LLMs. Another strategy involves the use of Mixture-of-Experts and meta-learning algorithms \cite{chen_improving_2021}. Some researchers have explored semi-supervised training datasets, as demonstrated by Sachan et al. \cite{sachan_questions_2023}, who developed ART, a training framework for dense retrievers that only requires questions and surpasses the standard \gls{dpr} training implementation. At the current point in time, there is no state-of-the-art approach to fine-tune a dense retriever on a small subdomain dataset.

In their study, Reddy et al. \cite{reddy_synthetic_2022} addressed the challenge of creating a \gls{qa}-System for Covid-19-related documents, where no supervised \gls{qa} dataset was available. Consequently, they conducted a comparison between the performance of zero-shot BM25 and \gls{dpr}. Their findings revealed that BM25 outperformed \gls{dpr} on the BiosQA \gls{qa} dataset, closely related to the Covid-19 domain. Throughout their experiments, they evaluated various approaches, including simple zero-shot techniques, and fine-tuning of \gls{dpr} using \gls{qg} via BART, which yielded superior results. Notably, the most effective retriever for unsupervised domain adaptation was a combination of BM25 and unsupervised fine-tuned \gls{dpr}.

Furthermore, Gururangan et al. \cite{gururangan_dont_2020} demonstrated in their experiments that fine-tuning \glspl{prlm} on domain-specific language or, even better, task-specific data led to a significant performance boost.

Gholami et al. \cite{gholami_zero-shot_2021} experimented with non-fine-tuned dense retrievers on a non-\gls{qa} dataset, specifically a collection of AWS documentations. Their results, particularly for the retrieval component, were sobering, aligning with the findings of benchmark studies by Thakur et al. \cite{thakur_beir_2021} and Lyu et al. \cite{farea_evaluation_2022}.

On the other hand, there exist reader components with a high degree of generalizability, as demonstrated by UnifiedQA-v2 \cite{khashabi_unifiedqa-v2_2022}, an extractive reader and T5 \cite{raffel_exploring_2023}, a generative reader. So the main challenge, when building a \gls{ir}-\gls{qa}-System, lies within the implementation and adaptation of the retriever component.


\subsection{Open-domain Conversational Question Answering}
\label{subsec:related_work_cqa}

\textbf{Datasets:} Notable datasets for \gls{convqa} include CoQA, TREC 2019, and QReCC, which primarily feature extractive questions \cite{reddy_coqa_2018, dalton_trec_2020, dai_dialog_2022}. While these datasets address conversation-specific challenges like coreference resolution, the absence of an adequate benchmark for \gls{convqa} is evident. Naveed et al.'s survey on \gls{llm}s \cite{naveed_comprehensive_2023} highlights various alternative datasets in this domain, largely focused on specific challenges. Recent works like Llama2 underscore the lack of a state-of-the-art benchmark, necessitating human-based evaluation as a primary approach \cite{touvron_llama_2023}.

\vspace{\baselineskip}
\noindent\textbf{Chat Fine-Tuned LLMs:} Fine-tuning \gls{llm}s on human chat-like tasks, introduced by LaMDA and later widely spread by ChatGPT, has emerged as a promising approach for conversational information retrieval \cite{thoppilan_lamda_2022, noauthor_chatgpt_2023}. These models are designed for conversational interactions and can generate human-like responses, although they may lack truthfulness, a crucial aspect for high-quality \gls{convqa} systems.

\vspace{\baselineskip}
\noindent\textbf{Truthfulness:} Addressing the issue of truthfulness, several approaches such as REALM, \gls{rag}, \gls{fid}, and WebGPT leverage external knowledge \cite{guu_realm_2020, lewis_retrieval-augmented_2021, izacard_leveraging_2021, nakano_webgpt_2022}. Among these, \gls{rag} has garnered significant research attention due to its ease of implementation and explicit knowledge, enhancing model understandability \cite{gao_retrieval-augmented_2024}.

\vspace{\baselineskip}
\noindent\textbf{Use-Case Implementation:} Research on implementing a real-world \gls{convqa} system using a specific dataset is limited. The DialDoc 2021 Shared Task presents various solutions, mainly focusing on extractive readers \cite{feng_dialdoc_2021}. However, there is a lack of research addressing the challenges, approaches, and considerations for designing such a system based on the latest advances in \gls{llm}s. The documentation of the Langchain Framework for Question Answering provides some guidens, although it lacks certain considerations \cite{noauthor_question_nodate}, such as complex retrievers or other paradigms then \gls{rag} for example \gls{fid}. Langchain mainly focuses on \gls{dpr} retrievers and explicit \gls{rag} implementations.
