\begin{thebibliography}{10}

\bibitem{farea_evaluation_2022}
Amer Farea, Zhen Yang, Kien Duong, Nadeesha Perera, and Frank Emmert-Streib.
\newblock Evaluation of question answering systems: Complexity of judging a natural language.

\bibitem{zhu_retrieving_2021}
Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua.
\newblock Retrieving and reading: A comprehensive survey on open-domain question answering.

\bibitem{jurafsky_speech_2023}
Dan Jurafsky and James~H. Martin.
\newblock {\em Speech and Language Processing}.
\newblock 3 edition.

\bibitem{green_baseball_1961}
Bert~F. Green, Alice~K. Wolf, Carol Chomsky, and Kenneth Laughery.
\newblock Baseball: an automatic question-answerer.
\newblock In {\em Papers presented at the May 9-11, 1961, western joint {IRE}-{AIEE}-{ACM} computer conference}, {IRE}-{AIEE}-{ACM} '61 (Western), pages 219--224. Association for Computing Machinery.

\bibitem{voorhees_trec-8_1999}
E.~Voorhees.
\newblock The {TREC}-8 question answering track report.

\bibitem{ferrucci_introduction_2012}
D.~A. Ferrucci.
\newblock Introduction to this is watson.
\newblock 56(3):1:1--1:15.
\newblock Conference Name: {IBM} Journal of Research and Development.

\bibitem{hao_recent_2022}
Tianyong Hao, Xinxin Li, Yulan He, Fu~Lee Wang, and Yingying Qu.
\newblock Recent progress in leveraging deep learning methods for question answering.
\newblock 34(4):2765--2783.

\bibitem{etezadi_state_2023}
Romina Etezadi and Mehrnoush Shamsfard.
\newblock The state of the art in open domain complex question answering: a survey.
\newblock 53(4):4124--4144.

\bibitem{zhang_survey_2023}
Qin Zhang, Shangsi Chen, Dongkuan Xu, Qingqing Cao, Xiaojun Chen, Trevor Cohn, and Meng Fang.
\newblock A survey for efficient open domain question answering.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 14447--14465. Association for Computational Linguistics.

\bibitem{mishra_survey_2016}
Amit Mishra and Sanjay~Kumar Jain.
\newblock A survey on question answering systems with classification.
\newblock 28(3):345--361.

\bibitem{mcdonald_detect_2022}
Tavish {McDonald}, Brian Tsan, Amar Saini, Juanita Ordonez, Luis Gutierrez, Phan Nguyen, Blake Mason, and Brenda Ng.
\newblock Detect, retrieve, comprehend: A flexible framework for zero-shot document-level question answering.

\bibitem{dasigi_dataset_2021}
Pradeep Dasigi, Kyle Lo, Iz~Beltagy, Arman Cohan, Noah~A. Smith, and Matt Gardner.
\newblock A dataset of information-seeking questions and answers anchored in research papers.

\bibitem{zamani_conversational_2023}
Hamed Zamani, Johanne~R. Trippas, Jeff Dalton, and Filip Radlinski.
\newblock Conversational information seeking.

\bibitem{rajpurkar_squad_2016}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQuAD}: 100,000+ questions for machine comprehension of text.

\bibitem{dimitrakis_survey_2020}
Eleftherios Dimitrakis, Konstantinos Sgontzos, and Yannis Tzitzikas.
\newblock A survey on question answering systems over linked data and documents.
\newblock 55(2):233--259.

\bibitem{roberts_how_2020}
Adam Roberts, Colin Raffel, and Noam Shazeer.
\newblock How much knowledge can you pack into the parameters of a language model?
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})}, pages 5418--5426. Association for Computational Linguistics.

\bibitem{harabagiu_open_domain_2003}
Sanda~M. Harabagiu, Steven~J. Maiorano, and Marius~A. Pasca.
\newblock Open-domain textual question answering techniques.
\newblock 9(3):231--267.
\newblock Publisher: Cambridge University Press.

\bibitem{nassiri_transformer_2023}
Khalid Nassiri and Moulay Akhloufi.
\newblock Transformer models used for text-based question answering systems.
\newblock 53(9):10602--10635.

\bibitem{lewis_retrieval-augmented_2021}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive {NLP} tasks.

\bibitem{guu_realm_2020}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
\newblock {REALM}: Retrieval-augmented language model pre-training.

\bibitem{nishida_retrieve-and-read_2018}
Kyosuke Nishida, Itsumi Saito, Atsushi Otsuka, Hisako Asano, and Junji Tomita.
\newblock Retrieve-and-read: Multi-task learning of information retrieval and reading comprehension.
\newblock In {\em Proceedings of the 27th {ACM} International Conference on Information and Knowledge Management}, pages 647--656.

\bibitem{wang_modern_2022}
Zhen Wang.
\newblock Modern question answering datasets and benchmarks: A survey.
\newblock Publisher: {arXiv} Version Number: 1.

\bibitem{tito_document_2021}
Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny.
\newblock Document collection visual question answering.
\newblock volume 12822, pages 778--792.

\bibitem{wang_multi-passage_2019}
Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang.
\newblock Multi-passage {BERT}: A globally normalized {BERT} model for open-domain question answering.

\bibitem{liu_dense_2021}
Ye~Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip~S. Yu.
\newblock Dense hierarchical retrieval for open-domain question answering.

\bibitem{mathew_document_2021}
Minesh Mathew, Ruben Tito, Dimosthenis Karatzas, R.~Manmatha, and C.~V. Jawahar.
\newblock Document visual question answering challenge 2020.

\bibitem{li_dit_2022}
Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei.
\newblock {DiT}: Self-supervised pre-training for document image transformer.

\bibitem{meuschke_benchmark_2023}
Norman Meuschke, Apurva Jagdale, Timo Spinde, Jelena Mitrovic, and Bela Gipp.
\newblock A benchmark of {PDF} information extraction tools using a multi-task and multi-domain evaluation framework for academic documents.
\newblock volume 13972, pages 383--405.

\bibitem{noauthor_langchain-ailangchain_nodate}
langchain-ai/langchain: Building applications with {LLMs} through composability.

\bibitem{noauthor_chatgpt_2023}
{ChatGPT} retrieval plugin.
\newblock original-date: 2023-03-23T06:06:22Z.

\bibitem{karpukhin_dense_2020}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.
\newblock Dense passage retrieval for open-domain question answering.

\bibitem{thakur_beir_2021}
Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych.
\newblock {BEIR}: A heterogenous benchmark for zero-shot evaluation of information retrieval models.

\bibitem{robertson_probabilistic_2009}
Stephen Robertson and Hugo Zaragoza.
\newblock The probabilistic relevance framework: {BM}25 and beyond.
\newblock 3:333--389.

\bibitem{devlin_bert_2019}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.

\bibitem{khattab_colbert_2020}
Omar Khattab and Matei Zaharia.
\newblock {ColBERT}: Efficient and effective passage search via contextualized late interaction over {BERT}.

\bibitem{yang_hotpotqa_2018}
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William~W. Cohen, Ruslan Salakhutdinov, and Christopher~D. Manning.
\newblock {HotpotQA}: A dataset for diverse, explainable multi-hop question answering.

\bibitem{zhang_beam_2023}
Jiahao Zhang, Haiyang Zhang, Dongmei Zhang, Yong Liu, and Shen Huang.
\newblock Beam retrieval: General end-to-end retrieval for multi-hop question answering.

\bibitem{he_deberta_2020}
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
\newblock {DEBERTA}: {DECODING}-{ENHANCED} {BERT} {WITH} {DISENTANGLED} {ATTENTION}.

\bibitem{luo_choose_2022}
Man Luo, Kazuma Hashimoto, Semih Yavuz, Zhiwei Liu, Chitta Baral, and Yingbo Zhou.
\newblock Choose your {QA} model wisely: A systematic study of generative and extractive readers for question answering.

\bibitem{liu_roberta_2019}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {RoBERTa}: A robustly optimized {BERT} pretraining approach.

\bibitem{raffel_exploring_2023}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.

\bibitem{lewis_bart_2019}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock {BART}: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.

\bibitem{pereira_visconde_2022}
Jayr Pereira, Robson Fidalgo, Roberto Lotufo, and Rodrigo Nogueira.
\newblock Visconde: Multi-document {QA} with {GPT}-3 and neural reranking.

\bibitem{serban_generating_2016}
Iulian~Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, and Yoshua Bengio.
\newblock Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus.
\newblock In {\em Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 588--598. Association for Computational Linguistics.

\bibitem{johnson_billion-scale_2017}
Jeff Johnson, Matthijs Douze, and Hervé Jégou.
\newblock Billion-scale similarity search with {GPUs}.

\bibitem{ding_v-doc_2022}
Yihao Ding, Zhe Huang, Runlin Wang, Yanhang Zhang, Xianru Chen, Yuzhong Ma, Hyunsuk Chung, and Soyeon~Caren Han.
\newblock V-doc : Visual questions answers with documents.

\bibitem{noauthor_question_nodate}
Question answering {\textbar} langchain.

\bibitem{ni_large_2021}
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo~Hernandez Abrego, Ji~Ma, Vincent~Y. Zhao, Yi~Luan, Keith~B. Hall, Ming-Wei Chang, and Yinfei Yang.
\newblock Large dual encoders are generalizable retrievers.

\bibitem{neelakantan_text_2022}
Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse~Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong~Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna~Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr, Felipe~Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng.
\newblock Text and code embeddings by contrastive pre-training.

\bibitem{izacard_leveraging_2021}
Gautier Izacard and Edouard Grave.
\newblock Leveraging passage retrieval with generative models for open domain question answering.
\newblock In {\em Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}, pages 874--880. Association for Computational Linguistics.

\bibitem{dai_promptagator_2022}
Zhuyun Dai, Vincent~Y. Zhao, Ji~Ma, Yi~Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith~B. Hall, and Ming-Wei Chang.
\newblock Promptagator: Few-shot dense retrieval from 8 examples.

\bibitem{chen_improving_2021}
Haofeng Chen.
\newblock Improving out-of-domain question answering with mixture of experts.

\bibitem{sachan_questions_2023}
Devendra~Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer.
\newblock Questions are all you need to train a dense passage retriever.

\bibitem{reddy_synthetic_2022}
Revanth~Gangi Reddy, Bhavani Iyer, Md~Arafat Sultan, Rong Zhang, Avirup Sil, Vittorio Castelli, Radu Florian, and Salim Roukos.
\newblock Synthetic target domain supervision for open retrieval {QA}.

\bibitem{gururangan_dont_2020}
Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy, Doug Downey, and Noah~A. Smith.
\newblock Don't stop pretraining: Adapt language models to domains and tasks.
\newblock pages 8342--8360.
\newblock Conference Name: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics Place: Online Publisher: Association for Computational Linguistics.

\bibitem{gholami_zero-shot_2021}
Sia Gholami and Mehdi Noori.
\newblock Zero-shot open-book question answering.

\bibitem{khashabi_unifiedqa-v2_2022}
Daniel Khashabi, Yeganeh Kordi, and Hannaneh Hajishirzi.
\newblock {UnifiedQA}-v2: Stronger generalization via broader cross-format training.

\bibitem{noauthor_welcome_nodate}
Welcome to pypdf2: Pypdf2 documentation.

\bibitem{santhanam_colbertv2_2022}
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia.
\newblock {ColBERTv}2: Effective and efficient retrieval via lightweight late interaction.

\bibitem{bajaj_ms_2018}
Payal Bajaj, Daniel Campos, Nick Craswell, Li~Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew {McNamara}, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang.
\newblock {MS} {MARCO}: A human generated {MAchine} reading {COmprehension} dataset.

\end{thebibliography}
