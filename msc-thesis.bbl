\begin{thebibliography}{}

\bibitem[Anil et~al., 2023]{anil_palm_2023}
Anil, R., Dai, A.~M., Firat, O., Johnson, M., Lepikhin, D., Passos, A.~T., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., Chu, E., Clark, J., Shafey, L.~E., Huang, Y., Meier-Hellstern, K.~S., Mishra, G., Moreira, E., Omernick, M., Robinson, K., Ruder, S., Tay, Y., Xiao, K., Xu, Y., Zhang, Y., Abrego, G.~H., Ahn, J., Austin, J., Barham, P., Botha, J.~A., Bradbury, J., Brahma, S., Brooks, K.~M., Catasta, M., Cheng, Y., Cherry, C., Choquette-Choo, C.~A., Chowdhery, A., Cr{\'e}py, C., Dave, S., Dehghani, M., Dev, S., Devlin, J., D'iaz, M.~C., Du, N., Dyer, E., Feinberg, V., Feng, F., Fienber, V., Freitag, M., Garc{\'i}a, X., Gehrmann, S., Gonz{\'a}lez, L., Gur-Ari, G., Hand, S., Hashemi, H., Hou, L., Howland, J., Hu, A.~R., Hui, J., Hurwitz, J., Isard, M., Ittycheriah, A., Jagielski, M., Jia, W.~H., Kenealy, K., Krikun, M., Kudugunta, S., Lan, C., Lee, K., Lee, B., Li, E., Li, M.-L., Li, W., Li, Y., Li, J.~Y., Lim, H., Lin, H., Liu, Z.-Z., Liu, F., Maggioni, M., Mahendru, A., Maynez, J., Misra, V., Moussalem, M., Nado, Z., Nham, J., Ni, E., Nystrom, A., Parrish, A., Pellat, M., Polacek, M., Polozov, O., Pope, R., Qiao, S., Reif, E., Richter, B., Riley, P., Ros, A., Roy, A., Saeta, B., Samuel, R., Shelby, R.~M., Slone, A., Smilkov, D., So, D.~R., Sohn, D., Tokumine, S., Valter, D., Vasudevan, V., Vodrahalli, K., Wang, X., Wang, P., Wang, Z., Wang, T., Wieting, J., Wu, Y., Xu, K., Xu, Y., Xue, L.~W., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng, C., Zhou, W., Zhou, D., Petrov, S., and Wu, Y. (2023).
\newblock Palm 2 technical report.
\newblock arXiv. \url{https://arxiv.org/abs/2305.10403}.

\bibitem[AutoGPTQ, 2023]{william_autogptq_2023}
AutoGPTQ (2023).
\newblock Autogptq. \url{https://github.com/PanQiWei/AutoGPTQ}.
\newblock Original date: 2023-04-13. Accessed: 2023-10-02.

\bibitem[Bajaj et~al., 2016]{bajaj2016ms}
Bajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R., McNamara, A., Mitra, B., Nguyen, T., et~al. (2016).
\newblock Ms marco: A human generated machine reading comprehension dataset.
\newblock {\em arXiv preprint arXiv:1611.09268}.

\bibitem[Brown et~al., 2020]{brown_language_2020}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020).
\newblock Language models are few-shot learners.
\newblock In {\em Proceedings of the 34th International Conference on Neural Information Processing Systems}, NIPS'20, Red Hook, NY, USA. Curran Associates Inc.

\bibitem[Chen, 2021]{chen_improving_2021}
Chen, H. (2021).
\newblock Improving {Out}-of-{Domain} {Question} {Answering} with {Mixture} of {Experts}.

\bibitem[Cohen, 1960]{cohen_coefficient_1960}
Cohen, J. (1960).
\newblock A {Coefficient} of {Agreement} for {Nominal} {Scales}.
\newblock {\em Educational and Psychological Measurement}, 20(1):37--46.

\bibitem[Dai et~al., 2022a]{dai_dialog_2022}
Dai, Z., Chaganty, A.~T., Zhao, V., Amini, A., Rashid, Q.~M., Green, M., and Guu, K. (2022a).
\newblock Dialog {Inpainting}: {Turning} {Documents} into {Dialogs}.
\newblock Publisher: arXiv Version Number: 2.

\bibitem[Dai et~al., 2022b]{dai_promptagator_2022}
Dai, Z., Zhao, V.~Y., Ma, J., Luan, Y., Ni, J., Lu, J., Bakalov, A., Guu, K., Hall, K.~B., and Chang, M.-W. (2022b).
\newblock Promptagator: {Few}-shot {Dense} {Retrieval} {From} 8 {Examples}.
\newblock arXiv. \url{http://arxiv.org/abs/2209.11755}.

\bibitem[Dalton et~al., 2020]{dalton_trec_2020}
Dalton, J., Xiong, C., and Callan, J. (2020).
\newblock {TREC} cast 2019: The conversational assistance track overview.
\newblock {\em CoRR}, abs/2003.13624.

\bibitem[Dasigi et~al., 2021]{dasigi_dataset_2021}
Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N.~A., and Gardner, M. (2021).
\newblock A dataset of information-seeking questions and answers anchored in research papers.
\newblock In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y., editors, {\em Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 4599--4610, Online. Association for Computational Linguistics.

\bibitem[Devlin et~al., 2019]{devlin_bert_2019}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In Burstein, J., Doran, C., and Solorio, T., editors, {\em Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[Dimitrakis et~al., 2020]{dimitrakis_survey_2020}
Dimitrakis, E., Sgontzos, K., and Tzitzikas, Y. (2020).
\newblock A survey on question answering systems over linked data and documents.
\newblock {\em Journal of Intelligent Information Systems}, 55(2):233--259.

\bibitem[Ding et~al., 2022]{ding_v-doc_2022}
Ding, Y., Huang, Z., Wang, R., Zhang, Y., Chen, X., Ma, Y., Chung, H., and Han, S. (2022).
\newblock V-doc : Visual questions answers with documents.
\newblock In {\em 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 21460--21466, Los Alamitos, CA, USA. IEEE Computer Society.

\bibitem[Elgohary et~al., 2019]{elgohary_can_2019}
Elgohary, A., Peskov, D., and Boyd-Graber, J. (2019).
\newblock Can {You} {Unpack} {That}? {Learning} to {Rewrite} {Questions}-in-{Context}.
\newblock In {\em Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})}, pages 5918--5924, Hong Kong, China. Association for Computational Linguistics.

\bibitem[Etezadi and Shamsfard, 2023]{etezadi_state_2023}
Etezadi, R. and Shamsfard, M. (2023).
\newblock The state of the art in open domain complex question answering: a survey.
\newblock {\em Applied Intelligence}, 53(4):4124--4144.

\bibitem[Farea et~al., 2022]{farea_evaluation_2022}
Farea, A. A.~S., Yang, Z., Duong, K., Perera, N., and Emmert-Streib, F. (2022).
\newblock Evaluation of question answering systems: Complexity of judging a natural language.
\newblock arXiv. \url{http://arxiv.org/abs/2209.12617}.

\bibitem[Feng, 2021]{feng_dialdoc_2021}
Feng, S. (2021).
\newblock {DialDoc} 2021 {Shared} {Task}: {Goal}-{Oriented} {Document}-grounded {Dialogue} {Modeling}.
\newblock In {\em Proceedings of the 1st {Workshop} on {Document}-grounded {Dialogue} and {Conversational} {Question} {Answering} ({DialDoc} 2021)}, pages 1--7, Online. Association for Computational Linguistics.

\bibitem[Ferrucci, 2012]{ferrucci_introduction_2012}
Ferrucci, D.~A. (2012).
\newblock Introduction to “{This} is {Watson}”.
\newblock {\em IBM Journal of Research and Development}, 56(3.4):1:1--1:15.
\newblock Conference Name: IBM Journal of Research and Development.

\bibitem[Frantar and Alistarh, 2022]{frantar_optimal_2023}
Frantar, E. and Alistarh, D. (2022).
\newblock Optimal brain compression: {A} framework for accurate post-training quantization and pruning.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, {\em Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}.

\bibitem[Frantar and Alistarh, 2023]{frantar_sparsegpt_2023}
Frantar, E. and Alistarh, D. (2023).
\newblock Sparsegpt: massive language models can be accurately pruned in one-shot.
\newblock In {\em Proceedings of the 40th International Conference on Machine Learning}, ICML'23. JMLR.org.

\bibitem[Frantar et~al., 2023]{frantar_gptq_2023}
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. (2023).
\newblock {GPTQ}: {Accurate} {Post}-{Training} {Quantization} for {Generative} {Pre}-trained {Transformers}.
\newblock arXiv. \url{http://arxiv.org/abs/2210.17323}.

\bibitem[Gangi~Reddy et~al., 2021]{reddy_synthetic_2022}
Gangi~Reddy, R., Iyer, B., Sultan, M.~A., Zhang, R., Sil, A., Castelli, V., Florian, R., and Roukos, S. (2021).
\newblock Synthetic target domain supervision for open retrieval qa.
\newblock In {\em Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval}, SIGIR '21, page 1793–1797, New York, NY, USA. Association for Computing Machinery.

\bibitem[Gao et~al., 2023]{gao_neural_2022}
Gao, J., Xiong, C., Bennett, P., and Craswell, N. (2023).
\newblock {\em Neural Approaches to Conversational Information Retrieval}, volume~44 of {\em The Information Retrieval Series}.
\newblock Springer.

\bibitem[Gao et~al., 2024]{gao_retrieval-augmented_2024}
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Guo, Q., Wang, M., and Wang, H. (2024).
\newblock Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}.
\newblock arXiv. \url{http://arxiv.org/abs/2312.10997}.

\bibitem[Gholami et~al., 2022]{gholami_survey_2021}
Gholami, A., Kim, S., Zhen, D., Yao, Z., Mahoney, M., and Keutzer, K. (2022).
\newblock {\em A Survey of Quantization Methods for Efficient Neural Network Inference}, pages 291--326.

\bibitem[Gholami and Noori, 2021]{gholami_zero-shot_2021}
Gholami, S. and Noori, M. (2021).
\newblock Zero-shot open-book question answering.
\newblock arXiv. \url{https://arxiv.org/abs/2111.11520}.

\bibitem[Green et~al., 1961]{green_baseball_1961}
Green, B.~F., Wolf, A.~K., Chomsky, C., and Laughery, K. (1961).
\newblock Baseball: an automatic question-answerer.
\newblock In {\em Papers presented at the {May} 9-11, 1961, western joint {IRE}-{AIEE}-{ACM} computer conference}, {IRE}-{AIEE}-{ACM} '61 ({Western}), pages 219--224, New York, NY, USA. Association for Computing Machinery.

\bibitem[Gu et~al., 2023]{gu_knowledge_2023}
Gu, Y., Dong, L., Wei, F., and Huang, M. (2023).
\newblock Knowledge {Distillation} of {Large} {Language} {Models}.
\newblock arXiv. \url{http://arxiv.org/abs/2306.08543}.

\bibitem[Guo et~al., 2021]{guo_abg-coqa_nodate}
Guo, M., Zhang, M., Reddy, S., and Alikhani, M. (2021).
\newblock Abg-{CoQA}: {Clarifying} {Ambiguity} in {Conversational} {Question} {Answering}.

\bibitem[Gupta et~al., 2020]{gupta_conversational_2020}
Gupta, S., Rawat, B. P.~S., and Yu, H. (2020).
\newblock Conversational {Machine} {Comprehension}: a {Literature} {Review}.
\newblock In {\em Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}}, pages 2739--2753, Barcelona, Spain (Online). International Committee on Computational Linguistics.

\bibitem[Gururangan et~al., 2020]{gururangan_dont_2020}
Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., and Smith, N.~A. (2020).
\newblock Don’t {Stop} {Pretraining}: {Adapt} {Language} {Models} to {Domains} and {Tasks}.
\newblock {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 8342--8360.
\newblock Conference Name: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics Place: Online Publisher: Association for Computational Linguistics.

\bibitem[Guu et~al., 2020]{guu_realm_2020}
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.-W. (2020).
\newblock Realm: retrieval-augmented language model pre-training.
\newblock In {\em Proceedings of the 37th International Conference on Machine Learning}, ICML'20. JMLR.org.

\bibitem[Hao et~al., 2022]{hao_recent_2022}
Hao, T., Li, X., He, Y., Wang, F.~L., and Qu, Y. (2022).
\newblock Recent progress in leveraging deep learning methods for question answering.
\newblock {\em Neural Computing and Applications}, 34(4):2765--2783.

\bibitem[Harabagiu et~al., 2003]{harabagiu_open-domain_2003}
Harabagiu, S.~M., Maiorano, S.~J., and Paşca, M.~A. (2003).
\newblock Open-domain textual question answering techniques.
\newblock {\em Natural Language Engineering}, 9(3):231--267.
\newblock Publisher: Cambridge University Press.

\bibitem[He et~al., 2020]{he_deberta_2020}
He, P., Liu, X., Gao, J., and Chen, W. (2020).
\newblock {DEBERTA}: {DECODING}-{ENHANCED} {BERT} {WITH} {DISENTANGLED} {ATTENTION}.

\bibitem[Hinton et~al., 2015]{hinton_distilling_2015}
Hinton, G., Vinyals, O., and Dean, J. (2015).
\newblock Distilling the {Knowledge} in a {Neural} {Network}.
\newblock arXiv. \url{http://arxiv.org/abs/1503.02531}.

\bibitem[Houlsby et~al., 2019]{houlsby_parameter-efficient_2019}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de~Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. (2019).
\newblock Parameter-efficient transfer learning for {NLP}.
\newblock In Chaudhuri, K. and Salakhutdinov, R., editors, {\em Proceedings of the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, volume~97 of {\em Proceedings of Machine Learning Research}, pages 2790--2799. {PMLR}.

\bibitem[Hu et~al., 2021]{hu_lora_nodate}
Hu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. (2021).
\newblock {LORA}: {LOW}-{RANK} {ADAPTATION} {OF} {LARGE} {LAN}- {GUAGE} {MODELS}.

\bibitem[Huang et~al., 2022]{huang_-context_2022}
Huang, Y., Chen, Y., Yu, Z., and McKeown, K. (2022).
\newblock In-context {Learning} {Distillation}: {Transferring} {Few}-shot {Learning} {Ability} of {Pre}-trained {Language} {Models}.
\newblock arXiv. \url{http://arxiv.org/abs/2212.10670}.

\bibitem[Huggingface, 2023a]{noauthor_peft_nodate}
Huggingface (2023a).
\newblock Peft. \url{https://huggingface.co/docs/peft/index}.
\newblock Accessed: 2023-10-01.

\bibitem[Huggingface, 2023b]{noauthor_quantize_nodate}
Huggingface (2023b).
\newblock Quantize transformers models. \url{https://huggingface.co/docs/transformers/main_classes/quantization}.
\newblock Accessed: 2023-10-02.

\bibitem[Izacard and Grave, 2021]{izacard_leveraging_2021}
Izacard, G. and Grave, E. (2021).
\newblock Leveraging {Passage} {Retrieval} with {Generative} {Models} for {Open} {Domain} {Question} {Answering}.
\newblock In {\em Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}}, pages 874--880, Online. Association for Computational Linguistics.

\bibitem[Izacard et~al., 2022]{izacard_atlas_2022}
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. (2022).
\newblock Atlas: {Few}-shot {Learning} with {Retrieval} {Augmented} {Language} {Models}.
\newblock Publisher: arXiv Version Number: 3.

\bibitem[Jiang et~al., 2023]{jiang_lion_2023}
Jiang, Y., Chan, C., Chen, M., and Wang, W. (2023).
\newblock Lion: Adversarial distillation of proprietary large language models.
\newblock In {\em Conference on Empirical Methods in Natural Language Processing}.

\bibitem[Johnson et~al., 2021]{johnson_billion-scale_2017}
Johnson, J., Douze, M., and Jégou, H. (2021).
\newblock Billion-scale similarity search with gpus.
\newblock {\em IEEE Transactions on Big Data}, 7(3):535--547.

\bibitem[Jurafsky and Martin, 2023]{jurafsky_speech_2023}
Jurafsky, D. and Martin, J.~H. (2023).
\newblock {\em Speech and {Language} {Processing}}.
\newblock Palo Alto, 3 edition.

\bibitem[Kamalloo et~al., 2023]{kamalloo_evaluating_2023}
Kamalloo, E., Dziri, N., Clarke, C. L.~A., and Rafiei, D. (2023).
\newblock Evaluating open-domain question answering in the era of large language models.
\newblock In Rogers, A., Boyd{-}Graber, J.~L., and Okazaki, N., editors, {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada, July 9-14, 2023}, pages 5591--5606. Association for Computational Linguistics.

\bibitem[Karpukhin et~al., 2020]{karpukhin_dense_2020}
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. (2020).
\newblock Dense passage retrieval for open-domain question answering.
\newblock In Webber, B., Cohn, T., He, Y., and Liu, Y., editors, {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 6769--6781, Online. Association for Computational Linguistics.

\bibitem[Khashabi et~al., 2022]{khashabi_unifiedqa-v2_2022}
Khashabi, D., Kordi, Y., and Hajishirzi, H. (2022).
\newblock Unifiedqa-v2: Stronger generalization via broader cross-format training.
\newblock arXiv. \url{http://arxiv.org/abs/2202.12359}.

\bibitem[Khattab and Zaharia, 2020]{khattab_colbert_2020}
Khattab, O. and Zaharia, M. (2020).
\newblock Colbert: Efficient and effective passage search via contextualized late interaction over bert.
\newblock In {\em Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval}, SIGIR '20, page 39–48, New York, NY, USA. Association for Computing Machinery.

\bibitem[Kuhn et~al., 2023]{kuhn_clam_2023}
Kuhn, L., Gal, Y., and Farquhar, S. (2023).
\newblock {CLAM}: {Selective} {Clarification} for {Ambiguous} {Questions} with {Generative} {Language} {Models}.
\newblock arXiv. \url{http://arxiv.org/abs/2212.07769}.

\bibitem[{Langchain}, 2023]{noauthor_langchain-ailangchain_nodate}
{Langchain} (2023).
\newblock Building applications with llms through composability. \url{https://github.com/langchain-ai/langchain}.
\newblock Accessed: 2023-09-22.

\bibitem[Langchain, 2023]{noauthor_question_nodate}
Langchain (2023).
\newblock Question answering | langchain. \url{https://python.langchain.com/docs/use_cases/question_answering/}.
\newblock Accessed: 2023-09-24.

\bibitem[Larsson, 2002]{larsson_issue-based_2002}
Larsson, S. (2002).
\newblock Issue-based {Dialogue} {Management}.

\bibitem[Lester et~al., 2021]{lester_power_2021}
Lester, B., Al-Rfou, R., and Constant, N. (2021).
\newblock The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}.
\newblock In {\em Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}}, pages 3045--3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[Lewis et~al., 2020a]{lewis_bart_2019}
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2020a).
\newblock {BART}: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
\newblock In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J., editors, {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 7871--7880, Online. Association for Computational Linguistics.

\bibitem[Lewis et~al., 2020b]{lewis_retrieval-augmented_2021}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K\"{u}ttler, H., Lewis, M., Yih, W.-t., Rockt\"{a}schel, T., Riedel, S., and Kiela, D. (2020b).
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock In {\em Proceedings of the 34th International Conference on Neural Information Processing Systems}, NIPS'20, Red Hook, NY, USA. Curran Associates Inc.

\bibitem[Li et~al., 2022a]{li_dit_2022}
Li, J., Xu, Y., Lv, T., Cui, L., Zhang, C., and Wei, F. (2022a).
\newblock Dit: Self-supervised pre-training for document image transformer.
\newblock In {\em Proceedings of the 30th ACM International Conference on Multimedia}, MM '22, page 3530–3539, New York, NY, USA. Association for Computing Machinery.

\bibitem[Li et~al., 2022b]{li_explanations_2022}
Li, S., Chen, J., Shen, Y., Chen, Z., Zhang, X., Li, Z., Wang, H., Qian, J., Peng, B., Mao, Y., Chen, W., and Yan, X. (2022b).
\newblock Explanations from {Large} {Language} {Models} {Make} {Small} {Reasoners} {Better}.
\newblock arXiv. \url{http://arxiv.org/abs/2210.06726}.

\bibitem[Li and Liang, 2021]{li_prefix-tuning_2021}
Li, X.~L. and Liang, P. (2021).
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In Zong, C., Xia, F., Li, W., and Navigli, R., editors, {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021}, pages 4582--4597. Association for Computational Linguistics.

\bibitem[Lin, 2004]{lin_rouge_2004}
Lin, C.-Y. (2004).
\newblock {ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}.
\newblock In {\em Text {Summarization} {Branches} {Out}}, pages 74--81, Barcelona, Spain. Association for Computational Linguistics.

\bibitem[Ling et~al., 2023]{ling_domain_2023}
Ling, C., Zhao, X., Lu, J., Deng, C., Zheng, C., Wang, J., Chowdhury, T., Li, Y., Cui, H., Zhang, X., Zhao, T., Panalkar, A., Cheng, W., Wang, H., Liu, Y., Chen, Z., Chen, H., White, C., Gu, Q., Pei, J., and Zhao, L. (2023).
\newblock Domain {Specialization} as the {Key} to {Make} {Large} {Language} {Models} {Disruptive}: {A} {Comprehensive} {Survey}.
\newblock arXiv. \url{http://arxiv.org/abs/2305.18703}.

\bibitem[Liu et~al., 2022]{liu_few-shot_2022}
Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. (2022).
\newblock Few-{Shot} {Parameter}-{Efficient} {Fine}-{Tuning} is {Better} and {Cheaper} than {In}-{Context} {Learning}.
\newblock Publisher: arXiv Version Number: 2.

\bibitem[Liu et~al., 2023a]{liu_gpt_2021}
Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J. (2023a).
\newblock Gpt understands, too.
\newblock {\em AI Open}.

\bibitem[Liu et~al., 2021]{liu_dense_2021}
Liu, Y., Hashimoto, K., Zhou, Y., Yavuz, S., Xiong, C., and Yu, P. (2021).
\newblock Dense hierarchical retrieval for open-domain question answering.
\newblock In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t., editors, {\em Findings of the Association for Computational Linguistics: EMNLP 2021}, pages 188--200, Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[Liu et~al., 2019]{liu_roberta_2019}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019).
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock {\em CoRR}, abs/1907.11692.

\bibitem[Liu et~al., 2023b]{liu_llm-qat_2023}
Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. (2023b).
\newblock {LLM}-{QAT}: {Data}-{Free} {Quantization} {Aware} {Training} for {Large} {Language} {Models}.
\newblock arXiv. \url{http://arxiv.org/abs/2305.17888}.

\bibitem[Liusie et~al., 2022]{liusie_university_nodate}
Liusie, A., Qian, M., Li, X., and Gales, M. (2022).
\newblock {UNIVERSITY} {OF} {CAMBRIDGE} {AT} {TREC} {CAST} 2022.

\bibitem[Luo et~al., 2022]{luo_choose_2022}
Luo, M., Hashimoto, K., Yavuz, S., Liu, Z., Baral, C., and Zhou, Y. (2022).
\newblock Choose your qa model wisely: A systematic study of generative and extractive readers for question answering.
\newblock In {\em Proceedings of the 1st Workshop on Semiparametric Methods in NLP: Decoupling Logic from Knowledge}, pages 7--22.

\bibitem[Ma et~al., 2023]{ma_llm-pruner_2023}
Ma, X., Fang, G., and Wang, X. (2023).
\newblock {LLM}-{Pruner}: {On} the {Structural} {Pruning} of {Large} {Language} {Models}.
\newblock arXiv. \url{http://arxiv.org/abs/2305.11627}.

\bibitem[Magic, 2023]{noauthor_deepsparse_2023}
Magic, N. (2023).
\newblock Deepsparse. \url{https://github.com/neuralmagic/deepsparse}.
\newblock Accessed: 2023-10-02.

\bibitem[Mao et~al., 2023]{mao_large_2023}
Mao, K., Dou, Z., Chen, H., Mo, F., and Qian, H. (2023).
\newblock Large {Language} {Models} {Know} {Your} {Contextual} {Search} {Intent}: {A} {Prompting} {Framework} for {Conversational} {Search}.
\newblock Publisher: arXiv Version Number: 1.

\bibitem[Mathew et~al., 2021]{mathew_document_2021}
Mathew, M., Tito, R., Karatzas, D., Manmatha, R., and Jawahar, C.~V. (2021).
\newblock Document {Visual} {Question} {Answering} {Challenge} 2020.
\newblock arXiv. \url{http://arxiv.org/abs/2008.08899}.

\bibitem[McDonald et~al., 2022]{mcdonald_detect_2022}
McDonald, T., Tsan, B., Saini, A., Ordonez, J., Gutierrez, L., Nguyen, P., Mason, B., and Ng, B. (2022).
\newblock Detect, {Retrieve}, {Comprehend}: {A} {Flexible} {Framework} for {Zero}-{Shot} {Document}-{Level} {Question} {Answering}.
\newblock arXiv. \url{http://arxiv.org/abs/2210.01959}.

\bibitem[Meuschke et~al., 2023]{meuschke_benchmark_2023}
Meuschke, N., Jagdale, A., Spinde, T., Mitrović, J., and Gipp, B. (2023).
\newblock A {Benchmark} of {PDF} {Information} {Extraction} {Tools} using a {Multi}-{Task} and {Multi}-{Domain} {Evaluation} {Framework} for {Academic} {Documents}.
\newblock volume 13972, pages 383--405.
\newblock arXiv:2303.09957 [cs].

\bibitem[Mishra and Jain, 2016]{mishra_survey_2016}
Mishra, A. and Jain, S.~K. (2016).
\newblock A survey on question answering systems with classification.
\newblock {\em Journal of King Saud University - Computer and Information Sciences}, 28(3):345--361.

\bibitem[Muennighoff, 2022]{muennighoff_sgpt_2022}
Muennighoff, N. (2022).
\newblock {SGPT}: {GPT} {Sentence} {Embeddings} for {Semantic} {Search}.
\newblock arXiv. \url{http://arxiv.org/abs/2202.08904}.

\bibitem[Nakano et~al., 2022]{nakano_webgpt_2022}
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., and Schulman, J. (2022).
\newblock {WebGPT}: {Browser}-assisted question-answering with human feedback.
\newblock arXiv. \url{http://arxiv.org/abs/2112.09332}.

\bibitem[Nassiri and Akhloufi, 2023]{nassiri_transformer_2023}
Nassiri, K. and Akhloufi, M. (2023).
\newblock Transformer models used for text-based question answering systems.
\newblock {\em Applied Intelligence}, 53(9):10602--10635.

\bibitem[Naveed et~al., 2023]{naveed_comprehensive_2023}
Naveed, H., Khan, A.~U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N., and Mian, A. (2023).
\newblock A {Comprehensive} {Overview} of {Large} {Language} {Models}.
\newblock arXiv. \url{http://arxiv.org/abs/2307.06435}.

\bibitem[Neelakantan et~al., 2022]{neelakantan_text_2022}
Neelakantan, A., Xu, T., Puri, R., Radford, A., Han, J.~M., Tworek, J., Yuan, Q., Tezak, N.~A., Kim, J.~W., Hallacy, C., Heidecke, J., Shyam, P., Power, B., Nekoul, T.~E., Sastry, G., Krueger, G., Schnurr, D.~P., Such, F.~P., Hsu, K. S.-K., Thompson, M., Khan, T., Sherbakov, T., Jang, J., Welinder, P., and Weng, L. (2022).
\newblock Text and code embeddings by contrastive pre-training.
\newblock arXiv. \url{https://arxiv.org/abs/2201.10005}.

\bibitem[Ni et~al., 2022]{ni_large_2021}
Ni, J., Qu, C., Lu, J., Dai, Z., Hernandez~Abrego, G., Ma, J., Zhao, V., Luan, Y., Hall, K., Chang, M.-W., and Yang, Y. (2022).
\newblock Large dual encoders are generalizable retrievers.
\newblock In Goldberg, Y., Kozareva, Z., and Zhang, Y., editors, {\em Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 9844--9855, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[Nishida et~al., 2018]{nishida_retrieve-and-read_2018}
Nishida, K., Saito, I., Otsuka, A., Asano, H., and Tomita, J. (2018).
\newblock Retrieve-and-{Read}: {Multi}-task {Learning} of {Information} {Retrieval} and {Reading} {Comprehension}.
\newblock In {\em Proceedings of the 27th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}}, pages 647--656.
\newblock arXiv:1808.10628 [cs].

\bibitem[{OpenAI}, 2023]{noauthor_chatgpt_2023}
{OpenAI} (2023).
\newblock Chatgpt retrieval plugin. \url{https://github.com/openai/chatgpt-retrieval-plugin}.
\newblock Accessed: 2023-09-22.

\bibitem[Ouyang et~al., 2022]{ouyang_training_2022}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.~L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P.~F., Leike, J., and Lowe, R. (2022).
\newblock Training language models to follow instructions with human feedback.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, {\em Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}.

\bibitem[Owoicho et~al., 2022]{owoicho_trec_2022}
Owoicho, P., Dalton, J., Aliannejadi, M., Azzopardi, L., Trippas, J.~R., and Vakulenko, S. (2022).
\newblock {TREC} {CAsT} 2022: {Going} {Beyond} {User} {Ask} and {System} {Retrieve} with {Initiative} and {Response} {Generation}.

\bibitem[Papineni et~al., 2002]{papineni_bleu_2002}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
\newblock Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}.
\newblock In {\em Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}}, pages 311--318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

\bibitem[Pereira et~al., 2023]{pereira_visconde_2022}
Pereira, J.~A., do~Nascimento~Fidalgo, R., de~Alencar~Lotufo, R., and Nogueira, R.~F. (2023).
\newblock Visconde: Multi-document {QA} with {GPT-3} and neural reranking.
\newblock In Kamps, J., Goeuriot, L., Crestani, F., Maistro, M., Joho, H., Davis, B., Gurrin, C., Kruschwitz, U., and Caputo, A., editors, {\em Advances in Information Retrieval - 45th European Conference on Information Retrieval, {ECIR} 2023, Dublin, Ireland, April 2-6, 2023, Proceedings, Part {II}}, volume 13981 of {\em Lecture Notes in Computer Science}, pages 534--543. Springer.

\bibitem[Plüster, 2023a]{pluster_leolm_nodate}
Plüster, B. (2023a).
\newblock {LeoLM}: {Ein} {Impuls} für {Deutschsprachige} {LLM}-{Forschung} {\textbar} {LAION}.

\bibitem[Plüster, 2023b]{pluster_leolm_2023}
Plüster, B. (2023b).
\newblock Leolm: Ein impuls für deutschsprachige llm-forschung | laion. \url{https://laion.ai/blog-de/leo-lm/}.
\newblock Accessed: 2023-10-17.

\bibitem[PyPDF2, 2023]{noauthor_welcome_nodate}
PyPDF2 (2023).
\newblock Welcome to pypdf2 — pypdf2 documentation. \url{https://pypdf2.readthedocs.io/en/3.0.0/index.html}.
\newblock Accessed: 2023-09-25.

\bibitem[Raffel et~al., 2020]{raffel_exploring_2023}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J. (2020).
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em J. Mach. Learn. Res.}, 21(1).

\bibitem[Ragas, 2024]{noauthor_ragas}
Ragas (2024).
\newblock Core concepts | ragas. \url{https://docs.ragas.io/en/stable/concepts/index.html}.
\newblock Accessed: 2024-01-11.

\bibitem[Rajpurkar et~al., 2016]{rajpurkar_squad_2016}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016).
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In Su, J., Duh, K., and Carreras, X., editors, {\em Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}, pages 2383--2392, Austin, Texas. Association for Computational Linguistics.

\bibitem[Rastogi et~al., 2020]{rastogi_schema-guided_2020}
Rastogi, A., Zang, X., Sunkara, S., Gupta, R., and Khaitan, P. (2020).
\newblock Schema-guided dialogue state tracking task at dstc8.
\newblock arXiv. \url{https://arxiv.org/abs/2002.01359}.

\bibitem[Reddy et~al., 2019]{reddy_coqa_2018}
Reddy, S., Chen, D., and Manning, C.~D. (2019).
\newblock {C}o{QA}: A conversational question answering challenge.
\newblock {\em Transactions of the Association for Computational Linguistics}, 7:249--266.

\bibitem[Roberts et~al., 2020]{roberts_how_2020}
Roberts, A., Raffel, C., and Shazeer, N. (2020).
\newblock How {Much} {Knowledge} {Can} {You} {Pack} {Into} the {Parameters} of a {Language} {Model}?
\newblock In {\em Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})}, pages 5418--5426, Online. Association for Computational Linguistics.

\bibitem[Robertson and Zaragoza, 2009]{robertson_probabilistic_2009}
Robertson, S. and Zaragoza, H. (2009).
\newblock The {Probabilistic} {Relevance} {Framework}: {BM25} and {Beyond}.
\newblock {\em Foundations and Trends in Information Retrieval}, 3:333--389.

\bibitem[Sachan et~al., 2023]{sachan_questions_2023}
Sachan, D.~S., Lewis, M., Yogatama, D., Zettlemoyer, L., Pineau, J., and Zaheer, M. (2023).
\newblock Questions are all you need to train a dense passage retriever.
\newblock {\em Transactions of the Association for Computational Linguistics}, 11:600--616.

\bibitem[Serban et~al., 2016]{serban_generating_2016}
Serban, I.~V., García-Durán, A., Gulcehre, C., Ahn, S., Chandar, S., Courville, A., and Bengio, Y. (2016).
\newblock Generating {Factoid} {Questions} {With} {Recurrent} {Neural} {Networks}: {The} {30M} {Factoid} {Question}-{Answer} {Corpus}.
\newblock In {\em Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})}, pages 588--598, Berlin, Germany. Association for Computational Linguistics.

\bibitem[Shen et~al., 2023]{chung_scaling_2022}
Shen, S., Hou, L., Zhou, Y., Du, N., Longpre, S., Wei, J., Chung, H.~W., Zoph, B., Fedus, W., Chen, X., Vu, T., Wu, Y., Chen, W., Webson, A., Li, Y., Zhao, V., Yu, H., Keutzer, K., Darrell, T., and Zhou, D. (2023).
\newblock Flan-moe: Scaling instruction-finetuned language models with sparse mixture of experts.
\newblock {\em CoRR}, abs/2305.14705.

\bibitem[Shewale, 2024]{demandsage2022chatgpt}
Shewale, R. (2024).
\newblock Chatgpt statistics. \url{https://www.demandsage.com/chatgpt-statistics/}.
\newblock Accessed: 2024-02-20.

\bibitem[Tavakoli et~al., 2021]{tavakoli_analyzing_2021}
Tavakoli, L., Zamani, H., Scholer, F., Croft, W., and Sanderson, M. (2021).
\newblock Analyzing clarification in asynchronous information‐seeking conversations.
\newblock {\em Journal of the Association for Information Science and Technology}, 73.

\bibitem[Thakur et~al., 2021]{thakur_beir_2021}
Thakur, N., Reimers, N., R{\"u}ckl{\'e}, A., Srivastava, A., and Gurevych, I. (2021).
\newblock {BEIR}: Heterogenous benchmark for zero-shot evaluation of information retrieval models.
\newblock In {\em Proceedings of the 2021 Neural Information Processing Systems (NeurIPS-2021): Track on Datasets and Benchmarks}.

\bibitem[Thoppilan et~al., 2022]{thoppilan_lamda_2022}
Thoppilan, R., De~Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H.~S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhao, V., Zhou, Y., Chang, C.-C., Krivokon, I., Rusch, W., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern, K., Morris, M.~R., Doshi, T., Santos, R.~D., Duke, T., Soraker, J., Zevenbergen, B., Prabhakaran, V., Diaz, M., Hutchinson, B., Olson, K., Molina, A., Hoffman-John, E., Lee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm, M., Kuzmina, V., Fenton, J., Cohen, A., Bernstein, R., Kurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi, E., and Le, Q. (2022).
\newblock {LaMDA}: {Language} {Models} for {Dialog} {Applications}.
\newblock arXiv. \url{http://arxiv.org/abs/2201.08239}.

\bibitem[Tito et~al., 2021]{tito_document_2021}
Tito, R., Karatzas, D., and Valveny, E. (2021).
\newblock Document {Collection} {Visual} {Question} {Answering}.
\newblock volume 12822, pages 778--792.
\newblock arXiv:2104.14336 [cs].

\bibitem[{Touvron} et~al., 2023]{touvron_llama_2023}
{Touvron}, H., {Martin}, L., {Stone}, K., {Albert}, P., {Almahairi}, A., {Babaei}, Y., {Bashlykov}, N., {Batra}, S., {Bhargava}, P., {Bhosale}, S., {Bikel}, D., {Blecher}, L., {Canton Ferrer}, C., {Chen}, M., {Cucurull}, G., {Esiobu}, D., {Fernandes}, J., {Fu}, J., {Fu}, W., {Fuller}, B., {Gao}, C., {Goswami}, V., {Goyal}, N., {Hartshorn}, A., {Hosseini}, S., {Hou}, R., {Inan}, H., {Kardas}, M., {Kerkez}, V., {Khabsa}, M., {Kloumann}, I., {Korenev}, A., {Singh Koura}, P., {Lachaux}, M.-A., {Lavril}, T., {Lee}, J., {Liskovich}, D., {Lu}, Y., {Mao}, Y., {Martinet}, X., {Mihaylov}, T., {Mishra}, P., {Molybog}, I., {Nie}, Y., {Poulton}, A., {Reizenstein}, J., {Rungta}, R., {Saladi}, K., {Schelten}, A., {Silva}, R., {Smith}, E.~M., {Subramanian}, R., {Tan}, X.~E., {Tang}, B., {Taylor}, R., {Williams}, A., {Kuan}, J.~X., {Xu}, P., {Yan}, Z., {Zarov}, I., {Zhang}, Y., {Fan}, A., {Kambadur}, M., {Narang}, S., {Rodriguez}, A., {Stojnic}, R., {Edunov}, S., and {Scialom}, T. (2023).
\newblock {Llama 2: Open Foundation and Fine-Tuned Chat Models}.
\newblock {\em arXiv e-prints}.

\bibitem[Treviso et~al., 2023]{treviso_efficient_2023}
Treviso, M., Lee, J.-U., Ji, T., Aken, B.~V., Cao, Q., Ciosici, M.~R., Hassid, M., Heafield, K., Hooker, S., Raffel, C., Martins, P.~H., Martins, A. F.~T., Forde, J.~Z., Milder, P., Simpson, E., Slonim, N., Dodge, J., Strubell, E., Balasubramanian, N., Derczynski, L., Gurevych, I., and Schwartz, R. (2023).
\newblock Efficient {Methods} for {Natural} {Language} {Processing}: {A} {Survey}.
\newblock {\em Transactions of the Association for Computational Linguistics}, 11:826--860.

\bibitem[TruLens, 2024]{noauthor_truelens}
TruLens (2024).
\newblock Rag triad - trulens. \url{https://www.trulens.org/trulens_eval/core_concepts_rag_triad/}.
\newblock Accessed: 2024-01-11.

\bibitem[Voorhees, 1999]{voorhees_trec-8_1999}
Voorhees, E. (1999).
\newblock The {TREC}-8 {Question} {Answering} {Track} {Report}.

\bibitem[Voskarides et~al., 2020]{voskarides_query_2020}
Voskarides, N., Li, D., Ren, P., Kanoulas, E., and de~Rijke, M. (2020).
\newblock Query {Resolution} for {Conversational} {Search} with {Limited} {Supervision}.
\newblock In {\em Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}}, pages 921--930.
\newblock arXiv:2005.11723 [cs].

\bibitem[Wang et~al., 2020]{wang_minilm_2020}
Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou, M. (2020).
\newblock Minilm: deep self-attention distillation for task-agnostic compression of pre-trained transformers.
\newblock In {\em Proceedings of the 34th International Conference on Neural Information Processing Systems}, NIPS'20, Red Hook, NY, USA. Curran Associates Inc.

\bibitem[Wang, 2022]{wang_modern_2022}
Wang, Z. (2022).
\newblock Modern {Question} {Answering} {Datasets} and {Benchmarks}: {A} {Survey}.
\newblock Publisher: arXiv Version Number: 1.

\bibitem[Wang et~al., 2019]{wang_multi-passage_2019}
Wang, Z., Ng, P., Ma, X., Nallapati, R., and Xiang, B. (2019).
\newblock Multi-passage {BERT}: A globally normalized {BERT} model for open-domain question answering.
\newblock In Inui, K., Jiang, J., Ng, V., and Wan, X., editors, {\em Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 5878--5882, Hong Kong, China. Association for Computational Linguistics.

\bibitem[Wei et~al., 2022]{wei_chain--thought_2023}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.~H., Le, Q.~V., and Zhou, D. (2022).
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, {\em Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}.

\bibitem[White et~al., 2023]{white_prompt_2023}
White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer{-}Smith, J., and Schmidt, D.~C. (2023).
\newblock A prompt pattern catalog to enhance prompt engineering with chatgpt.
\newblock {\em CoRR}, abs/2302.11382.

\bibitem[Yang et~al., 2019]{yang_query_2019}
Yang, J.-H., Lin, S.-C., Lin, J., Tsai, M.-F., and Wang, C.-J. (2019).
\newblock Query and {Answer} {Expansion} from {Conversation} {History}.

\bibitem[Yang et~al., 2018]{yang_hotpotqa_2018}
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C.~D. (2018).
\newblock {H}otpot{QA}: A dataset for diverse, explainable multi-hop question answering.
\newblock In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J., editors, {\em Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 2369--2380, Brussels, Belgium. Association for Computational Linguistics.

\bibitem[{Zaib} et~al., 2021]{zaib_conversational_2021}
{Zaib}, M., {Zhang}, W.~E., {Sheng}, Q.~Z., {Mahmood}, A., and {Zhang}, Y. (2021).
\newblock {Conversational Question Answering: A Survey}.
\newblock {\em arXiv e-prints}, page arXiv:2106.00874.

\bibitem[Zamani et~al., 2020]{zamani_analyzing_2020}
Zamani, H., Mitra, B., Chen, E., Lueck, G., Diaz, F., Bennett, P.~N., Craswell, N., and Dumais, S.~T. (2020).
\newblock Analyzing and learning from user interactions for search clarification.
\newblock In {\em Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval}, SIGIR '20, page 1181–1190, New York, NY, USA. Association for Computing Machinery.

\bibitem[Zamani et~al., 2023]{zamani_conversational_2023}
Zamani, H., Trippas, J.~R., Dalton, J., and Radlinski, F. (2023).
\newblock Conversational {Information} {Seeking}.
\newblock arXiv. \url{http://arxiv.org/abs/2201.08808}.

\bibitem[Zhang et~al., 2023a]{zhang_beam_2023}
Zhang, J., Zhang, H., Zhang, D., Liu, Y., and Huang, S. (2023a).
\newblock Beam {Retrieval}: {General} {End}-to-{End} {Retrieval} for {Multi}-{Hop} {Question} {Answering}.
\newblock arXiv. \url{http://arxiv.org/abs/2308.08973}.

\bibitem[Zhang et~al., 2023b]{zhang_survey_2023}
Zhang, Q., Chen, S., Xu, D., Cao, Q., Chen, X., Cohn, T., and Fang, M. (2023b).
\newblock A {Survey} for {Efficient} {Open} {Domain} {Question} {Answering}.
\newblock In {\em Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})}, pages 14447--14465, Toronto, Canada. Association for Computational Linguistics.

\bibitem[Zhang et~al., 2020]{zhang_bertscore_2020}
Zhang, T., Kishore, V., Wu, F., Weinberger, K.~Q., and Artzi, Y. (2020).
\newblock Bertscore: Evaluating text generation with {BERT}.
\newblock In {\em 8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net.

\bibitem[Zhao et~al., 2023]{zhao_survey_2023}
Zhao, W.~X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J., and rong Wen, J. (2023).
\newblock A survey of large language models.
\newblock arXiv. \url{https://arxiv.org/abs/2303.18223}.

\bibitem[Zhu et~al., 2021]{zhu_retrieving_2021}
Zhu, F., Lei, W., Wang, C., Zheng, J., Poria, S., and Chua, T. (2021).
\newblock Retrieving and reading: {A} comprehensive survey on open-domain question answering.
\newblock {\em CoRR}, abs/2101.00774.

\bibitem[Zhu et~al., 2023]{zhu_survey_2023}
Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. (2023).
\newblock A {Survey} on {Model} {Compression} for {Large} {Language} {Models}.
\newblock arXiv. \url{https://arxiv.org/abs/2308.07633v3}.

\bibitem[Zhu et~al., 2022]{zhu_teach_nodate}
Zhu, Y., Liu, N., Xu, Z., Liu, X., Meng, W., and Wang, Y. (2022).
\newblock Teach {Less}, {Learn} {More}: {On} the {Undistillable} {Classes} in {Knowledge} {Distillation}.

\end{thebibliography}
