\section{Motivation}

The journey of exploring natural language querying dates back to as early as 1961, with researchers embarking on projects like Baseball \cite{green_baseball_1961}, a program designed to respond to users' natural language queries within the domain of baseball. In 1999, the \gls{trec} initiated the \gls{trec}-8 Question Answering track, marking \enquote{the first large-scale evaluation of domain-independent question-answering systems} \cite{voorhees_trec-8_1999}. A more renowned \gls{qa} system is IBM's \textit{Watson}, an open-domain \gls{qa} system that famously triumphed on the television game show Jeopardy! in 2011 \cite{ferrucci_introduction_2012}. The advent of conversational assistants such as Alexa, Siri, and Cortana further fueled interest in conversational information seeking. In 2019, the establishment of the \gls{trec} \gls{cast} aimed to nurture \gls{cis} as an active research field and provide large-scale reusable test beds. However, the true surge in user engagement came with the release of ChatGPT by OpenAI in November 2022, which achieved an astounding one million users within five days \cite{demandsage2022chatgpt}.

Since the widespread adoption of ChatGPT, interest in conversational information seeking has surged once again. However, using a generative \gls{llm} only has certain drawbacks, including the following:

\begin{enumerate}
    \item Parameterized Knowledge: Expansion or updating of knowledge is challenging.
    \item Explainability: Understanding the rationale behind predictions is difficult.
    \item Hallucination: The model may invent seemingly factual information not present in the underlying knowledge source.
\end{enumerate}

These limitations render the sole use of generative models inadequate for practical conversational information-seeking use cases. Consequently, researchers have explored various approaches to address these issues and develop conversational information-seeking systems that mimic human-like conversations \cite{ferrucci_introduction_2012,guu_realm_2020,lewis_retrieval-augmented_2021,nakano_webgpt_2022}. Among these approaches, \gls{rag} \cite{lewis_retrieval-augmented_2021} has emerged as a leading solution, evident in trending frameworks like Langchain \cite{noauthor_question_nodate} and even ChatGPT itself \cite{noauthor_chatgpt_2023}.

However, despite advancements in research, there is a notable absence of papers detailing the adoption of these techniques in real-world use cases. To our knowledge, only two such papers exist \cite{feng_dialdoc_2021,gholami_zero-shot_2021}, which do not comprehensively address the entire problem domain associated with this task, such as the extraction step, and fail to leverage modern possibilities with \gls{llm}s.

\section{Objectives and Contributions}

This thesis aims to bridge this gap by addressing the question of how to build a faithful conversational question-answering system for real-world use cases, a topic that has not been adequately explored in scientific literature to date. This thesis resolves this issue by firstly laying out the fundamental concepts and techniques necessary, secondly developing a holistic framework for constructing such a system based on \gls{rag}, and thirdly implementing a \gls{poc} based on the examination regulations of Heidelberg University. Finally, we evaluate the \gls{poc} using synthetic data and human evaluation. The core contributions of this thesis are as follows:

\begin{itemize}
    \item Providing a holistic overview of the challenges in conversational question-answering systems tailored to a specific document collection and developing a comprehensive framework for constructing such systems based on the four components: Extract, CQU, Retriever and Reader.
    \item Breaking down the extraction of passages from a document collection into pipeline operations.
    \item Identifying challenges faced by the Retriever component towards both the knowledge base and the evidence set and how to resolve them using synthetic-data-based fine-tuning and Mixture-of-Experts.
    \item Decomposing the task of the Reader component into micro-challenges and proposing solutions for them.
    \item Offering multiple approaches for evaluating conversational question-answering systems, with the major insight, that the component-wise evaluation for non-factoid questions needs new approaches and currently at least in the case of this thesis, the human end-to-end evaluation was the most robust approach.
    \item Highlighting the limitations of synthetic data for system evaluation, as those can only be used for factoid question generation due to the used pattern of question-context-answer tuples.
    \item Providing insights into the current bottlenecks of use-case-specific \gls{convqa} systems. Mainly the retriever component and the associated extraction of passages are drawbacks. In the end-to-end evaluation, 46.5\% of errors were attributed to the retriever component and 18.5\% to the extraction component directly.
\end{itemize}

Those cover the main contributions and insights of this thesis, for detailed insights, please refer to the respective chapters. 

\section{Thesis Structure}

The construction of a conversational question-answering system is approached in two stages within this thesis. Firstly, we focus on question-answering based on a single natural language query, followed by the addition of the conversational component. Thus, Chapter \ref{chap:grundlagen} provides a comprehensive overview. Section \ref{sec:qa} presents various approaches and concepts in the field of question-answering, while Section \ref{sec:cqa} extends this perspective to include conversational elements. Narrowing down the range of potential solutions, Chapter \ref{chap:main} introduces the \gls{conrag} framework, which lays out the spectrum of challenges, from handling document collections to enabling conversational question-answering. It's important to note that we streamline the choice of possible implementations to a system consisting of four distinct components: Extract, CQU, Retriever and Reader. The validation of this newly established system approach is conducted in a \gls{poc} manner in Chapter \ref{chap:eval}. Here, we implement and evaluate an exemplary system using the collection of examination regulations from Heidelberg University. This chapter also includes insightful reflections on the evaluation of conversational question-answering systems. This concludes the content outline of this thesis.
